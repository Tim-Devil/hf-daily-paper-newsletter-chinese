[
  {
    "title": "The End of Manual Decoding: Towards Truly End-to-End Language Models",
    "summary": "The \"end-to-end\" label for LLMs is a misnomer. In practice, they depend on a\nnon-differentiable decoding process that requires laborious, hand-tuning of\nhyperparameters like temperature and top-p. This paper introduces AutoDeco, a\nnovel architecture that enables truly \"end-to-end\" generation by learning to\ncontrol its own decoding strategy. We augment the standard transformer with\nlightweight heads that, at each step, dynamically predict context-specific\ntemperature and top-p values alongside the next-token logits. This approach\ntransforms decoding into a parametric, token-level process, allowing the model\nto self-regulate its sampling strategy within a single forward pass.\n  Through extensive experiments on eight benchmarks, we demonstrate that\nAutoDeco not only significantly outperforms default decoding strategies but\nalso achieves performance comparable to an oracle-tuned baseline derived from\n\"hacking the test set\"-a practical upper bound for any static method.\nCrucially, we uncover an emergent capability for instruction-based decoding\ncontrol: the model learns to interpret natural language commands (e.g.,\n\"generate with low randomness\") and adjusts its predicted temperature and top-p\non a token-by-token basis, opening a new paradigm for steerable and interactive\nLLM decoding.",
    "translation": "标题：手动解码的终结：迈向真正端到端的语言模型\n\n摘要：当前大语言模型的\"端到端\"属性实为误称。实践中它们仍依赖于不可微的解码过程，需要人工繁琐调整温度系数和top-p等超参数。本文提出AutoDeco创新架构，通过让模型学习自主控制解码策略，实现真正的\"端到端\"生成。我们在标准Transformer基础上增设轻量化头部模块，使其能够在每个生成步骤中动态预测上下文相关的温度值与top-p参数，同时输出下一个词元的逻辑值。这种方法将解码过程转化为参数化的词元级操作，使模型在单次前向传播中即可实现自适应的采样策略调控。\n\n通过在八个基准测试上的大量实验，我们证明AutoDeco不仅显著优于默认解码策略，其性能更接近通过\"测试集调优\"获得的理想基线——这代表了所有静态方法的实际上界。更重要的是，我们发现了基于指令的解码控制新兴能力：模型能够理解自然语言指令（如\"低随机性生成\"），并在词元级别动态调整预测的温度值和top-p参数，这为可控交互式大语言模型解码开辟了新范式。",
    "url": "https://huggingface.co/papers/2510.26697",
    "arxiv_url": "https://arxiv.org/abs/2510.26697"
  },
  {
    "title": "Emu3.5: Native Multimodal Models are World Learners",
    "summary": "We introduce Emu3.5, a large-scale multimodal world model that natively\npredicts the next state across vision and language. Emu3.5 is pre-trained\nend-to-end with a unified next-token prediction objective on a corpus of\nvision-language interleaved data containing over 10 trillion tokens, primarily\nderived from sequential frames and transcripts of internet videos. The model\nnaturally accepts interleaved vision-language inputs and generates interleaved\nvision-language outputs. Emu3.5 is further post-trained with large-scale\nreinforcement learning to enhance multimodal reasoning and generation. To\nimprove inference efficiency, we propose Discrete Diffusion Adaptation (DiDA),\nwhich converts token-by-token decoding into bidirectional parallel prediction,\naccelerating per-image inference by about 20x without sacrificing performance.\nEmu3.5 exhibits strong native multimodal capabilities, including long-horizon\nvision-language generation, any-to-image (X2I) generation, and complex\ntext-rich image generation. It also exhibits generalizable world-modeling\nabilities, enabling spatiotemporally consistent world exploration and\nopen-world embodied manipulation across diverse scenarios and tasks. For\ncomparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image\n(Nano Banana) on image generation and editing tasks and demonstrates superior\nresults on a suite of interleaved generation tasks. We open-source Emu3.5 at\nhttps://github.com/baaivision/Emu3.5 to support community research.",
    "translation": "标题：Emu3.5：原生多模态模型的世界认知系统\n\n摘要：本文提出Emu3.5——一种通过原生方式预测视觉与语言跨模态状态演进的大规模多模态世界模型。该模型基于包含超过10万亿标记的视觉语言交织数据集进行端到端预训练，这些数据主要源自互联网视频的连续帧序列及对应文本转录，采用统一的下一标记预测目标。该模型天然支持交织式视觉语言输入，并生成交织式视觉语言输出。通过大规模强化学习后训练，Emu3.5进一步增强了多模态推理与生成能力。为提升推理效率，我们提出离散扩散适配方法（DiDA），将逐标记解码转换为双向并行预测，在保持性能不变的前提下实现单图像推理约20倍加速。实验表明，Emu3.5具备强大的原生多模态能力，包括长程视觉语言生成、任意模态到图像（X2I）生成以及复杂文本图像合成。该模型还展现出可泛化的世界建模能力，支持跨场景任务的时空一致性世界探索与开放世界具身操作。在图像生成与编辑任务中，Emu3.5达到与Gemini 2.5 Flash Image（Nano Banana）相当的性能，并在交织生成任务集上展现更优结果。我们已在https://github.com/baaivision/Emu3.5开源Emu3.5以支持社区研究。",
    "url": "https://huggingface.co/papers/2510.26583",
    "arxiv_url": "https://arxiv.org/abs/2510.26583"
  },
  {
    "title": "Kimi Linear: An Expressive, Efficient Attention Architecture",
    "summary": "We introduce Kimi Linear, a hybrid linear attention architecture that, for\nthe first time, outperforms full attention under fair comparisons across\nvarious scenarios -- including short-context, long-context, and reinforcement\nlearning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an\nexpressive linear attention module that extends Gated DeltaNet with a\nfiner-grained gating mechanism, enabling more effective use of limited\nfinite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware\nefficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)\ntransition matrices, which substantially reduces computation compared to the\ngeneral DPLR formulation while remaining more consistent with the classical\ndelta rule.\n  We pretrain a Kimi Linear model with 3B activated parameters and 48B total\nparameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention\n(MLA). Our experiments show that with an identical training recipe, Kimi Linear\noutperforms full MLA with a sizeable margin across all evaluated tasks, while\nreducing KV cache usage by up to 75% and achieving up to 6 times decoding\nthroughput for a 1M context. These results demonstrate that Kimi Linear can be\na drop-in replacement for full attention architectures with superior\nperformance and efficiency, including tasks with longer input and output\nlengths.\n  To support further research, we open-source the KDA kernel and vLLM\nimplementations, and release the pre-trained and instruction-tuned model\ncheckpoints.",
    "translation": "标题：Kimi Linear：一种高表达能力的高效注意力架构\n\n摘要：本文提出Kimi Linear混合线性注意力架构，该架构在公平对比条件下首次在多类场景中全面超越完全注意力机制——包括短上下文、长上下文及强化学习的扩展场景。其核心组件Kimi Delta注意力（KDA）是一种具备高表达能力的线性注意力模块，通过细粒度门控机制扩展了门控DeltaNet，从而更有效地利用有限状态的RNN记忆容量。我们独创的分块算法采用对角加低秩（DPLR）转移矩阵的特化变体，在保持与经典Delta规则更高一致性的同时，相较通用DPLR公式显著降低计算量，实现了优异的硬件效率。\n\n基于KDA与多头潜在注意力（MLA）的层级混合架构，我们预训练了具有30亿激活参数和480亿总参数的Kimi Linear模型。实验表明：在相同训练方案下，Kimi Linear在所有评估任务中均以明显优势超越完全MLA架构，同时将KV缓存使用量降低最高达75%，在百万级上下文场景中实现最高6倍的解码吞吐量。这些结果证明Kimi Linear可作为完全注意力架构的高性能替代方案，在包括更长输入输出序列的任务中均展现出卓越的性能与效率。\n\n为促进后续研究，我们开源了KDA内核与vLLM实现，并发布了预训练及指令微调的模型检查点。",
    "url": "https://huggingface.co/papers/2510.26692",
    "arxiv_url": "https://arxiv.org/abs/2510.26692"
  },
  {
    "title": "Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in\n  Web Games",
    "summary": "OpenAI's ChatGPT Atlas introduces new capabilities for web interaction,\nenabling the model to analyze webpages, process user intents, and execute\ncursor and keyboard inputs directly within the browser. While its capacity for\ninformation retrieval tasks has been demonstrated, its performance in dynamic,\ninteractive environments remains less explored. In this study, we conduct an\nearly evaluation of Atlas's web interaction capabilities using browser-based\ngames as test scenarios, including Google's T-Rex Runner, Sudoku, Flappy Bird,\nand Stein.world. We employ in-game performance scores as quantitative metrics\nto assess performance across different task types. Our results show that Atlas\nperforms strongly in logical reasoning tasks like Sudoku, completing puzzles\nsignificantly faster than human baselines, but struggles substantially in\nreal-time games requiring precise timing and motor control, often failing to\nprogress beyond initial obstacles. These findings suggest that while Atlas\ndemonstrates capable analytical processing, there remain notable limitations in\ndynamic web environments requiring real-time interaction. The website of our\nproject can be found at https://atlas-game-eval.github.io.",
    "translation": "标题：智能体能否征服网络？探索ChatGPT Atlas智能体在网络游戏中的前沿表现\n\n摘要：OpenAI的ChatGPT Atlas模型引入了全新的网络交互能力，使模型能够分析网页内容、处理用户意图，并直接在浏览器中执行光标与键盘输入操作。尽管该模型在信息检索任务中已展现出卓越能力，但其在动态交互环境中的表现仍待深入探索。本研究以浏览器游戏作为测试场景，对Atlas的网络交互能力进行了初步评估，测试对象包括谷歌恐龙跑酷、数独游戏、像素鸟和Stein.world等游戏。我们采用游戏内评分作为量化指标，以评估其在不同任务类型中的表现。研究结果表明，Atlas在数独等逻辑推理任务中表现优异，解题速度显著超越人类基准，但在需要精确时序和动作控制的实时游戏中表现欠佳，往往难以突破初始障碍。这些发现表明，虽然Atlas具备出色的分析处理能力，但在需要实时交互的动态网络环境中仍存在明显局限。本项目网站地址：https://atlas-game-eval.github.io。",
    "url": "https://huggingface.co/papers/2510.26298",
    "arxiv_url": "https://arxiv.org/abs/2510.26298"
  },
  {
    "title": "Exploring Conditions for Diffusion models in Robotic Control",
    "summary": "While pre-trained visual representations have significantly advanced\nimitation learning, they are often task-agnostic as they remain frozen during\npolicy learning. In this work, we explore leveraging pre-trained text-to-image\ndiffusion models to obtain task-adaptive visual representations for robotic\ncontrol, without fine-tuning the model itself. However, we find that naively\napplying textual conditions - a successful strategy in other vision domains -\nyields minimal or even negative gains in control tasks. We attribute this to\nthe domain gap between the diffusion model's training data and robotic control\nenvironments, leading us to argue for conditions that consider the specific,\ndynamic visual information required for control. To this end, we propose ORCA,\nwhich introduces learnable task prompts that adapt to the control environment\nand visual prompts that capture fine-grained, frame-specific details. Through\nfacilitating task-adaptive representations with our newly devised conditions,\nour approach achieves state-of-the-art performance on various robotic control\nbenchmarks, significantly surpassing prior methods.",
    "translation": "标题：探索扩散模型在机器人控制中的应用条件\n\n摘要：尽管预训练的视觉表征显著推动了模仿学习的发展，但这些表征通常在策略学习过程中保持固定，导致其与具体任务无关。在本研究中，我们探索如何利用预训练的文本到图像扩散模型来获取适用于机器人控制的任务自适应视觉表征，而无需对模型本身进行微调。然而我们发现，直接应用文本条件（在其他视觉领域已获成功的策略）在控制任务中收效甚微甚至会产生负面效果。我们将此归因于扩散模型训练数据与机器人控制环境之间的领域差异，进而提出应当采用能够考虑控制任务所需特定动态视觉信息的条件参数。为此，我们提出ORCA框架，该框架引入可学习的任务提示以适应控制环境，同时采用视觉提示来捕捉细粒度的帧级特征。通过新设计的条件参数实现任务自适应表征，我们的方法在多个机器人控制基准测试中达到了最先进的性能水平，显著超越了现有方法。",
    "url": "https://huggingface.co/papers/2510.15510",
    "arxiv_url": "https://arxiv.org/abs/2510.15510"
  },
  {
    "title": "AMO-Bench: Large Language Models Still Struggle in High School Math\n  Competitions",
    "summary": "We present AMO-Bench, an Advanced Mathematical reasoning benchmark with\nOlympiad level or even higher difficulty, comprising 50 human-crafted problems.\nExisting benchmarks have widely leveraged high school math competitions for\nevaluating mathematical reasoning capabilities of large language models (LLMs).\nHowever, many existing math competitions are becoming less effective for\nassessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To\naddress this, AMO-Bench introduces more rigorous challenges by ensuring all 50\nproblems are (1) cross-validated by experts to meet at least the International\nMathematical Olympiad (IMO) difficulty standards, and (2) entirely original\nproblems to prevent potential performance leakages from data memorization.\nMoreover, each problem in AMO-Bench requires only a final answer rather than a\nproof, enabling automatic and robust grading for evaluation. Experimental\nresults across 26 LLMs on AMO-Bench show that even the best-performing model\nachieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%.\nBeyond these poor performances, our further analysis reveals a promising\nscaling trend with increasing test-time compute on AMO-Bench. These results\nhighlight the significant room for improving the mathematical reasoning in\ncurrent LLMs. We release AMO-Bench to facilitate further research into\nadvancing the reasoning abilities of language models.\nhttps://amo-bench.github.io/",
    "translation": "标题：AMO-Bench：大型语言模型在高中数学竞赛中仍面临挑战\n\n摘要：我们提出AMO-Bench——一个具有奥林匹克竞赛级别或更高难度的进阶数学推理基准，包含50道人工设计的题目。现有基准普遍采用高中数学竞赛来评估大型语言模型的数学推理能力，但由于性能饱和现象（如AIME24/25），许多现有数学竞赛对顶尖大型语言模型的评估效能正在减弱。为此，AMO-Bench通过确保所有50道题目满足以下条件来引入更严格的挑战：（1）经专家交叉验证达到国际数学奥林匹克竞赛难度标准；（2）全部为原创题目以避免数据记忆导致的性能泄露。此外，AMO-Bench中每道题目仅需最终答案而非证明过程，支持自动可靠的评估打分。在26个大型语言模型上的实验结果表明，性能最优模型在AMO-Bench上的准确率仅为52.4%，大多数模型得分低于40%。除表现欠佳外，我们进一步分析发现增加测试时计算量会带来显著的规模扩展趋势。这些结果揭示了当前大型语言模型在数学推理方面存在巨大改进空间。我们公开发布AMO-Bench以推动语言模型推理能力的前沿研究。\nhttps://amo-bench.github.io/",
    "url": "https://huggingface.co/papers/2510.26768",
    "arxiv_url": "https://arxiv.org/abs/2510.26768"
  },
  {
    "title": "Surfer 2: The Next Generation of Cross-Platform Computer Use Agents",
    "summary": "Building agents that generalize across web, desktop, and mobile environments\nremains an open challenge, as prior systems rely on environment-specific\ninterfaces that limit cross-platform deployment. We introduce Surfer 2, a\nunified architecture operating purely from visual observations that achieves\nstate-of-the-art performance across all three environments. Surfer 2 integrates\nhierarchical context management, decoupled planning and execution, and\nself-verification with adaptive recovery, enabling reliable operation over long\ntask horizons. Our system achieves 97.1% accuracy on WebVoyager, 69.6% on\nWebArena, 60.1% on OSWorld, and 87.1% on AndroidWorld, outperforming all prior\nsystems without task-specific fine-tuning. With multiple attempts, Surfer 2\nexceeds human performance on all benchmarks. These results demonstrate that\nsystematic orchestration amplifies foundation model capabilities and enables\ngeneral-purpose computer control through visual interaction alone, while\ncalling for a next-generation vision language model to achieve Pareto-optimal\ncost-efficiency.",
    "translation": "标题：Surfer 2：新一代跨平台计算机智能体系统\n\n摘要：构建能够泛化应用于网页、桌面及移动环境的智能体仍面临重大挑战，现有系统多依赖特定环境接口，限制了跨平台部署能力。本文提出Surfer 2统一架构，该系统仅通过视觉观察实现操作，在三大环境均达到最先进性能。Surfer 2融合了分层上下文管理、解耦规划与执行机制，以及具备自适应恢复能力的自我验证模块，确保长周期任务的可靠运行。实验表明，本系统在WebVoyager达到97.1%准确率，WebArena 69.6%，OSWorld 60.1%，AndroidWorld 87.1%，无需任务特定微调即超越所有现有系统。经多轮尝试，Surfer 2在所有基准测试中均超越人类表现。这些成果证明：系统化架构设计能显著增强基础模型能力，实现纯视觉交互的通用计算机控制，同时指出需开发新一代视觉语言模型以实现帕累托最优的成本效益。",
    "url": "https://huggingface.co/papers/2510.19949",
    "arxiv_url": "https://arxiv.org/abs/2510.19949"
  },
  {
    "title": "Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with\n  the MME-CoF Benchmark",
    "summary": "Recent video generation models can produce high-fidelity, temporally coherent\nvideos, indicating that they may encode substantial world knowledge. Beyond\nrealistic synthesis, they also exhibit emerging behaviors indicative of visual\nperception, modeling, and manipulation. Yet, an important question still\nremains: Are video models ready to serve as zero-shot reasoners in challenging\nvisual reasoning scenarios? In this work, we conduct an empirical study to\ncomprehensively investigate this question, focusing on the leading and popular\nVeo-3. We evaluate its reasoning behavior across 12 dimensions, including\nspatial, geometric, physical, temporal, and embodied logic, systematically\ncharacterizing both its strengths and failure modes. To standardize this study,\nwe curate the evaluation data into MME-CoF, a compact benchmark that enables\nin-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our\nfindings reveal that while current video models demonstrate promising reasoning\npatterns on short-horizon spatial coherence, fine-grained grounding, and\nlocally consistent dynamics, they remain limited in long-horizon causal\nreasoning, strict geometric constraints, and abstract logic. Overall, they are\nnot yet reliable as standalone zero-shot reasoners, but exhibit encouraging\nsigns as complementary visual engines alongside dedicated reasoning models.\nProject page: https://video-cof.github.io",
    "translation": "标题：视频模型是否已具备零样本推理能力？基于MME-CoF基准的实证研究\n\n摘要：当前视频生成模型能够产出高保真度、时序连贯的视频内容，表明其可能编码了丰富的世界知识。除真实感合成外，这些模型还展现出表征视觉感知、建模与操作的新兴能力。然而，一个重要问题尚未解决：在具有挑战性的视觉推理场景中，视频模型是否已具备作为零样本推理器的条件？本研究通过实证分析系统探讨该问题，聚焦于主流模型Veo-3。我们从12个维度评估其推理行为，涵盖空间、几何、物理、时序及具身逻辑等领域，系统刻画其优势与失效模式。为规范研究过程，我们构建了MME-CoF评估基准——一个支持对帧序列推理进行深入全面评估的紧凑基准。研究发现：当前视频模型在短时域空间一致性、细粒度语义 grounding 及局部动态连贯性方面展现出有前景的推理模式，但在长时域因果推理、严格几何约束和抽象逻辑方面仍存在局限。总体而言，视频模型尚未成为可靠的独立零样本推理器，但作为专用推理模型的互补视觉引擎展现出令人鼓舞的潜力。项目主页：https://video-cof.github.io",
    "url": "https://huggingface.co/papers/2510.26802",
    "arxiv_url": "https://arxiv.org/abs/2510.26802"
  },
  {
    "title": "The Quest for Generalizable Motion Generation: Data, Model, and\n  Evaluation",
    "summary": "Despite recent advances in 3D human motion generation (MoGen) on standard\nbenchmarks, existing models still face a fundamental bottleneck in their\ngeneralization capability. In contrast, adjacent generative fields, most\nnotably video generation (ViGen), have demonstrated remarkable generalization\nin modeling human behaviors, highlighting transferable insights that MoGen can\nleverage. Motivated by this observation, we present a comprehensive framework\nthat systematically transfers knowledge from ViGen to MoGen across three key\npillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a\nlarge-scale dataset comprising 228,000 high-quality motion samples that\nintegrates high-fidelity optical MoCap data with semantically annotated motions\nfrom web videos and synthesized samples generated by state-of-the-art ViGen\nmodels. The dataset includes both text-motion pairs and text-video-motion\ntriplets, substantially expanding semantic diversity. Second, we propose\nViMoGen, a flow-matching-based diffusion transformer that unifies priors from\nMoCap data and ViGen models through gated multimodal conditioning. To enhance\nefficiency, we further develop ViMoGen-light, a distilled variant that\neliminates video generation dependencies while preserving strong\ngeneralization. Finally, we present MBench, a hierarchical benchmark designed\nfor fine-grained evaluation across motion quality, prompt fidelity, and\ngeneralization ability. Extensive experiments show that our framework\nsignificantly outperforms existing approaches in both automatic and human\nevaluations. The code, data, and benchmark will be made publicly available.",
    "translation": "标题：可泛化运动生成的探索：数据、模型与评估框架\n\n摘要：尽管三维人体运动生成在标准基准测试中取得显著进展，现有模型在泛化能力方面仍面临根本性瓶颈。相比之下，相邻生成领域（尤以视频生成为代表）在人体行为建模中展现出卓越的泛化性能，这为运动生成领域提供了可迁移的重要启示。基于此观察，我们提出一个综合框架，系统性地将视频生成的知识迁移至运动生成领域，涵盖数据、建模与评估三大支柱。首先，我们推出ViMoGen-228K大规模数据集，包含228,000个高质量运动样本，融合了高精度光学运动捕捉数据、网络视频中的语义标注运动，以及顶尖视频生成模型合成的样本。该数据集同时包含文本-运动对和文本-视频-运动三元组，显著扩展了语义多样性。其次，我们提出基于流匹配的扩散变换器ViMoGen，通过门控多模态条件机制统一运动捕捉数据与视频生成模型的前验知识。为提升效率，我们进一步开发轻量化版本ViMoGen-light，在保持强泛化能力的同时消除对视频生成的依赖。最后，我们建立MBench分层评估基准，支持运动质量、提示符保真度与泛化能力的细粒度评估。大量实验表明，我们的框架在自动评估与人工评估中均显著优于现有方法。相关代码、数据与评估基准将公开发布。",
    "url": "https://huggingface.co/papers/2510.26794",
    "arxiv_url": "https://arxiv.org/abs/2510.26794"
  },
  {
    "title": "Supervised Reinforcement Learning: From Expert Trajectories to Step-wise\n  Reasoning",
    "summary": "Large Language Models (LLMs) often struggle with problems that require\nmulti-step reasoning. For small-scale open-source models, Reinforcement\nLearning with Verifiable Rewards (RLVR) fails when correct solutions are rarely\nsampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to\noverfit long demonstrations through rigid token-by-token imitation. To address\nthis gap, we propose Supervised Reinforcement Learning (SRL), a framework that\nreformulates problem solving as generating a sequence of logical \"actions\". SRL\ntrains the model to generate an internal reasoning monologue before committing\nto each action. It provides smoother rewards based on the similarity between\nthe model's actions and expert actions extracted from the SFT dataset in a\nstep-wise manner. This supervision offers richer learning signals even when all\nrollouts are incorrect, while encouraging flexible reasoning guided by expert\ndemonstrations. As a result, SRL enables small models to learn challenging\nproblems previously unlearnable by SFT or RLVR. Moreover, initializing training\nwith SRL before refining with RLVR yields the strongest overall performance.\nBeyond reasoning benchmarks, SRL generalizes effectively to agentic software\nengineering tasks, establishing it as a robust and versatile training framework\nfor reasoning-oriented LLMs.",
    "translation": "标题：监督式强化学习：从专家轨迹到分步推理\n\n摘要：大型语言模型在处理需要多步推理的问题时常面临困难。对于小规模开源模型，基于可验证奖励的强化学习在多次尝试后仍难以采样到正确解，而监督微调则容易通过逐词僵化模仿长演示示例导致过拟合。为解决这一问题，我们提出监督式强化学习框架，将问题求解重新定义为生成逻辑\"动作\"序列的过程。该框架训练模型在执行每个动作前生成内部推理独白，并以分步方式根据模型动作与从监督微调数据集中提取的专家动作之间的相似性提供平滑奖励。这种监督机制即使在所有推演都错误的情况下也能提供更丰富的学习信号，同时鼓励在专家演示引导下进行灵活推理。实验表明，监督式强化学习能使小规模模型学会原本无法通过监督微调或可验证奖励强化学习掌握的复杂问题。此外，在采用可验证奖励强化学习进行精调前，先用监督式强化学习初始化训练，可获得最强的综合性能。除推理基准测试外，监督式强化学习在自主软件工程任务中也展现出色泛化能力，确立了其作为面向推理的大型语言模型的稳健且通用的训练框架地位。",
    "url": "https://huggingface.co/papers/2510.25992",
    "arxiv_url": "https://arxiv.org/abs/2510.25992"
  },
  {
    "title": "The Era of Agentic Organization: Learning to Organize with Language\n  Models",
    "summary": "We envision a new era of AI, termed agentic organization, where agents solve\ncomplex problems by working collaboratively and concurrently, enabling outcomes\nbeyond individual intelligence. To realize this vision, we introduce\nasynchronous thinking (AsyncThink) as a new paradigm of reasoning with large\nlanguage models, which organizes the internal thinking process into\nconcurrently executable structures. Specifically, we propose a thinking\nprotocol where an organizer dynamically assigns sub-queries to workers, merges\nintermediate knowledge, and produces coherent solutions. More importantly, the\nthinking structure in this protocol can be further optimized through\nreinforcement learning. Experiments demonstrate that AsyncThink achieves 28%\nlower inference latency compared to parallel thinking while improving accuracy\non mathematical reasoning. Moreover, AsyncThink generalizes its learned\nasynchronous thinking capabilities, effectively tackling unseen tasks without\nadditional training.",
    "translation": "标题：主体性组织时代：利用语言模型实现组织化学习\n\n摘要：我们展望人工智能的新纪元——主体性组织时代，在这一时代中，智能体通过协同并行工作解决复杂问题，实现超越个体智能的成果。为实现这一愿景，我们引入异步思维作为大语言模型推理的新范式，将内部思考过程组织为可并行执行的结构。具体而言，我们提出一种思维协议：组织者动态分配子任务给工作节点，整合中间知识，最终生成连贯解决方案。更重要的是，该协议中的思维结构可通过强化学习进一步优化。实验表明，相较于并行思维模式，异步思维在数学推理任务中推理延迟降低28%的同时提升了准确率。此外，异步思维能够泛化其习得的异步思考能力，无需额外训练即可有效处理未知任务。",
    "url": "https://huggingface.co/papers/2510.26658",
    "arxiv_url": "https://arxiv.org/abs/2510.26658"
  },
  {
    "title": "OmniX: From Unified Panoramic Generation and Perception to\n  Graphics-Ready 3D Scenes",
    "summary": "There are two prevalent ways to constructing 3D scenes: procedural generation\nand 2D lifting. Among them, panorama-based 2D lifting has emerged as a\npromising technique, leveraging powerful 2D generative priors to produce\nimmersive, realistic, and diverse 3D environments. In this work, we advance\nthis technique to generate graphics-ready 3D scenes suitable for physically\nbased rendering (PBR), relighting, and simulation. Our key insight is to\nrepurpose 2D generative models for panoramic perception of geometry, textures,\nand PBR materials. Unlike existing 2D lifting approaches that emphasize\nappearance generation and ignore the perception of intrinsic properties, we\npresent OmniX, a versatile and unified framework. Based on a lightweight and\nefficient cross-modal adapter structure, OmniX reuses 2D generative priors for\na broad range of panoramic vision tasks, including panoramic perception,\ngeneration, and completion. Furthermore, we construct a large-scale synthetic\npanorama dataset containing high-quality multimodal panoramas from diverse\nindoor and outdoor scenes. Extensive experiments demonstrate the effectiveness\nof our model in panoramic visual perception and graphics-ready 3D scene\ngeneration, opening new possibilities for immersive and physically realistic\nvirtual world generation.",
    "translation": "标题：OmniX：从统一全景生成与感知到图形就绪型三维场景\n\n摘要：当前构建三维场景主要存在两种主流方式：程序化生成与二维提升技术。其中基于全景的二维提升技术展现出显著潜力，通过利用强大的二维生成先验知识，能够创造出沉浸感强、真实性高且多样化的三维环境。本研究推进该技术以生成适用于基于物理渲染（PBR）、重光照及仿真的图形就绪型三维场景。我们的核心思路是重新定位二维生成模型，使其具备对几何结构、纹理及PBR材质的全景感知能力。与现有侧重外观生成而忽略内在属性感知的二维提升方法不同，我们提出了OmniX——一个通用统一的框架。基于轻量级高效跨模态适配器结构，OmniX将二维生成先验知识复用于全景视觉任务领域，包括全景感知、生成与补全。此外，我们构建了大规模合成全景数据集，包含来自多样化室内外场景的高质量多模态全景数据。大量实验证明我们的模型在全景视觉感知和图形就绪型三维场景生成方面的有效性，为沉浸式物理真实虚拟世界生成开辟了新可能。",
    "url": "https://huggingface.co/papers/2510.26800",
    "arxiv_url": "https://arxiv.org/abs/2510.26800"
  },
  {
    "title": "MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and\n  efficiency",
    "summary": "Current text-to-image generative models are trained on large uncurated\ndatasets to enable diverse generation capabilities. However, this does not\nalign well with user preferences. Recently, reward models have been\nspecifically designed to perform post-hoc selection of generated images and\nalign them to a reward, typically user preference. This discarding of\ninformative data together with the optimizing for a single reward tend to harm\ndiversity, semantic fidelity and efficiency. Instead of this post-processing,\nwe propose to condition the model on multiple reward models during training to\nlet the model learn user preferences directly. We show that this not only\ndramatically improves the visual quality of the generated images but it also\nsignificantly speeds up the training. Our proposed method, called MIRO,\nachieves state-of-the-art performances on the GenEval compositional benchmark\nand user-preference scores (PickAScore, ImageReward, HPSv2).",
    "translation": "标题：MIRO：多奖励条件预训练提升文本生成图像的质量与效率\n\n摘要：当前文本生成图像模型通过在大型未筛选数据集上进行训练以实现多样化的生成能力，但这种做法与用户偏好存在显著偏差。近期研究专门设计了奖励模型，对生成图像进行后验选择以使其符合特定奖励（通常指用户偏好）。这种丢弃有效数据并仅针对单一奖励进行优化的方式往往会损害生成结果的多样性、语义保真度及训练效率。为替代此类后处理方法，我们提出在训练过程中使模型同时适应多个奖励模型的条件约束，从而让模型直接学习用户偏好。研究表明，该方法不仅显著提升了生成图像的视觉质量，同时大幅加快了训练速度。我们提出的MIRO方法在GenEval组合基准测试和用户偏好评分（PickAScore、ImageReward、HPSv2）中均达到了最先进的性能水平。",
    "url": "https://huggingface.co/papers/2510.25897",
    "arxiv_url": "https://arxiv.org/abs/2510.25897"
  },
  {
    "title": "EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic\n  Health Record Analysis",
    "summary": "Electronic Health Records (EHRs) contain rich yet complex information, and\ntheir automated analysis is critical for clinical decision-making. Despite\nrecent advances of large language models (LLMs) in clinical workflows, their\nability to analyze EHRs remains limited due to narrow task coverage and lack of\nEHR-oriented reasoning capabilities. This paper aims to bridge the gap,\nspecifically, we present EHR-Ins, a large-scale, comprehensive EHR reasoning\ninstruction dataset, comprising 300k high-quality reasoning cases and 4M\nnon-reasoning cases across 42 distinct EHR tasks. Its core innovation is a\nthinking-graph-driven framework that enables to generate high-quality reasoning\ndata at scale. Based on it, we develop EHR-R1, a series of reasoning-enhanced\nLLMs with up to 72B parameters tailored for EHR analysis. Through a multi-stage\ntraining paradigm, including domain adaptation, reasoning enhancement, and\nreinforcement learning, EHR-R1 systematically acquires domain knowledge and\ndiverse reasoning capabilities, enabling accurate and robust EHR analysis.\nLastly, we introduce EHR-Bench, a new benchmark curated from MIMIC-IV, spanning\n42 tasks, to comprehensively assess reasoning and prediction across EHR\nscenarios. In experiments, we show that the resulting EHR-R1 consistently\noutperforms state-of-the-art commercial and open-source LLMs (including\nDeepSeek-V3 and GPT-4o), surpassing GPT-4o by over 30 points on MIMIC-Bench and\nachieving a 10\\% higher zero-shot AUROC on EHRSHOT. Collectively, EHR-Ins,\nEHR-R1, and EHR-Bench have significantly advanced the development for more\nreliable and clinically relevant EHR analysis.",
    "translation": "标题：EHR-R1：面向电子健康记录分析的推理增强型基础语言模型\n\n摘要：电子健康记录包含丰富而复杂的信息，其自动化分析对临床决策至关重要。尽管大语言模型在临床工作流程中取得进展，但由于任务覆盖范围有限且缺乏面向电子健康记录的推理能力，其分析电子健康记录的效能仍受制约。本文致力于填补这一空白，具体提出EHR-Ins——一个大规模综合性电子健康记录推理指令数据集，涵盖42个不同电子健康记录任务的30万条高质量推理案例与400万条非推理案例。其核心创新在于采用思维图驱动的框架，实现高质量推理数据的大规模生成。基于此，我们开发出EHR-R1系列推理增强型大语言模型，参数量最高达720亿，专为电子健康记录分析定制。通过包含领域适应、推理增强和强化学习的多阶段训练范式，EHR-R1系统性地掌握了领域知识与多样化推理能力，可实现精准稳健的电子健康记录分析。最后，我们基于MIMIC-IV构建了EHR-Bench新基准，涵盖42项任务，全面评估电子健康记录场景中的推理与预测能力。实验结果表明，EHR-R1持续超越最先进的商业及开源大语言模型（包括DeepSeek-V3和GPT-4o），在MIMIC-Bench上较GPT-4o提升逾30分，在EHRSHOT上实现10%的零样本AUROC提升。EHR-Ins、EHR-R1与EHR-Bench共同为开发更可靠且具临床相关性的电子健康记录分析系统奠定了重要基础。",
    "url": "https://huggingface.co/papers/2510.25628",
    "arxiv_url": "https://arxiv.org/abs/2510.25628"
  },
  {
    "title": "OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal\n  Document Layout Generation",
    "summary": "Document AI has advanced rapidly and is attracting increasing attention. Yet,\nwhile most efforts have focused on document layout analysis (DLA), its\ngenerative counterpart, document layout generation, remains underexplored. A\nmajor obstacle lies in the scarcity of diverse layouts: academic papers with\nManhattan-style structures dominate existing studies, while open-world genres\nsuch as newspapers and magazines remain severely underrepresented. To address\nthis gap, we curate OmniLayout-1M, the first million-scale dataset of diverse\ndocument layouts, covering six common document types and comprising\ncontemporary layouts collected from multiple sources. Moreover, since existing\nmethods struggle in complex domains and often fail to arrange long sequences\ncoherently, we introduce OmniLayout-LLM, a 0.5B model with designed two-stage\nCoarse-to-Fine learning paradigm: 1) learning universal layout principles from\nOmniLayout-1M with coarse category definitions, and 2) transferring the\nknowledge to a specific domain with fine-grained annotations. Extensive\nexperiments demonstrate that our approach achieves strong performance on\nmultiple domains in M^{6}Doc dataset, substantially surpassing both existing\nlayout generation experts and several latest general-purpose LLMs. Our code,\nmodels, and dataset will be publicly released.",
    "translation": "标题：OmniLayout：基于大语言模型的从粗到精学习实现通用文档布局生成\n\n摘要：文档智能技术发展迅猛并日益受到关注。然而当前研究大多集中于文档布局分析，其生成式对应方向——文档布局生成仍待深入探索。主要障碍在于多样化布局数据的稀缺：现有研究主要集中于曼哈顿式结构的学术论文，而报纸、杂志等开放领域文档类型严重缺乏代表性。为弥补这一空白，我们构建了OmniLayout-1M——首个百万量级的多样化文档布局数据集，涵盖六种常见文档类型，包含从多源采集的当代布局样本。针对现有方法在复杂领域表现不佳且难以连贯排列长序列的问题，我们提出了OmniLayout-LLM模型（参数量5亿），采用设计的两阶段从粗到精学习范式：1）通过粗粒度类别定义从OmniLayout-1M学习通用布局原则；2）利用细粒度标注将知识迁移至特定领域。在M^{6}Doc数据集上的大量实验表明，我们的方法在多个领域均取得卓越性能，显著超越现有布局生成专家模型及多个最新通用大语言模型。我们的代码、模型和数据集将公开发布。",
    "url": "https://huggingface.co/papers/2510.26213",
    "arxiv_url": "https://arxiv.org/abs/2510.26213"
  },
  {
    "title": "Magentic Marketplace: An Open-Source Environment for Studying Agentic\n  Markets",
    "summary": "As LLM agents advance, they are increasingly mediating economic decisions,\nranging from product discovery to transactions, on behalf of users. Such\napplications promise benefits but also raise many questions about agent\naccountability and value for users. Addressing these questions requires\nunderstanding how agents behave in realistic market conditions. However,\nprevious research has largely evaluated agents in constrained settings, such as\nsingle-task marketplaces (e.g., negotiation) or structured two-agent\ninteractions. Real-world markets are fundamentally different: they require\nagents to handle diverse economic activities and coordinate within large,\ndynamic ecosystems where multiple agents with opaque behaviors may engage in\nopen-ended dialogues. To bridge this gap, we investigate two-sided agentic\nmarketplaces where Assistant agents represent consumers and Service agents\nrepresent competing businesses. To study these interactions safely, we develop\nMagentic-Marketplace-- a simulated environment where Assistants and Services\ncan operate. This environment enables us to study key market dynamics: the\nutility agents achieve, behavioral biases, vulnerability to manipulation, and\nhow search mechanisms shape market outcomes. Our experiments show that frontier\nmodels can approach optimal welfare-- but only under ideal search conditions.\nPerformance degrades sharply with scale, and all models exhibit severe\nfirst-proposal bias, creating 10-30x advantages for response speed over\nquality. These findings reveal how behaviors emerge across market conditions,\ninforming the design of fair and efficient agentic marketplaces.",
    "translation": "标题：Magentic Marketplace：一个用于研究智能体市场的开源环境\n\n摘要：随着大语言模型智能体的发展，它们正越来越多地代表用户介导经济决策，从产品发现到交易执行。这类应用虽前景可期，但也引发了关于智能体问责机制与用户价值实现的诸多问题。要解决这些问题，需要深入理解智能体在真实市场环境中的行为模式。然而现有研究大多在受限场景中评估智能体性能，例如单一任务市场（如谈判场景）或结构化的双智能体交互。现实市场存在本质差异：智能体需要处理多样化的经济活动，在由多个行为不透明的智能体构成的大型动态生态系统中进行开放对话协调。为弥补这一差距，我们研究由代表消费者的助手智能体与代表商家的服务智能体构成的双边智能体市场。为安全研究此类交互，我们开发了Magentic-Marketplace——一个支持助手与服务智能体运行的仿真环境。该环境使我们能研究关键市场动态：智能体实现的效用、行为偏差、受操纵脆弱性以及搜索机制如何影响市场结果。实验表明，前沿模型仅能在理想搜索条件下接近最优福利水平。随着规模扩大，性能急剧下降，所有模型均表现出严重的一轮提案偏差，导致响应速度的重要性达到质量因素的10-30倍。这些发现揭示了不同市场条件下行为模式的涌现规律，为设计公平高效的智能体市场提供了重要参考。",
    "url": "https://huggingface.co/papers/2510.25779",
    "arxiv_url": "https://arxiv.org/abs/2510.25779"
  },
  {
    "title": "MedVLSynther: Synthesizing High-Quality Visual Question Answering from\n  Medical Documents with Generator-Verifier LMMs",
    "summary": "Large Multimodal Models (LMMs) are increasingly capable of answering medical\nquestions that require joint reasoning over images and text, yet training\ngeneral medical VQA systems is impeded by the lack of large, openly usable,\nhigh-quality corpora. We present MedVLSynther, a rubric-guided\ngenerator-verifier framework that synthesizes high-quality multiple-choice VQA\nitems directly from open biomedical literature by conditioning on figures,\ncaptions, and in-text references. The generator produces self-contained stems\nand parallel, mutually exclusive options under a machine-checkable JSON schema;\na multi-stage verifier enforces essential gates (self-containment, single\ncorrect answer, clinical validity, image-text consistency), awards fine-grained\npositive points, and penalizes common failure modes before acceptance. Applying\nthis pipeline to PubMed Central yields MedSynVQA: 13,087 audited questions over\n14,803 images spanning 13 imaging modalities and 28 anatomical regions.\nTraining open-weight LMMs with reinforcement learning using verifiable rewards\nimproves accuracy across six medical VQA benchmarks, achieving averages of\n55.85 (3B) and 58.15 (7B), with up to 77.57 on VQA-RAD and 67.76 on PathVQA,\noutperforming strong medical LMMs. A Ablations verify that both generation and\nverification are necessary and that more verified data consistently helps, and\na targeted contamination analysis detects no leakage from evaluation suites. By\noperating entirely on open literature and open-weight models, MedVLSynther\noffers an auditable, reproducible, and privacy-preserving path to scalable\nmedical VQA training data.",
    "translation": "标题：MedVLSynther：基于生成器-验证器大语言模型从医学文档合成高质量视觉问答数据\n\n摘要：大型多模态模型在解答需要联合推理图像与文本的医学问题方面能力日益增强，然而缺乏大规模、可公开使用的高质量语料库阻碍了通用医学视觉问答系统的训练。本文提出MedVLSynther——一个基于规则指导的生成器-验证器框架，该框架通过关联图表、标题及文内参考文献，直接从开放生物医学文献中合成高质量多选题形式的视觉问答条目。生成器按照机器可校验的JSON规范生成自包含题干及并行互斥的选项；多阶段验证器在接收数据前执行核心校验（自包含性、单一正确答案、临床有效性、图文一致性），授予细粒度正向评分，并对常见错误模式进行扣分处理。将该流程应用于PubMed Central数据库后得到MedSynVQA：包含13,087道已审核问题，覆盖14,803张图像，涉及13种影像模态和28个解剖区域。使用可验证奖励通过强化学习训练开放权重大型多模态模型，在六项医学视觉问答基准测试中准确率全面提升，3B和7B模型分别达到55.85和58.15的平均准确率，其中VQA-RAD最高达77.57，PathVQA达67.76，超越现有强医学大型多模态模型。消融实验验证生成与验证环节均不可或缺，更多已验证数据持续带来提升，针对性污染分析未检测到评估集泄露。通过完全基于开放文献和开放权重模型运行，MedVLSynther为可扩展的医学视觉问答训练数据提供了一条可审计、可复现且保护隐私的技术路径。",
    "url": "https://huggingface.co/papers/2510.25867",
    "arxiv_url": "https://arxiv.org/abs/2510.25867"
  },
  {
    "title": "Remote Labor Index: Measuring AI Automation of Remote Work",
    "summary": "AIs have made rapid progress on research-oriented benchmarks of knowledge and\nreasoning, but it remains unclear how these gains translate into economic value\nand automation. To measure this, we introduce the Remote Labor Index (RLI), a\nbroadly multi-sector benchmark comprising real-world, economically valuable\nprojects designed to evaluate end-to-end agent performance in practical\nsettings. AI agents perform near the floor on RLI, with the highest-performing\nagent achieving an automation rate of 2.5%. These results help ground\ndiscussions of AI automation in empirical evidence, setting a common basis for\ntracking AI impacts and enabling stakeholders to proactively navigate AI-driven\nlabor automation.",
    "translation": "标题：远程劳动指数：衡量人工智能对远程工作的自动化程度\n\n摘要：人工智能在知识与推理的研究型基准测试中进展迅速，但这些成果如何转化为经济价值与自动化效能仍不明确。为此我们提出远程劳动指数（RLI）——一个广泛覆盖多领域的基准体系，包含旨在评估实际场景中端到端智能体性能的具经济价值的现实项目。人工智能体在RLI上的表现接近基准下限，表现最佳的智能体仅实现2.5%的自动化率。这些研究结果有助于将人工智能自动化的讨论建立在实证依据之上，为追踪人工智能影响设立共同基准，并使利益相关者能够主动应对人工智能驱动的劳动力自动化变革。",
    "url": "https://huggingface.co/papers/2510.26787",
    "arxiv_url": "https://arxiv.org/abs/2510.26787"
  },
  {
    "title": "Counteracting Matthew Effect in Self-Improvement of LVLMs through\n  Head-Tail Re-balancing",
    "summary": "Self-improvement has emerged as a mainstream paradigm for advancing the\nreasoning capabilities of large vision-language models (LVLMs), where models\nexplore and learn from successful trajectories iteratively. However, we\nidentify a critical issue during this process: the model excels at generating\nhigh-quality trajectories for simple queries (i.e., head data) but struggles\nwith more complex ones (i.e., tail data). This leads to an imbalanced\noptimization that drives the model to prioritize simple reasoning skills, while\nhindering its ability to tackle more complex reasoning tasks. Over iterations,\nthis imbalance becomes increasingly pronounced--a dynamic we term the \"Matthew\neffect\"--which ultimately hinders further model improvement and leads to\nperformance bottlenecks. To counteract this challenge, we introduce four\nefficient strategies from two perspectives: distribution-reshaping and\ntrajectory-resampling, to achieve head-tail re-balancing during the\nexploration-and-learning self-improvement process. Extensive experiments on\nQwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks\ndemonstrate that our methods consistently improve visual reasoning\ncapabilities, outperforming vanilla self-improvement by 3.86 points on average.",
    "translation": "标题：通过头尾数据重平衡抑制大视觉语言模型自改进中的马太效应\n\n摘要：自改进已成为提升大视觉语言模型推理能力的主流范式，该范式通过模型迭代探索并学习成功推理轨迹来实现进步。然而我们发现这一过程中存在关键问题：模型擅长为简单查询（即头部数据）生成高质量轨迹，却难以处理复杂查询（即尾部数据）。这种不平衡优化导致模型偏重简单推理技能，同时阻碍其处理复杂推理任务的能力。随着迭代进行，这种失衡现象日益显著——我们称之为“马太效应”——最终制约模型的持续改进并导致性能瓶颈。为应对这一挑战，我们从分布重塑与轨迹重采样两个维度提出四种高效策略，在探索式学习的自改进过程中实现头尾数据重平衡。在Qwen2-VL-7B-Instruct和InternVL2.5-4B模型上的视觉推理任务实验表明，我们的方法能持续提升视觉推理能力，相较原始自改进范式平均提升3.86个性能点。",
    "url": "https://huggingface.co/papers/2510.26474",
    "arxiv_url": "https://arxiv.org/abs/2510.26474"
  },
  {
    "title": "CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark",
    "summary": "Wearable devices such as smart glasses are transforming the way people\ninteract with their surroundings, enabling users to seek information regarding\nentities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG)\nplays a key role in supporting such questions, yet there is still no\ncomprehensive benchmark for this task, especially regarding wearables\nscenarios. To fill this gap, we present CRAG-MM -- a Comprehensive RAG\nbenchmark for Multi-modal Multi-turn conversations. CRAG-MM contains a diverse\nset of 6.5K (image, question, answer) triplets and 2K visual-based multi-turn\nconversations across 13 domains, including 6.2K egocentric images designed to\nmimic captures from wearable devices. We carefully constructed the questions to\nreflect real-world scenarios and challenges, including five types of\nimage-quality issues, six question types, varying entity popularity, differing\ninformation dynamism, and different conversation turns. We design three tasks:\nsingle-source augmentation, multi-source augmentation, and multi-turn\nconversations -- each paired with an associated retrieval corpus and APIs for\nboth image-KG retrieval and webpage retrieval. Our evaluation shows that\nstraightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM\nsingle- and multi-turn QA, respectively, whereas state-of-the-art industry\nsolutions have similar quality (32%/45%), underscoring ample room for\nimprovement. The benchmark has hosted KDD Cup 2025, attracting about 1K\nparticipants and 5K submissions, with winning solutions improving baseline\nperformance by 28%, highlighting its early impact on advancing the field.",
    "translation": "标题：CRAG-MM：多模态多轮综合检索增强生成基准  \n\n摘要：以智能眼镜为代表的可穿戴设备正在改变人们与周围环境的交互方式，使用户能够实时获取视野中实体的相关信息。多模态检索增强生成（MM-RAG）技术对此类查询具有重要支撑作用，但目前该领域仍缺乏系统性基准，尤其针对可穿戴场景。为此，我们提出CRAG-MM——面向多模态多轮对话的综合RAG基准。该基准包含13个领域的6,500组（图像、问题、答案）三元组及2,000组基于视觉的多轮对话，其中6,200张具身视角图像专为模拟可穿戴设备采集场景设计。我们通过五类图像质量问题、六种提问类型、差异化实体热度、动态信息变化及多轮对话深度等维度精心构建问题集，以反映真实场景的复杂性。基准设置三大任务：单源增强、多源增强与多轮对话，每个任务均配备关联检索库及支持图像-知识图谱检索与网页检索的API接口。评估数据显示，传统RAG方法在单轮与多轮问答中的真实性指标仅达32%与43%，而业界前沿方案的性能表现相近（32%/45%），表明该领域存在显著提升空间。本基准已成为KDD Cup 2025竞赛平台，吸引约1,000名参赛者提交5,000次方案，优胜方案较基线提升28%，彰显其对领域发展的早期推动作用。",
    "url": "https://huggingface.co/papers/2510.26160",
    "arxiv_url": "https://arxiv.org/abs/2510.26160"
  },
  {
    "title": "FullPart: Generating each 3D Part at Full Resolution",
    "summary": "Part-based 3D generation holds great potential for various applications.\nPrevious part generators that represent parts using implicit vector-set tokens\noften suffer from insufficient geometric details. Another line of work adopts\nan explicit voxel representation but shares a global voxel grid among all\nparts; this often causes small parts to occupy too few voxels, leading to\ndegraded quality. In this paper, we propose FullPart, a novel framework that\ncombines both implicit and explicit paradigms. It first derives the bounding\nbox layout through an implicit box vector-set diffusion process, a task that\nimplicit diffusion handles effectively since box tokens contain little\ngeometric detail. Then, it generates detailed parts, each within its own fixed\nfull-resolution voxel grid. Instead of sharing a global low-resolution space,\neach part in our method - even small ones - is generated at full resolution,\nenabling the synthesis of intricate details. We further introduce a\ncenter-point encoding strategy to address the misalignment issue when\nexchanging information between parts of different actual sizes, thereby\nmaintaining global coherence. Moreover, to tackle the scarcity of reliable part\ndata, we present PartVerse-XL, the largest human-annotated 3D part dataset to\ndate with 40K objects and 320K parts. Extensive experiments demonstrate that\nFullPart achieves state-of-the-art results in 3D part generation. We will\nrelease all code, data, and model to benefit future research in 3D part\ngeneration.",
    "translation": "标题：FullPart：全分辨率生成每个三维部件\n\n摘要：基于部件的三维生成技术具有广泛的应用前景。现有部件生成方法主要存在两类局限：一类采用隐式向量集令牌表示部件，往往难以充分刻画几何细节；另一类采用显式体素表征但共享全局体素网格，导致小型部件分配体素过少而生成质量下降。本文提出FullPart这一融合隐式与显式范式的新型框架。该框架首先通过隐式边界框向量集扩散过程推导包围盒布局——由于边界框令牌仅包含基础几何信息，该任务特别适合隐式扩散处理；随后在每个部件独立的固定全分辨率体素网格中生成细节化部件。相较于共享低分辨率全局空间的方法，本方案使每个部件（包括小型部件）均能以全分辨率生成，从而实现复杂几何细节的合成。针对不同尺寸部件间信息交互时的错位问题，我们进一步提出中心点编码策略以保持全局一致性。此外，为解决可靠部件数据稀缺的难题，我们构建了迄今最大规模的人工标注三维部件数据集PartVerse-XL，包含4万个三维对象与32万个部件。大量实验表明，FullPart在三维部件生成任务上达到了最先进的性能水平。我们将公开全部代码、数据与模型，以促进三维部件生成领域的后续研究。",
    "url": "https://huggingface.co/papers/2510.26140",
    "arxiv_url": "https://arxiv.org/abs/2510.26140"
  },
  {
    "title": "PORTool: Tool-Use LLM Training with Rewarded Tree",
    "summary": "Current tool-use large language models (LLMs) are trained on static datasets,\nenabling them to interact with external tools and perform multi-step,\ntool-integrated reasoning, which produces tool-call trajectories. However,\nthese models imitate how a query is resolved in a generic tool-call routine,\nthereby failing to explore possible solutions and demonstrating limited\nperformance in an evolved, dynamic tool-call environment. In this work, we\npropose PORTool, a reinforcement learning (RL) method that encourages a\ntool-use LLM to explore various trajectories yielding the correct answer.\nSpecifically, this method starts with generating multiple rollouts for a given\nquery, and some of them share the first few tool-call steps, thereby forming a\ntree-like structure. Next, we assign rewards to each step, based on its ability\nto produce a correct answer and make successful tool calls. A shared step\nacross different trajectories receives the same reward, while different steps\nunder the same fork receive different rewards. Finally, these step-wise rewards\nare used to calculate fork-relative advantages, blended with\ntrajectory-relative advantages, to train the LLM for tool use. The experiments\nutilize 17 tools to address user queries, covering both time-sensitive and\ntime-invariant topics. We conduct ablation studies to systematically justify\nthe necessity and the design robustness of step-wise rewards. Furthermore, we\ncompare the proposed PORTool with other training approaches and demonstrate\nsignificant improvements in final accuracy and the number of tool-call steps.",
    "translation": "标题：PORTool：基于奖励树结构的工具调用大语言模型训练方法\n\n摘要：当前工具调用大语言模型基于静态数据集进行训练，使其能够与外部工具交互并执行多步骤、工具集成的推理过程，从而生成工具调用轨迹。然而，这些模型仅模仿通用工具调用流程中查询的解决方式，未能探索可能的解决方案，在动态演变的工具调用环境中表现出性能局限。本研究提出PORTool——一种强化学习方法，通过激励工具调用大语言模型探索产生正确答案的多样化轨迹。具体而言，该方法首先生成针对给定查询的多个执行路径，其中部分路径共享初始工具调用步骤，从而形成树状结构。随后基于每个步骤生成正确答案与成功完成工具调用的能力分配奖励：跨轨迹共享步骤获得相同奖励，而同分支下的不同步骤获得差异化奖励。最后，这些逐步骤奖励被用于计算分支相对优势值，并与轨迹相对优势值融合，以训练工具调用大语言模型。实验采用17种工具处理用户查询，涵盖时效性与非时效性主题。通过消融研究系统验证了逐步骤奖励的必要性及设计鲁棒性。进一步将PORTool与其他训练方法对比，在最终准确率和工具调用步骤数量方面均展现出显著提升。\n\n摘要：[当前工具调用大语言模型基于静态数据集进行训练，使其能够与外部工具交互并执行多步骤、工具集成的推理过程，从而生成工具调用轨迹。然而，这些模型仅模仿通用工具调用流程中查询的解决方式，未能探索可能的解决方案，在动态演变的工具调用环境中表现出性能局限。本研究提出PORTool——一种强化学习方法，通过激励工具调用大语言模型探索产生正确答案的多样化轨迹。具体而言，该方法首先生成针对给定查询的多个执行路径，其中部分路径共享初始工具调用步骤，从而形成树状结构。随后基于每个步骤生成正确答案与成功完成工具调用的能力分配奖励：跨轨迹共享步骤获得相同奖励，而同分支下的不同步骤获得差异化奖励。最后，这些逐步骤奖励被用于计算分支相对优势值，并与轨迹相对优势值融合，以训练工具调用大语言模型。实验采用17种工具处理用户查询，涵盖时效性与非时效性主题。通过消融研究系统验证了逐步骤奖励的必要性及设计鲁棒性。进一步将PORTool与其他训练方法对比，在最终准确率和工具调用步骤数量方面均展现出显著提升。]",
    "url": "https://huggingface.co/papers/2510.26020",
    "arxiv_url": "https://arxiv.org/abs/2510.26020"
  },
  {
    "title": "EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme\n  Backbone Generation",
    "summary": "Designing enzyme backbones with substrate-specific functionality is a\ncritical challenge in computational protein engineering. Current generative\nmodels excel in protein design but face limitations in binding data,\nsubstrate-specific control, and flexibility for de novo enzyme backbone\ngeneration. To address this, we introduce EnzyBind, a dataset with 11,100\nexperimentally validated enzyme-substrate pairs specifically curated from\nPDBbind. Building on this, we propose EnzyControl, a method that enables\nfunctional and substrate-specific control in enzyme backbone generation. Our\napproach generates enzyme backbones conditioned on MSA-annotated catalytic\nsites and their corresponding substrates, which are automatically extracted\nfrom curated enzyme-substrate data. At the core of EnzyControl is EnzyAdapter,\na lightweight, modular component integrated into a pretrained motif-scaffolding\nmodel, allowing it to become substrate-aware. A two-stage training paradigm\nfurther refines the model's ability to generate accurate and functional enzyme\nstructures. Experiments show that our EnzyControl achieves the best performance\nacross structural and functional metrics on EnzyBind and EnzyBench benchmarks,\nwith particularly notable improvements of 13\\% in designability and 13\\% in\ncatalytic efficiency compared to the baseline models. The code is released at\nhttps://github.com/Vecteur-libre/EnzyControl.",
    "translation": "标题：EnzyControl：为酶骨架生成添加功能性与底物特异性调控\n\n摘要：设计具有底物特异性功能的酶骨架是计算蛋白质工程领域的关键挑战。当前生成模型在蛋白质设计中表现出色，但在结合数据、底物特异性调控及从头生成酶骨架的灵活性方面存在局限。为此，我们首先构建了EnzyBind数据集，该数据集从PDBbind中精选出11,100个经实验验证的酶-底物对。在此基础上，我们提出EnzyControl方法，实现酶骨架生成过程中的功能性与底物特异性调控。该方法通过从精选酶-底物数据中自动提取的MSA注释催化位点及其对应底物作为条件，生成酶骨架结构。EnzyControl的核心是EnzyAdapter模块——一个集成于预训练基序支架模型中的轻量化可插拔组件，使模型具备底物识别能力。采用两阶段训练范式进一步优化模型生成精确功能性酶结构的能力。实验表明，在EnzyBind和EnzyBench基准测试中，我们的EnzyControl在结构与功能指标上均取得最佳性能，其中可设计性提升13%，催化效率较基线模型提高13%。代码已发布于https://github.com/Vecteur-libre/EnzyControl。",
    "url": "https://huggingface.co/papers/2510.25132",
    "arxiv_url": "https://arxiv.org/abs/2510.25132"
  },
  {
    "title": "CLASS-IT: Conversational and Lecture-Aligned Small-Scale Instruction\n  Tuning for BabyLMs",
    "summary": "This work investigates whether small-scale LMs can benefit from instruction\ntuning. We compare conversational and question-answering instruction tuning\ndatasets, applied either in a merged or sequential curriculum, using\ndecoder-only models with 100M and 140M parameters. Evaluation spans both\nfine-tuning (SuperGLUE) and zero-shot (BLiMP, EWoK, WUGs, entity tracking, and\npsycholinguistic correlation) settings. Results show that instruction tuning\nyields small but consistent gains in fine-tuning scenarios, with sequential\ncurricula outperforming merged data; however, improvements do not consistently\ntransfer to zero-shot tasks, suggesting a trade-off between interaction-focused\nadaptation and broad linguistic generalization. These results highlight both\nthe potential and the constraints of adapting human-inspired learning\nstrategies to low-resource LMs, and point toward hybrid, curriculum-based\napproaches for enhancing generalization under ecological training limits.",
    "translation": "标题：CLASS-IT：面向BabyLMs的对话式与讲座对齐小规模指令微调框架\n\n摘要：本研究探讨小规模语言模型能否从指令微调中获益。我们分别采用融合式与序列课程式两种策略，对比分析了对话型与问答型指令微调数据集在1亿/1.4亿参数解码器模型上的表现。评估涵盖精调（SuperGLUE）与零样本（BLiMP/EWoK/WUGs/实体追踪/心理语言学关联）双重场景。实验结果表明：指令微调在精调场景中能产生持续但有限的性能提升，其中序列课程式训练优于数据融合策略；然而这些改进无法稳定迁移至零样本任务，揭示了交互导向适应与广义语言泛化之间的权衡关系。这些发现既印证了将人类启发式学习策略应用于低资源语言模型的潜力，也凸显了其局限性，为在生态化训练约束下通过混合式课程学习方法增强模型泛化能力指明了方向。",
    "url": "https://huggingface.co/papers/2510.25364",
    "arxiv_url": "https://arxiv.org/abs/2510.25364"
  },
  {
    "title": "CityRiSE: Reasoning Urban Socio-Economic Status in Vision-Language\n  Models via Reinforcement Learning",
    "summary": "Harnessing publicly available, large-scale web data, such as street view and\nsatellite imagery, urban socio-economic sensing is of paramount importance for\nachieving global sustainable development goals. With the emergence of Large\nVision-Language Models (LVLMs), new opportunities have arisen to solve this\ntask by treating it as a multi-modal perception and understanding problem.\nHowever, recent studies reveal that LVLMs still struggle with accurate and\ninterpretable socio-economic predictions from visual data. To address these\nlimitations and maximize the potential of LVLMs, we introduce\nCityRiSE, a novel framework for Reasoning urban\nSocio-Economic status in LVLMs through pure reinforcement\nlearning (RL). With carefully curated multi-modal data and verifiable reward\ndesign, our approach guides the LVLM to focus on semantically meaningful visual\ncues, enabling structured and goal-oriented reasoning for generalist\nsocio-economic status prediction. Experiments demonstrate that CityRiSE with\nemergent reasoning process significantly outperforms existing baselines,\nimproving both prediction accuracy and generalization across diverse urban\ncontexts, particularly for prediction on unseen cities and unseen indicators.\nThis work highlights the promise of combining RL and LVLMs for interpretable\nand generalist urban socio-economic sensing.",
    "translation": "标题：CityRiSE：基于强化学习的视觉语言模型城市社会经济地位推理框架\n\n摘要：利用街景图像和卫星影像等公开可获取的大规模网络数据，城市社会经济感知对于实现全球可持续发展目标具有至关重要的意义。随着大规模视觉语言模型（LVLMs）的出现，通过将其视为多模态感知与理解问题，为解决这一任务创造了新的机遇。然而，近期研究表明，LVLMs在基于视觉数据实现准确且可解释的社会经济预测方面仍存在困难。为突破这些限制并充分释放LVLMs的潜力，我们提出CityRiSE——一种通过纯强化学习（RL）在LVLMs中实现城市社会经济地位推理的创新框架。通过精心构建的多模态数据和可验证的奖励机制设计，我们的方法引导LVLM聚焦于具有语义意义的视觉线索，实现面向通用社会经济预测的结构化目标导向推理。实验表明，具有涌现推理能力的CityRiSE显著优于现有基线模型，在预测精度和跨城市泛化能力方面均实现提升，特别是在未见过的城市和未接触过的指标预测任务中表现突出。本研究彰显了强化学习与LVLMs相结合在可解释通用型城市社会经济感知领域的应用前景。",
    "url": "https://huggingface.co/papers/2510.22282",
    "arxiv_url": "https://arxiv.org/abs/2510.22282"
  },
  {
    "title": "Performance Trade-offs of Optimizing Small Language Models for\n  E-Commerce",
    "summary": "Large Language Models (LLMs) offer state-of-the-art performance in natural\nlanguage understanding and generation tasks. However, the deployment of leading\ncommercial models for specialized tasks, such as e-commerce, is often hindered\nby high computational costs, latency, and operational expenses. This paper\ninvestigates the viability of smaller, open-weight models as a\nresource-efficient alternative. We present a methodology for optimizing a\none-billion-parameter Llama 3.2 model for multilingual e-commerce intent\nrecognition. The model was fine-tuned using Quantized Low-Rank Adaptation\n(QLoRA) on a synthetically generated dataset designed to mimic real-world user\nqueries. Subsequently, we applied post-training quantization techniques,\ncreating GPU-optimized (GPTQ) and CPU-optimized (GGUF) versions. Our results\ndemonstrate that the specialized 1B model achieves 99% accuracy, matching the\nperformance of the significantly larger GPT-4.1 model. A detailed performance\nanalysis revealed critical, hardware-dependent trade-offs: while 4-bit GPTQ\nreduced VRAM usage by 41%, it paradoxically slowed inference by 82% on an older\nGPU architecture (NVIDIA T4) due to dequantization overhead. Conversely, GGUF\nformats on a CPU achieved a speedup of up to 18x in inference throughput and a\nreduction of over 90% in RAM consumption compared to the FP16 baseline. We\nconclude that small, properly optimized open-weight models are not just a\nviable but a more suitable alternative for domain-specific applications,\noffering state-of-the-art accuracy at a fraction of the computational cost.",
    "translation": "标题：面向电子商务的小型语言模型优化性能权衡研究\n\n摘要：大语言模型在自然语言理解与生成任务中展现出顶尖性能。然而，在电子商务等专业领域部署主流商业模型时，常受限于高计算成本、延迟问题及运营开支。本文探讨了小型开放权重模型作为资源高效替代方案的可行性。我们提出了一套针对十亿参数规模Llama 3.2模型进行多语言电商意图识别优化的方法体系。该模型通过量化低秩自适应技术，在模拟真实用户查询的合成数据集上完成微调，继而采用训练后量化方案分别生成GPU优化（GPTQ）与CPU优化（GGUF）版本。实验结果表明：专用化的十亿参数模型达到99%准确率，与规模显著更大的GPT-4.1模型性能持平。深度性能分析揭示了关键硬件依赖权衡：4位GPTQ虽降低41%显存占用，但在旧版GPU架构（英伟达T4）上因反量化开销导致推理速度反而降低82%；相较之下，CPU环境运行的GGUF格式相较FP16基线实现了18倍推理吞吐量提升与超90%内存占用削减。我们得出结论：经过恰当优化的开放权重小型模型不仅是可行的领域专用替代方案，更是更优选择，能够以极低计算成本实现顶尖精度。",
    "url": "https://huggingface.co/papers/2510.21970",
    "arxiv_url": "https://arxiv.org/abs/2510.21970"
  },
  {
    "title": "L^2M^3OF: A Large Language Multimodal Model for Metal-Organic Frameworks",
    "summary": "Large language models have demonstrated remarkable reasoning capabilities\nacross diverse natural language tasks. However, comparable breakthroughs in\nscientific discovery are more limited, because understanding complex physical\nphenomena demands multifaceted representations far beyond language alone. A\ncompelling example is the design of functional materials such as MOFs-critical\nfor a range of impactful applications like carbon capture and hydrogen storage.\nNavigating their vast and intricate design space in language-based\nrepresentations interpretable by LLMs is challenging due to the numerous\npossible three-dimensional atomic arrangements and strict reticular rules of\ncoordination geometry and topology. Despite promising early results in\nLLM-assisted discovery for simpler materials systems, MOF design remains\nheavily reliant on tacit human expertise rarely codified in textual information\nalone. To overcome this barrier, we introduce L2M3OF, the first multimodal LLM\nfor MOFs. L2M3OF integrates crystal representation learning with language\nunderstanding to process structural, textual, and knowledge modalities jointly.\nL2M3OF employs a pre-trained crystal encoder with a lightweight projection\nlayer to compress structural information into a token space, enabling efficient\nalignment with language instructions. To facilitate training and evaluation, we\ncurate a structure-property-knowledge database of crystalline materials and\nbenchmark L2M3OF against state-of-the-art closed-source LLMs such as GPT-5,\nGemini-2.5-Pro and DeepSeek-R1. Experiments show that L2M3OF outperforms\nleading text-based closed-source LLMs in property prediction and knowledge\ngeneration tasks, despite using far fewer parameters. These results highlight\nthe importance of multimodal approaches for porous material understanding and\nestablish L2M3OF as a foundation for next-generation AI systems in materials\ndiscovery.",
    "translation": "标题：L²M³OF：面向金属有机框架的大语言多模态模型\n\n摘要：大语言模型在多样化自然语言任务中展现出卓越的推理能力，但在科学发现领域的可比性突破仍较为有限，这源于理解复杂物理现象需要远超纯语言范畴的多维度表征。金属有机框架材料的设计即为典型例证——这类对碳捕集、储氢等重要应用至关重要的功能材料，因其存在海量可能的三维原子排列方式及严格的配位几何与拓扑网络规则，难以通过LLMs可解读的语言化表征来驾驭其庞大复杂的设计空间。尽管LLM辅助设计在简单材料体系中已取得早期成果，MOF设计仍高度依赖人类隐性经验，这些知识鲜少以纯文本形式记载。为突破此限制，我们提出首个面向MOFs的多模态大语言模型L²M³OF。该模型通过晶体表征学习与语言理解相融合，实现了结构、文本与知识模态的联合处理。L²M³OF采用预训练晶体编码器与轻量化投影层，将结构信息压缩至令牌空间，从而实现与语言指令的高效对齐。为促进训练与评估，我们构建了晶体材料的结构-性质-知识数据库，并以GPT-5、Gemini-2.5-Pro和DeepSeek-R1等顶尖闭源LLMs为基准进行性能对比。实验表明，在参数量大为减少的情况下，L²M³OF在性质预测与知识生成任务中仍显著优于主流文本型闭源LLMs。这些成果凸显了多模态方法在多孔材料认知中的关键价值，确立了L²M³OF作为新一代材料发现人工智能系统的基础地位。",
    "url": "https://huggingface.co/papers/2510.20976",
    "arxiv_url": "https://arxiv.org/abs/2510.20976"
  },
  {
    "title": "ChartAB: A Benchmark for Chart Grounding & Dense Alignment",
    "summary": "Charts play an important role in visualization, reasoning, data analysis, and\nthe exchange of ideas among humans. However, existing vision-language models\n(VLMs) still lack accurate perception of details and struggle to extract\nfine-grained structures from charts. Such limitations in chart grounding also\nhinder their ability to compare multiple charts and reason over them. In this\npaper, we introduce a novel \"ChartAlign Benchmark (ChartAB)\" to provide a\ncomprehensive evaluation of VLMs in chart grounding tasks, i.e., extracting\ntabular data, localizing visualization elements, and recognizing various\nattributes from charts of diverse types and complexities. We design a JSON\ntemplate to facilitate the calculation of evaluation metrics specifically\ntailored for each grounding task. By incorporating a novel two-stage inference\nworkflow, the benchmark can further evaluate VLMs' capability to align and\ncompare elements/attributes across two charts. Our analysis of evaluations on\nseveral recent VLMs reveals new insights into their perception biases,\nweaknesses, robustness, and hallucinations in chart understanding. These\nfindings highlight the fine-grained discrepancies among VLMs in chart\nunderstanding tasks and point to specific skills that need to be strengthened\nin current models.",
    "translation": "标题：ChartAB：图表定位与密集对齐基准测试系统\n\n摘要：图表在可视化呈现、逻辑推理、数据分析和人类思想交流中发挥着重要作用。然而，现有的视觉语言模型在细节感知方面仍存在不足，难以从图表中提取细粒度结构。这种图表定位能力的局限也制约了模型在多图表对比和推理方面的表现。本文提出创新的\"ChartAlign基准测试系统（ChartAB）\"，通过涵盖不同类型和复杂度的图表，对视觉语言模型在表格数据提取、可视化元素定位和多重属性识别等图表定位任务中进行全面评估。我们设计了专用JSON模板以实现针对各项定位任务的定制化评估指标计算。通过引入新型两阶段推理机制，该基准系统能进一步评估视觉语言模型在跨图表元素/属性对齐与比较方面的能力。基于对多个前沿视觉语言模型的评估分析，我们揭示了其在图表理解任务中存在的感知偏差、能力缺陷、鲁棒性不足和幻觉现象等新发现。这些研究结果不仅凸显了不同视觉语言模型在图表理解任务中存在的细粒度差异，更为当前模型指明了需要重点加强的能力维度。",
    "url": "https://huggingface.co/papers/2510.26781",
    "arxiv_url": "https://arxiv.org/abs/2510.26781"
  },
  {
    "title": "POWSM: A Phonetic Open Whisper-Style Speech Foundation Model",
    "summary": "Recent advances in spoken language processing have led to substantial\nprogress in phonetic tasks such as automatic speech recognition (ASR), phone\nrecognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme\nconversion (P2G). Despite their conceptual similarity, these tasks have largely\nbeen studied in isolation, each relying on task-specific architectures and\ndatasets. In this paper, we introduce POWSM (Phonetic Open Whisper-style Speech\nModel), the first unified framework capable of jointly performing multiple\nphone-related tasks. POWSM enables seamless conversion between audio, text\n(graphemes), and phones, opening up new possibilities for universal and\nlow-resource speech processing. Our model outperforms or matches specialized PR\nmodels of similar size (Wav2Vec2Phoneme and ZIPA) while jointly supporting G2P,\nP2G, and ASR. Our training data, code and models are released to foster open\nscience.",
    "translation": "标题：POWSM：语音开放式耳语风格语音基础模型\n\n摘要：近期口语处理领域的突破性进展显著推动了语音任务的发展，例如自动语音识别（ASR）、音素识别（PR）、字形到音素转换（G2P）以及音素到字形转换（P2G）。尽管这些任务在概念上具有相似性，但现有研究大多孤立进行，分别依赖特定任务架构与专用数据集。本文提出POWSM（语音开放式耳语风格语音模型），这是首个能够联合执行多种音素相关任务的统一框架。该模型实现了音频、文本（字形）与音素间的无缝转换，为通用化与低资源语音处理开辟了新路径。实验表明，本模型在保持相似参数量级的前提下，其性能超越或匹配专用音素识别模型（Wav2Vec2Phoneme与ZIPA），同时联合支持G2P、P2G与ASR任务。我们已公开训练数据、代码与模型，以促进开放科学发展。",
    "url": "https://huggingface.co/papers/2510.24992",
    "arxiv_url": "https://arxiv.org/abs/2510.24992"
  }
]