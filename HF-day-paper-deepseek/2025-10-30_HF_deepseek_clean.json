[
  {
    "title": "Video-Thinker: Sparking \"Thinking with Videos\" via Reinforcement\n  Learning",
    "summary": "Recent advances in image reasoning methods, particularly \"Thinking with\nImages\", have demonstrated remarkable success in Multimodal Large Language\nModels (MLLMs); however, this dynamic reasoning paradigm has not yet been\nextended to video reasoning tasks. In this paper, we propose Video-Thinker,\nwhich empowers MLLMs to think with videos by autonomously leveraging their\nintrinsic \"grounding\" and \"captioning\" capabilities to generate reasoning clues\nthroughout the inference process. To spark this capability, we construct\nVideo-Thinker-10K, a curated dataset featuring autonomous tool usage within\nchain-of-thought reasoning sequences. Our training strategy begins with\nSupervised Fine-Tuning (SFT) to learn the reasoning format, followed by Group\nRelative Policy Optimization (GRPO) to strengthen this reasoning capability.\nThrough this approach, Video-Thinker enables MLLMs to autonomously navigate\ngrounding and captioning tasks for video reasoning, eliminating the need for\nconstructing and calling external tools. Extensive experiments demonstrate that\nVideo-Thinker achieves significant performance gains on both in-domain tasks\nand challenging out-of-domain video reasoning benchmarks, including\nVideo-Holmes, CG-Bench-Reasoning, and VRBench. Our Video-Thinker-7B\nsubstantially outperforms existing baselines such as Video-R1 and establishes\nstate-of-the-art performance among 7B-sized MLLMs.",
    "translation": "标题：Video-Thinker：通过强化学习实现“视频思维”的突破\n\n摘要：近期图像推理方法（尤其是“图像思维”范式）在多模态大语言模型中取得显著成功，然而这种动态推理范式尚未扩展至视频推理任务。本文提出Video-Thinker模型，通过自主利用多模态大语言模型固有的“定位”与“描述”能力，在推理过程中持续生成思维线索，实现视频思维推理。为激发此能力，我们构建了Video-Thinker-10K数据集，该精选数据集包含思维链推理序列中的自主工具使用记录。训练策略首先采用监督微调学习推理格式，继而通过分组相对策略优化强化推理能力。该方法使多模态大语言模型能自主执行视频推理中的定位与描述任务，无需构建和调用外部工具。大量实验表明，Video-Thinker在领域内任务及具有挑战性的领域外视频推理基准（包括Video-Holmes、CG-Bench-Reasoning和VRBench）上均取得显著性能提升。我们的Video-Thinker-7B模型显著超越Video-R1等现有基线，在70亿参数规模的多模态大语言模型中确立了最先进的性能表现。",
    "url": "https://huggingface.co/papers/2510.23473",
    "arxiv_url": "https://arxiv.org/abs/2510.23473"
  },
  {
    "title": "JanusCoder: Towards a Foundational Visual-Programmatic Interface for\n  Code Intelligence",
    "summary": "The scope of neural code intelligence is rapidly expanding beyond text-based\nsource code to encompass the rich visual outputs that programs generate. This\nvisual dimension is critical for advanced applications like flexible content\ngeneration and precise, program-driven editing of visualizations. However,\nprogress has been impeded by the scarcity of high-quality multimodal code data,\na bottleneck stemming from challenges in synthesis and quality assessment. To\naddress these challenges, we make contributions from both a data and modeling\nperspective. We first introduce a complete synthesis toolkit that leverages\nreciprocal synergies between data modalities to efficiently produce a\nlarge-scale, high-quality corpus spanning from standard charts to complex\ninteractive web UIs and code-driven animations. Leveraging this toolkit, we\nconstruct JanusCode-800K, the largest multimodal code corpus to date. This\npowers the training of our models, JanusCoder and JanusCoderV, which establish\na visual-programmatic interface for generating code from textual instructions,\nvisual inputs, or a combination of both. Our unified model is a departure from\nexisting approaches that build specialized models for isolated tasks. Extensive\nexperiments on both text-centric and vision-centric coding tasks demonstrate\nthe superior performance of the JanusCoder series, with our 7B to 14B scale\nmodels approaching or even exceeding the performance of commercial models.\nFurthermore, extensive analysis provides key insights into harmonizing\nprogrammatic logic with its visual expression. Our code and checkpoints will\nare available at https://github.com/InternLM/JanusCoder.",
    "translation": "标题：JanusCoder：构建面向代码智能的基础视觉-编程交互框架\n\n摘要：神经代码智能的研究范畴正从基于文本的源代码快速扩展至程序生成的丰富视觉输出领域。这种视觉维度对于灵活内容生成和可视化作品的精确程序化编辑等高级应用至关重要。然而，高质量多模态代码数据的稀缺阻碍了研究进展，这一瓶颈源于数据合成与质量评估的双重挑战。为应对这些挑战，我们从数据构建与模型设计两个维度做出贡献：首先提出了一套完整的合成工具包，通过利用数据模态间的协同效应，高效构建了涵盖标准图表、复杂交互式网页界面及代码驱动动画的大规模高质量语料库。基于该工具包，我们构建了迄今规模最大的多模态代码数据集JanusCode-800K。依托这一数据集，我们训练了JanusCoder与JanusCoderV系列模型，建立起支持从文本指令、视觉输入或其组合生成代码的视觉-编程交互接口。我们的统一模型突破了现有方法为孤立任务构建专用模型的局限。在文本主导与视觉主导的编码任务上的大量实验表明，JanusCoder系列表现出卓越性能，其70亿至140亿参数规模的模型在多项指标上接近甚至超越商业模型。此外，深入分析为协调程序逻辑与视觉表达提供了关键见解。我们的代码与模型参数已开源：https://github.com/InternLM/JanusCoder。",
    "url": "https://huggingface.co/papers/2510.23538",
    "arxiv_url": "https://arxiv.org/abs/2510.23538"
  },
  {
    "title": "RegionE: Adaptive Region-Aware Generation for Efficient Image Editing",
    "summary": "Recently, instruction-based image editing (IIE) has received widespread\nattention. In practice, IIE often modifies only specific regions of an image,\nwhile the remaining areas largely remain unchanged. Although these two types of\nregions differ significantly in generation difficulty and computational\nredundancy, existing IIE models do not account for this distinction, instead\napplying a uniform generation process across the entire image. This motivates\nus to propose RegionE, an adaptive, region-aware generation framework that\naccelerates IIE tasks without additional training. Specifically, the RegionE\nframework consists of three main components: 1) Adaptive Region Partition. We\nobserved that the trajectory of unedited regions is straight, allowing for\nmulti-step denoised predictions to be inferred in a single step. Therefore, in\nthe early denoising stages, we partition the image into edited and unedited\nregions based on the difference between the final estimated result and the\nreference image. 2) Region-Aware Generation. After distinguishing the regions,\nwe replace multi-step denoising with one-step prediction for unedited areas.\nFor edited regions, the trajectory is curved, requiring local iterative\ndenoising. To improve the efficiency and quality of local iterative generation,\nwe propose the Region-Instruction KV Cache, which reduces computational cost\nwhile incorporating global information. 3) Adaptive Velocity Decay Cache.\nObserving that adjacent timesteps in edited regions exhibit strong velocity\nsimilarity, we further propose an adaptive velocity decay cache to accelerate\nthe local denoising process. We applied RegionE to state-of-the-art IIE base\nmodels, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE\nachieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o\nconfirmed that semantic and perceptual fidelity were well preserved.",
    "translation": "标题：RegionE：面向高效图像编辑的自适应区域感知生成框架\n\n摘要：近年来，基于指令的图像编辑技术受到广泛关注。实际应用中，此类编辑通常仅需修改图像的特定区域，而其他区域基本保持不变。尽管这两类区域在生成难度和计算冗余度上存在显著差异，现有方法却未考虑这种区别，而是对整个图像采用统一的生成流程。为此，我们提出RegionE——一种无需额外训练的自适应区域感知生成框架，可显著加速图像编辑任务。该框架包含三个核心组件：1）自适应区域划分。通过观察发现未编辑区域的生成轨迹呈直线特性，支持通过单步推理实现多步去噪预测。因此在去噪早期阶段，我们根据最终预估结果与参考图像的差异将图像划分为编辑区域和未编辑区域；2）区域感知生成。完成区域划分后，对未编辑区域用单步预测替代多步去噪；对于轨迹呈弯曲特性的编辑区域，则采用局部迭代去噪。为提升局部迭代生成的效率与质量，我们提出区域指令键值缓存技术，在融入全局信息的同时降低计算成本；3）自适应速度衰减缓存。通过观测发现编辑区域相邻时间步的速度向量具有强相关性，我们进一步提出自适应速度衰减缓存机制来加速局部去噪过程。将RegionE应用于Step1X-Edit、FLUX.1 Kontext和Qwen-Image-Edit等前沿基础模型后，分别实现了2.57倍、2.41倍和2.06倍的加速效果。经GPT-4o评估验证，该方法在保持语义一致性和视觉保真度方面均有优异表现。",
    "url": "https://huggingface.co/papers/2510.25590",
    "arxiv_url": "https://arxiv.org/abs/2510.25590"
  },
  {
    "title": "Scaling Latent Reasoning via Looped Language Models",
    "summary": "Modern LLMs are trained to \"think\" primarily via explicit text generation,\nsuch as chain-of-thought (CoT), which defers reasoning to post-training and\nunder-leverages pre-training data. We present and open-source Ouro, named after\nthe recursive Ouroboros, a family of pre-trained Looped Language Models\n(LoopLM) that instead build reasoning into the pre-training phase through (i)\niterative computation in latent space, (ii) an entropy-regularized objective\nfor learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and\n2.6B models enjoy superior performance that match the results of up to 12B SOTA\nLLMs across a wide range of benchmarks. Through controlled experiments, we show\nthis advantage stems not from increased knowledge capacity, but from superior\nknowledge manipulation capabilities. We also show that LoopLM yields reasoning\ntraces more aligned with final outputs than explicit CoT. We hope our results\nshow the potential of LoopLM as a novel scaling direction in the reasoning era.\nOur model could be found in: http://ouro-llm.github.io.",
    "translation": "标题：基于循环语言模型的潜在推理规模化研究\n\n摘要：现代大语言模型主要通过显式文本生成（如思维链）进行\"思考\"训练，这种方式将推理过程推迟至训练后阶段，未能充分利用预训练数据。我们提出并开源了以递归衔尾蛇命名的Ouro模型系列——基于预训练的循环语言模型，通过以下方式将推理能力构建于预训练阶段：（i）潜在空间中的迭代计算，（ii）基于熵正则化的深度分配目标函数，（iii）扩展至7.7万亿训练令牌。Ouro 1.4B和2.6B模型在广泛基准测试中表现出卓越性能，其效果可媲美当前最优的120亿参数大语言模型。通过受控实验，我们证明该优势并非源于知识容量的提升，而是来自更卓越的知识操纵能力。研究还表明，与显式思维链相比，循环语言模型生成的推理轨迹与最终输出具有更高一致性。我们的研究成果揭示了循环语言模型作为推理时代新型扩展方向的潜力。模型获取地址：http://ouro-llm.github.io。",
    "url": "https://huggingface.co/papers/2510.25741",
    "arxiv_url": "https://arxiv.org/abs/2510.25741"
  },
  {
    "title": "The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic,\n  and Long-Horizon Task Execution",
    "summary": "Real-world language agents must handle complex, multi-step workflows across\ndiverse Apps. For instance, an agent may manage emails by coordinating with\ncalendars and file systems, or monitor a production database to detect\nanomalies and generate reports following an operating manual. However, existing\nlanguage agent benchmarks often focus on narrow domains or simplified tasks\nthat lack the diversity, realism, and long-horizon complexity required to\nevaluate agents' real-world performance. To address this gap, we introduce the\nTool Decathlon (dubbed as Toolathlon), a benchmark for language agents offering\ndiverse Apps and tools, realistic environment setup, and reliable\nexecution-based evaluation. Toolathlon spans 32 software applications and 604\ntools, ranging from everyday platforms such as Google Calendar and Notion to\nprofessional ones like WooCommerce, Kubernetes, and BigQuery. Most of the tools\nare based on a high-quality set of Model Context Protocol (MCP) servers that we\nmay have revised or implemented ourselves. Unlike prior works, which primarily\nensure functional realism but offer limited environment state diversity, we\nprovide realistic initial environment states from real software, such as Canvas\ncourses with dozens of students or real financial spreadsheets. This benchmark\nincludes 108 manually sourced or crafted tasks in total, requiring interacting\nwith multiple Apps over around 20 turns on average to complete. Each task is\nstrictly verifiable through dedicated evaluation scripts. Comprehensive\nevaluation of SOTA models highlights their significant shortcomings: the\nbest-performing model, Claude-4.5-Sonnet, achieves only a 38.6% success rate\nwith 20.2 tool calling turns on average, while the top open-weights model\nDeepSeek-V3.2-Exp reaches 20.1%. We expect Toolathlon to drive the development\nof more capable language agents for real-world, long-horizon task execution.",
    "translation": "标题：工具十项全能：面向多样化、真实化及长周期任务执行的语言智能体基准测试\n\n摘要：现实世界中的语言智能体必须能够处理跨多样应用程序的复杂多步骤工作流。例如，智能体可能需要通过协调日历和文件系统来管理电子邮件，或根据操作手册监控生产数据库以检测异常并生成报告。然而，现有语言智能体基准测试往往局限于狭窄领域或简化任务，缺乏评估智能体真实性能所需的多样性、真实性和长周期复杂性。为弥补这一空白，我们推出工具十项全能（Toolathlon）基准测试，该测试通过提供多样化应用程序与工具、真实环境设置及可靠的执行评估机制，全面衡量语言智能体能力。Toolathlon涵盖32个软件应用和604种工具，范围从日常平台（如Google日历、Notion）到专业系统（如WooCommerce、Kubernetes和BigQuery）。大部分工具基于我们修订或自主实现的高质量模型上下文协议（MCP）服务器构建。与既往主要确保功能真实性但环境状态多样性有限的研究不同，我们提供源自真实软件的初始环境状态，例如包含数十名学生的Canvas课程系统或真实财务报表。该基准测试共包含108项人工采集或精心设计的任务，平均需要跨多个应用程序进行约20轮交互才能完成。每项任务均可通过专用评估脚本进行严格验证。对前沿模型的综合评估揭示了其显著缺陷：表现最佳的Claude-4.5-Sonnet模型成功率仅达38.6%，平均需要20.2次工具调用；而表现最优的开源模型DeepSeek-V3.2-Exp成功率仅为20.1%。我们期待Toolathlon能推动面向现实世界长周期任务执行的更强大语言智能体的发展。",
    "url": "https://huggingface.co/papers/2510.25726",
    "arxiv_url": "https://arxiv.org/abs/2510.25726"
  },
  {
    "title": "Reasoning-Aware GRPO using Process Mining",
    "summary": "Reinforcement learning (RL)-based post-training has been crucial for enabling\nmulti-step reasoning in large reasoning models (LRMs), yet current reward\nschemes are typically outcome-centric. We propose PM4GRPO, a reasoning-aware\nGroup Relative Policy Optimization (GRPO) that augments standard answer/format\nrewards with signals over the reasoning procedure. To this end, process mining\ntechniques are utilized to compute a scalar conformance reward that measures\nhow closely a policy model's reasoning aligns with the pretrained teacher\nmodel. The empirical results on five benchmarks demonstrate that PM4GRPO\nsignificantly outperforms existing methodologies for GRPO-based post-training.\nThese results highlight that leveraging process mining for reasoning-aware GRPO\neffectively enhances the reasoning capabilities of policy models.",
    "translation": "标题：基于过程挖掘的推理感知型GRPO方法研究\n\n摘要：基于强化学习(RL)的后期训练对于实现大型推理模型(LRMs)的多步推理能力至关重要，然而现有的奖励机制通常以结果为中心。本文提出PM4GRPO方法，这是一种推理感知型群体相对策略优化(GRPO)框架，通过在标准答案/格式奖励基础上融入推理过程的评估信号。该方法创新性地运用过程挖掘技术计算标量一致性奖励，用以量化策略模型推理过程与预训练教师模型的契合程度。在五个基准测试上的实证结果表明，PM4GRPO显著优于现有基于GRPO的后期训练方法。这些发现充分证明，利用过程挖掘技术实现推理感知的GRPO能有效增强策略模型的推理能力。",
    "url": "https://huggingface.co/papers/2510.25065",
    "arxiv_url": "https://arxiv.org/abs/2510.25065"
  },
  {
    "title": "VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context\n  Learning",
    "summary": "Visual effects (VFX) are crucial to the expressive power of digital media,\nyet their creation remains a major challenge for generative AI. Prevailing\nmethods often rely on the one-LoRA-per-effect paradigm, which is\nresource-intensive and fundamentally incapable of generalizing to unseen\neffects, thus limiting scalability and creation. To address this challenge, we\nintroduce VFXMaster, the first unified, reference-based framework for VFX video\ngeneration. It recasts effect generation as an in-context learning task,\nenabling it to reproduce diverse dynamic effects from a reference video onto\ntarget content. In addition, it demonstrates remarkable generalization to\nunseen effect categories. Specifically, we design an in-context conditioning\nstrategy that prompts the model with a reference example. An in-context\nattention mask is designed to precisely decouple and inject the essential\neffect attributes, allowing a single unified model to master the effect\nimitation without information leakage. In addition, we propose an efficient\none-shot effect adaptation mechanism to boost generalization capability on\ntough unseen effects from a single user-provided video rapidly. Extensive\nexperiments demonstrate that our method effectively imitates various categories\nof effect information and exhibits outstanding generalization to out-of-domain\neffects. To foster future research, we will release our code, models, and a\ncomprehensive dataset to the community.",
    "translation": "标题：VFXMaster：通过上下文学习解锁动态视觉特效生成\n\n摘要：视觉特效（VFX）对数字媒体的表现力至关重要，但其创作对生成式人工智能仍构成重大挑战。现有方法通常采用\"单一特效对应单一LoRA\"的范式，这种模式不仅资源消耗大，且本质上无法泛化至未见过的特效类型，从而限制了系统的可扩展性与创作能力。为应对这一挑战，我们提出VFXMaster——首个基于参考视频的统一视觉特效生成框架。该框架将特效生成重构为上下文学习任务，使其能够将参考视频中的多样化动态特效迁移至目标内容。此外，该系统展现出对未知特效类别的卓越泛化能力。具体而言，我们设计了上下文条件策略，通过参考样本对模型进行提示；同时开发了上下文注意力掩码机制，可精准解耦并注入关键特效属性，使统一模型在避免信息泄露的前提下实现特效模仿。我们还提出高效的单样本特效自适应机制，能够基于用户提供的单段视频快速提升对复杂未知特效的泛化能力。大量实验表明，本方法能有效模仿多类别特效信息，并对领域外特效表现出优异的泛化性能。为促进后续研究，我们将向社区公开代码、模型及完整数据集。",
    "url": "https://huggingface.co/papers/2510.25772",
    "arxiv_url": "https://arxiv.org/abs/2510.25772"
  },
  {
    "title": "ReForm: Reflective Autoformalization with Prospective Bounded Sequence\n  Optimization",
    "summary": "Autoformalization, which translates natural language mathematics into\nmachine-verifiable formal statements, is critical for using formal mathematical\nreasoning to solve math problems stated in natural language. While Large\nLanguage Models can generate syntactically correct formal statements, they\noften fail to preserve the original problem's semantic intent. This limitation\narises from the LLM approaches' treating autoformalization as a simplistic\ntranslation task which lacks mechanisms for self-reflection and iterative\nrefinement that human experts naturally employ. To address these issues, we\npropose ReForm, a Reflective Autoformalization method that tightly integrates\nsemantic consistency evaluation into the autoformalization process. This\nenables the model to iteratively generate formal statements, assess its\nsemantic fidelity, and self-correct identified errors through progressive\nrefinement. To effectively train this reflective model, we introduce\nProspective Bounded Sequence Optimization (PBSO), which employs different\nrewards at different sequence positions to ensure that the model develops both\naccurate autoformalization and correct semantic validations, preventing\nsuperficial critiques that would undermine the purpose of reflection. Extensive\nexperiments across four autoformalization benchmarks demonstrate that ReForm\nachieves an average improvement of 17.2 percentage points over the strongest\nbaselines. To further ensure evaluation reliability, we introduce\nConsistencyCheck, a benchmark of 859 expert-annotated items that not only\nvalidates LLMs as judges but also reveals that autoformalization is inherently\ndifficult: even human experts produce semantic errors in up to 38.5% of cases.",
    "translation": "标题：ReForm：基于前瞻有界序列优化的反射式自动形式化方法\n\n摘要：自动形式化旨在将自然语言数学表述转化为机器可验证的形式化陈述，对于运用形式化数学推理解决自然语言数学问题具有关键意义。尽管大语言模型能够生成语法正确的形式化陈述，但往往难以保持原始问题的语义意图。这一局限源于现有方法将自动形式化简单视为翻译任务，缺乏人类专家自然运用的自我反思与迭代优化机制。为解决这些问题，我们提出ReForm——一种反射式自动形式化方法，通过将语义一致性评估深度集成至形式化过程，使模型能够迭代生成形式化陈述、评估语义保真度，并通过渐进优化实现自我纠错。为有效训练该反射模型，我们提出前瞻有界序列优化方法，通过在序列不同位置采用差异化奖励机制，确保模型既实现精准的形式化转换又完成正确的语义验证，避免产生削弱反思价值的表面化评判。在四个自动形式化基准测试上的大量实验表明，ReForm相较最强基线模型平均提升17.2个百分点。为进一步确保评估可靠性，我们构建了ConsistencyCheck基准数据集，包含859个经专家标注的样本项。该数据集不仅验证了大语言模型作为评估者的有效性，还揭示出自动形式化本身固有的困难性：即使人类专家在最高38.5%的情况下也会产生语义错误。",
    "url": "https://huggingface.co/papers/2510.24592",
    "arxiv_url": "https://arxiv.org/abs/2510.24592"
  },
  {
    "title": "Rethinking Driving World Model as Synthetic Data Generator for\n  Perception Tasks",
    "summary": "Recent advancements in driving world models enable controllable generation of\nhigh-quality RGB videos or multimodal videos. Existing methods primarily focus\non metrics related to generation quality and controllability. However, they\noften overlook the evaluation of downstream perception tasks, which are\nreally crucial for the performance of autonomous driving. Existing\nmethods usually leverage a training strategy that first pretrains on synthetic\ndata and finetunes on real data, resulting in twice the epochs compared to the\nbaseline (real data only). When we double the epochs in the baseline, the\nbenefit of synthetic data becomes negligible. To thoroughly demonstrate the\nbenefit of synthetic data, we introduce Dream4Drive, a novel synthetic data\ngeneration framework designed for enhancing the downstream perception tasks.\nDream4Drive first decomposes the input video into several 3D-aware guidance\nmaps and subsequently renders the 3D assets onto these guidance maps. Finally,\nthe driving world model is fine-tuned to produce the edited, multi-view\nphotorealistic videos, which can be used to train the downstream perception\nmodels. Dream4Drive enables unprecedented flexibility in generating multi-view\ncorner cases at scale, significantly boosting corner case perception in\nautonomous driving. To facilitate future research, we also contribute a\nlarge-scale 3D asset dataset named DriveObj3D, covering the typical categories\nin driving scenarios and enabling diverse 3D-aware video editing. We conduct\ncomprehensive experiments to show that Dream4Drive can effectively boost the\nperformance of downstream perception models under various training epochs.\nPage: https://wm-research.github.io/Dream4Drive/ GitHub Link:\nhttps://github.com/wm-research/Dream4Drive",
    "translation": "标题：将驾驶世界模型重新构想为感知任务的合成数据生成器\n\n摘要：驾驶世界模型的最新进展实现了高质量RGB视频或多模态视频的可控生成。现有方法主要关注生成质量与可控性相关的评估指标，却往往忽视对下游感知任务的评估——这对自动驾驶系统的性能至关重要。传统方法通常采用先在合成数据上预训练、再在真实数据上微调的训练策略，导致训练周期达到基准方法（仅使用真实数据）的两倍。当我们对基准方法加倍训练周期时，合成数据的优势变得微乎其微。为充分验证合成数据的价值，我们提出Dream4Drive——一个专为增强下游感知任务设计的新型合成数据生成框架。该框架首先将输入视频分解为若干三维感知引导图，随后将三维资源渲染至这些引导图，最后通过微调驾驶世界模型生成可用于训练下游感知模型的编辑后多视角逼真视频。Dream4Drive实现了大规模生成多视角边缘案例的前所未有的灵活性，显著提升了自动驾驶中的边缘案例感知能力。为促进后续研究，我们还构建了名为DriveObj3D的大规模三维资源数据集，涵盖驾驶场景中的典型类别，支持多样化的三维感知视频编辑。综合实验表明，在不同训练周期下，Dream4Drive均能有效提升下游感知模型的性能。\n项目主页：https://wm-research.github.io/Dream4Drive/\n代码仓库：https://github.com/wm-research/Dream4Drive",
    "url": "https://huggingface.co/papers/2510.19195",
    "arxiv_url": "https://arxiv.org/abs/2510.19195"
  },
  {
    "title": "Parallel Loop Transformer for Efficient Test-Time Computation Scaling",
    "summary": "Large Language Models (LLMs) are powerful but often too slow and costly for\nreal-world use during inference. Looped transformers save on parameters by\nreusing the same weights for multiple computational steps, or \"loops.\" However,\nthis approach has a major flaw: the loops run one after another, causing\ninference latency and memory requirements to increase with each added loop.\nThis makes them impractical for fast applications. To solve this problem, we\nintroduce the Parallel Loop Transformer (PLT). PLT is a new architecture that\ndelivers the performance benefits of a deep, looped model but with the low\nlatency of a standard, non-looped model. PLT works using two key techniques.\nFirst, Cross-Loop Parallelism (CLP) breaks the sequential dependency by\ncomputing different loops for different tokens at the same time, all within a\nsingle pass. Second, to prevent memory costs from growing, we use an Efficient\nRepresentation Enhancement strategy. This method shares the memory (KV cache)\nfrom the first loop with all other loops. It then uses a Gated Sliding-Window\nAttention (G-SWA) to combine this shared global information with local\ninformation, maintaining high accuracy. Our experiments show that PLT achieves\nthe high accuracy of a traditional looped model but with almost no extra\nlatency or memory cost compared to a standard transformer.",
    "translation": "标题：并行循环Transformer：高效测试时计算扩展\n\n摘要：大型语言模型虽然功能强大，但在实际推理应用中往往存在速度缓慢和计算成本高昂的问题。循环Transformer通过在多轮计算步骤（即“循环”）中复用相同权重来节省参数量，但该方法存在显著缺陷：循环必须顺序执行，导致推理延迟和内存需求随循环次数增加而递增，难以满足快速应用需求。为解决这一问题，我们提出并行循环Transformer（PLT）。这种新型架构既能实现深度循环模型的性能优势，又能保持标准非循环模型的低延迟特性。PLT通过两大核心技术实现突破：首先，跨循环并行技术（CLP）通过单次前向传播同时处理不同标记的循环计算，打破了顺序依赖；其次，为控制内存增长，采用高效表征增强策略——将首轮循环的键值缓存共享至所有后续循环，并引入门控滑动窗口注意力机制（G-SWA），将全局共享信息与局部信息动态融合以保持高精度。实验表明，PLT在实现传统循环模型高精度的同时，其延迟与内存开销与标准Transformer相比几乎无增加。",
    "url": "https://huggingface.co/papers/2510.24824",
    "arxiv_url": "https://arxiv.org/abs/2510.24824"
  },
  {
    "title": "MASPRM: Multi-Agent System Process Reward Model",
    "summary": "Practical deployment of Multi-Agent Systems (MAS) demands strong test-time\nperformance, motivating methods that guide inference-time search and\nselectively spend compute to improve quality. We present the Multi-Agent System\nProcess Reward Model (MASPRM). It assigns per-action, per-agent values to\npartial inter-agent transcripts and acts as an inference-time controller.\nMASPRM is trained from multi-agent Monte Carlo Tree Search (MCTS) rollouts\nwithout requiring step-level human annotations, by propagating returns to local\ntargets. At inference, MASPRM guides step-level beam search and MCTS, focusing\ncomputation on promising branches and pruning early. On GSM8K and MATH,\nMASPRM-guided decoding with an outcome reward model (ORM) applied to the final\nanswer, improves exact match (EM) over a single straight-through MAS pass by\n+30.7 and +22.9 points, respectively. A MASPRM trained on GSM8K transfers\nzero-shot to MATH without retraining, adding 8.4 EM points at the same\nbudget. MASPRM is a plug-in value model that estimates per-agent progress and\ncomplements verifier-style decoders, enabling more reliable, compute-aware\nmulti-agent reasoning. Code: https://github.com/milad1378yz/MASPRM",
    "translation": "标题：MASPRM：多智能体系统过程奖励模型\n\n摘要：多智能体系统（MAS）的实际部署需要强大的测试时性能，这推动了引导推理时搜索并选择性分配计算资源以提升质量的方法发展。我们提出多智能体系统过程奖励模型（MASPRM）。该模型通过为智能体间交互片段中的每个动作和每个智能体分配价值，充当推理时控制器。MASPRM无需步骤级人工标注，通过将回报传播至局部目标，基于多智能体蒙特卡洛树搜索（MCTS） rollout进行训练。在推理阶段，MASPRM指导步骤级束搜索和MCTS，将计算资源聚焦于潜力分支并实现早期剪枝。在GSM8K和MATH数据集上，结合最终答案的结果奖励模型（ORM），MASPRM引导的解码相较于单次直通式MAS处理，其精确匹配率（EM）分别提升30.7和22.9个百分点。在GSM8K上训练的MASPRM模型无需重新训练即可零样本迁移至MATH数据集，在相同计算预算下实现8.4个EM点的提升。MASPRM作为插件式价值模型，能够评估单智能体进度并补充验证器式解码器，为实现更可靠、具备计算感知能力的多智能体推理提供支持。代码地址：https://github.com/milad1378yz/MASPRM",
    "url": "https://huggingface.co/papers/2510.24803",
    "arxiv_url": "https://arxiv.org/abs/2510.24803"
  },
  {
    "title": "Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal\n  Perception and Generation",
    "summary": "We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a\nsparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion\ntotal parameters, of which only 6.1 billion are active per token. This\narchitecture enables highly efficient scaling (dramatically improving\ncomputational efficiency while significantly expanding model capacity) and\nempowers stronger unified multimodal intelligence across vision, speech, and\nlanguage, representing a key step toward Artificial General Intelligence (AGI).\nCompared to its predecessor, the upgraded version exhibits substantial\nimprovements across multimodal understanding and generation. We significantly\nadvance speech recognition capabilities, achieving state-of-the-art performance\nin contextual ASR and highly competitive results in dialect-aware ASR. In image\ngeneration, Ming-Flash-Omni introduces high-fidelity text rendering and\ndemonstrates marked gains in scene consistency and identity preservation during\nimage editing. Furthermore, Ming-Flash-Omni introduces generative segmentation,\na capability that not only achieves strong standalone segmentation performance\nbut also enhances spatial control in image generation and improves editing\nconsistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in\ntext-to-image generation and generative segmentation, and sets new records on\nall 12 contextual ASR benchmarks, all within a single unified architecture.",
    "translation": "标题：明闪万象：面向多模态感知与生成的稀疏统一架构\n\n摘要：本文提出明闪万象——明万象的升级版本，其基于灵闪2.0的稀疏专家混合架构（MoE），总参数量达千亿级别，但每个令牌仅激活61亿参数。该架构实现了高效扩展（在显著提升模型容量的同时大幅改善计算效率），并强化了视觉、语音与文本领域的统一多模态智能，标志着向通用人工智能（AGI）迈进的关键一步。相较于前代模型，升级版本在多模态理解与生成任务上均取得显著提升：我们大幅推进语音识别能力，在上下文感知语音识别任务中达到最优性能，在方言敏感语音识别中取得极具竞争力的结果；在图像生成领域，明闪万象实现了高保真文本渲染能力，并在图像编辑的场景连贯性与身份特征保持方面展现显著进步；此外，该模型创新性地提出生成式分割技术，不仅具备独立的分割性能，更增强了图像生成的空间控制能力并提升编辑一致性。值得关注的是，明闪万象在文图生成与生成式分割任务中均达到当前最优水平，并在全部12项上下文语音识别基准测试中刷新记录，所有功能均集成于单一统一架构之中。",
    "url": "https://huggingface.co/papers/2510.24821",
    "arxiv_url": "https://arxiv.org/abs/2510.24821"
  },
  {
    "title": "SeeingEye: Agentic Information Flow Unlocks Multimodal Reasoning In\n  Text-only LLMs",
    "summary": "Recent advances in text-only large language models (LLMs), such as\nDeepSeek-R1, demonstrate remarkable reasoning ability. However, these models\nremain fragile or entirely incapable when extended to multi-modal tasks.\nExisting approaches largely rely on single-form captions, which lack diversity\nand often fail to adapt across different types of Visual Question Answering\n(VQA) benchmarks. As a result, they provide no principled or efficient channel\nfor transmitting fine-grained visual information. We introduce Seeing Eye, a\nmodular framework that unlocks multimodal reasoning in text-only LLMs through\nan agent-based small VLM translator. This translator acts as a perception\nagent: it can invoke specialized tools (e.g., OCR and crop) and iteratively\ndistill multimodal inputs into structured intermediate representations (SIRs)\ntailored to the question. These SIRs are then passed to the text-only LLM,\nwhich serves as a reasoning agent. Crucially, the translator and reasoner\nengage in multi-round feedback and interaction, enabling the extraction of\ntargeted visual details and yielding more confident answers. Experiments on\nknowledge-intensive VQA benchmarks, including MMMU and MIA-Bench, demonstrate\nthat Seeing Eye not only reduces inference cost but also surpasses much larger\nend-to-end VLMs. For example, an instantiation combining a 3B-parameter vision\ntranslator with an 8B-parameter language reasoner outperforms a monolithic 32B\nVLM on challenging knowledge-based questions. Our results highlight that\ndecoupling perception from reasoning via agent information flow offers a\nscalable and plug-and-play pathway to multimodal reasoning, allowing strong\ntext-only LLMs to fully leverage their reasoning capabilities. Code is\navailable at: https://github.com/ulab-uiuc/SeeingEye",
    "translation": "标题：SeeingEye：基于智能体信息流解锁纯文本大语言模型的多模态推理能力\n\n摘要：纯文本大语言模型（如DeepSeek-R1）近期取得显著进展，展现出卓越的推理能力。然而当扩展至多模态任务时，这些模型仍表现脆弱或完全失效。现有方法主要依赖单一形式的图像描述，这种描述缺乏多样性且难以适应不同类型的视觉问答基准测试，导致无法建立传递细粒度视觉信息的规范化高效通道。我们提出SeeingEye模块化框架，通过基于智能体的小型视觉语言模型翻译器，解锁纯文本大语言模型的多模态推理能力。该翻译器作为感知智能体：可调用专用工具（如OCR和图像裁剪），将多模态输入迭代提炼为针对问题定制的结构化中间表示。这些结构化中间表示随后传递给作为推理智能体的纯文本大语言模型。关键在于，翻译器与推理器通过多轮反馈与交互，实现针对性视觉细节提取并生成更高置信度的答案。在知识密集型视觉问答基准测试（包括MMMU和MIA-Bench）上的实验表明，SeeingEye不仅降低推理成本，更超越规模更大的端到端视觉语言模型。例如，结合30亿参数视觉翻译器与80亿参数语言推理器的实例，在基于知识的挑战性问题上性能优于单体320亿参数视觉语言模型。我们的研究结果证明，通过智能体信息流实现感知与推理的解耦，为多模态推理提供了可扩展的即插即用路径，使强效纯文本大语言模型得以充分发挥其推理潜力。代码已开源：https://github.com/ulab-uiuc/SeeingEye",
    "url": "https://huggingface.co/papers/2510.25092",
    "arxiv_url": "https://arxiv.org/abs/2510.25092"
  },
  {
    "title": "TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological\n  Counseling",
    "summary": "Large language models (LLMs) in psychological counseling have attracted\nincreasing attention. However, existing approaches often lack emotional\nunderstanding, adaptive strategies, and the use of therapeutic methods across\nmultiple sessions with long-term memory, leaving them far from real clinical\npractice. To address these critical gaps, we introduce TheraMind, a strategic\nand adaptive agent for longitudinal psychological counseling. The cornerstone\nof TheraMind is a novel dual-loop architecture that decouples the complex\ncounseling process into an Intra-Session Loop for tactical dialogue management\nand a Cross-Session Loop for strategic therapeutic planning. The Intra-Session\nLoop perceives the patient's emotional state to dynamically select response\nstrategies while leveraging cross-session memory to ensure continuity.\nCrucially, the Cross-Session Loop empowers the agent with long-term\nadaptability by evaluating the efficacy of the applied therapy after each\nsession and adjusting the method for subsequent interactions. We validate our\napproach in a high-fidelity simulation environment grounded in real clinical\ncases. Extensive evaluations show that TheraMind outperforms other methods,\nespecially on multi-session metrics like Coherence, Flexibility, and\nTherapeutic Attunement, validating the effectiveness of its dual-loop design in\nemulating strategic, adaptive, and longitudinal therapeutic behavior. The code\nis publicly available at https://0mwwm0.github.io/TheraMind/.",
    "translation": "标题：TheraMind：面向纵向心理咨询的战略性自适应智能体\n\n摘要：基于大语言模型的心理咨询应用日益受到关注。然而现有方法普遍存在情感理解不足、策略适应性欠缺以及跨会话治疗技术运用不充分等问题，且缺乏长期记忆机制，导致与真实临床实践存在显著差距。为突破这些关键瓶颈，我们提出TheraMind——一个面向纵向心理咨询的战略性自适应智能体。该系统的核心创新是采用新型双循环架构，将复杂咨询过程解耦为负责战术性对话管理的会话内循环，与专注战略性治疗规划的跨会话循环。会话内循环通过感知患者情绪状态动态选择应答策略，并借助跨会话记忆确保连续性；跨会话循环则通过评估每次咨询的治疗效果，动态调整后续干预方法，实现长期适应性。我们在基于真实临床案例构建的高保真仿真环境中验证该方法。综合评估表明，TheraMind在连贯性、灵活性和治疗协调性等跨会话指标上显著优于现有方法，有效验证了双循环设计在模拟战略性、自适应及纵向治疗行为方面的优越性。代码已开源：https://0mwwm0.github.io/TheraMind/。",
    "url": "https://huggingface.co/papers/2510.25758",
    "arxiv_url": "https://arxiv.org/abs/2510.25758"
  },
  {
    "title": "PairUni: Pairwise Training for Unified Multimodal Language Models",
    "summary": "Unified vision-language models (UVLMs) must perform both understanding and\ngeneration within a single architecture, but these tasks rely on heterogeneous\ndata and supervision, making it difficult to balance them during reinforcement\nlearning (RL). We propose PairUni, a unified framework that reorganizes data\ninto understanding-generation (UG) pairs and aligns optimization accordingly.\nWe first use GPT-o3 to augment single-task data, generating captions for\nunderstanding samples and question-answer (QA) pairs for generation samples,\nforming aligned pairs from the same instance. Additionally, for each generation\nsample, we retrieve a semantically related understanding example to form a\nretrieved pair, linking different but related data points. These paired\nstructures expose cross-task semantic correspondences and support consistent\npolicy learning. To leverage this structure, we present Pair-GPRO, a pair-aware\nvariant based on Group Relative Policy Optimization. It assigns a similarity\nscore to each pair to modulate the advantage, strengthening learning from\nwell-aligned examples and reducing task interference. We curate a high-quality\ndataset of 16K UG pairs named PairUG for RL fine-tuning and evaluate PairUni on\nthe powerful Janus-Pro UVLMs. Our approach achieves balanced improvements on\nvarious UVLMs, outperforming strong UVLM RL baselines. Code:\nhttps://github.com/Haochen-Wang409/PairUni{github.com/Haochen-Wang409/PairUni}",
    "translation": "标题：PairUni：面向统一多模态语言模型的成对训练方法\n\n摘要：统一视觉语言模型需要在单一架构中同时实现理解与生成任务，但这两类任务依赖异构数据和监督信号，导致在强化学习阶段难以实现有效平衡。本文提出PairUni统一框架，通过将数据重组为理解-生成配对样本并相应调整优化策略。我们首先利用GPT-4o对单任务数据进行增强：为理解样本生成对应描述，为生成样本构建问答对，从而形成源自同一实例的对齐配对。此外，针对每个生成样本，我们检索语义相关的理解样本构建检索配对，建立不同数据点间的语义关联。这种配对结构显式揭示了跨任务语义对应关系，并为一致性策略学习提供支撑。基于此，我们提出Pair-GPRO——基于群组相对策略优化的配对感知变体，通过为每个配对分配相似度评分来调节优势函数，从而强化对齐良好样本的学习效果并降低任务间干扰。我们精心构建了包含1.6万组理解-生成配对的高质量数据集PairUG用于强化学习微调，并在强大的Janus-Pro统一视觉语言模型上评估PairUni。实验表明，本方法在多种统一视觉语言模型上实现了均衡性能提升，显著优于现有强化学习基线模型。代码地址：https://github.com/Haochen-Wang409/PairUni",
    "url": "https://huggingface.co/papers/2510.25682",
    "arxiv_url": "https://arxiv.org/abs/2510.25682"
  },
  {
    "title": "BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic\n  Domains",
    "summary": "The rapid advancement of large language models(LLMs) has intensified the need\nfor domain and culture specific evaluation. Existing benchmarks are largely\nAnglocentric and domain-agnostic, limiting their applicability to India-centric\ncontexts. To address this gap, we introduce BhashaBench V1, the first\ndomain-specific, multi-task, bilingual benchmark focusing on critical Indic\nknowledge systems. BhashaBench V1 contains 74,166 meticulously curated\nquestion-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from\nauthentic government and domain-specific exams. It spans four major domains:\nAgriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and\ncovering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs\nreveals significant domain and language specific performance gaps, with\nespecially large disparities in low-resource domains. For instance, GPT-4o\nachieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models\nconsistently perform better on English content compared to Hindi across all\ndomains. Subdomain-level analysis shows that areas such as Cyber Law,\nInternational Finance perform relatively well, while Panchakarma, Seed Science,\nand Human Rights remain notably weak. BhashaBench V1 provides a comprehensive\ndataset for evaluating large language models across India's diverse knowledge\ndomains. It enables assessment of models' ability to integrate domain-specific\nknowledge with bilingual understanding. All code, benchmarks, and resources are\npublicly available to support open research.",
    "translation": "标题：BhashaBench V1：印度知识象限领域的综合性基准测试框架\n\n摘要：大语言模型的快速发展加剧了对领域与文化特异性评估的需求。现有基准测试主要围绕英语语境且缺乏领域针对性，限制了其在印度中心场景下的适用性。为弥补这一空白，我们推出BhashaBench V1——首个聚焦关键印度知识系统的领域特异性、多任务、双语基准测试。该基准包含74,166个精心构建的问答对（其中英文52,494组，印地语21,672组），数据源自主管部门及专业领域考试，涵盖农业、法律、金融与阿育吠陀四大核心领域，包含90余个子领域及500多个专题，支持细粒度评估。对29款大语言模型的测试结果显示：模型在不同领域和语言间存在显著性能差异，资源稀缺领域表现尤为薄弱（例如GPT-4o在法律领域准确率达76.49%，而在阿育吠陀领域仅为59.74%）；所有领域内模型对英文内容的处理能力均稳定优于印地语。子领域分析表明，网络法、国际金融等领域表现相对较好，而潘查卡尔玛疗法、种子科学、人权等领域仍明显薄弱。BhashaBench V1为评估大语言模型在印度多元知识领域的表现提供了完整数据集，可检验模型融合领域专业知识与双语理解的能力。所有代码、基准数据及相关资源均已公开，以支持开放式研究。",
    "url": "https://huggingface.co/papers/2510.25409",
    "arxiv_url": "https://arxiv.org/abs/2510.25409"
  }
]