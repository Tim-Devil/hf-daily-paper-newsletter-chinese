[
  {
    "title": "InteractComp: Evaluating Search Agents With Ambiguous Queries",
    "summary": "Language agents have demonstrated remarkable potential in web search and\ninformation retrieval. However, these search agents assume user queries are\ncomplete and unambiguous, an assumption that diverges from reality where users\nbegin with incomplete queries requiring clarification through interaction. Yet\nmost agents lack interactive mechanisms during the search process, and existing\nbenchmarks cannot assess this capability. To address this gap, we introduce\nInteractComp, a benchmark designed to evaluate whether search agents can\nrecognize query ambiguity and actively interact to resolve it during search.\nFollowing the principle of easy to verify, interact to disambiguate, we\nconstruct 210 expert-curated questions across 9 domains through a\ntarget-distractor methodology that creates genuine ambiguity resolvable only\nthrough interaction. Evaluation of 17 models reveals striking failure: the best\nmodel achieves only 13.73% accuracy despite 71.50% with complete context,\nexposing systematic overconfidence rather than reasoning deficits. Forced\ninteraction produces dramatic gains, demonstrating latent capability current\nstrategies fail to engage. Longitudinal analysis shows interaction capabilities\nstagnated over 15 months while search performance improved seven-fold,\nrevealing a critical blind spot. This stagnation, coupled with the immediate\nfeedback inherent to search tasks, makes InteractComp a valuable resource for\nboth evaluating and training interaction capabilities in search agents. The\ncode is available at https://github.com/FoundationAgents/InteractComp.",
    "translation": "标题：InteractComp：基于模糊查询的搜索智能体评估框架\n\n摘要：语言智能体在网络搜索与信息检索领域展现出显著潜力。然而现有搜索智能体均假设用户查询是完整且明确的，这与实际场景中用户常以需经交互澄清的不完整查询为起点的现实相悖。当前多数智能体缺乏搜索过程中的交互机制，且现有基准无法有效评估此项能力。为填补这一空白，我们提出InteractComp基准，专门用于评估搜索智能体能否识别查询模糊性并在搜索过程中主动交互以解决问题。遵循\"易于验证、交互消歧\"的原则，我们通过目标-干扰项方法构建了涵盖9个领域的210道专家评审问题，这些问题均存在必须通过交互才能化解的真实模糊性。对17个模型的评估揭示出惊人缺陷：最佳模型仅达到13.73%的准确率（完整上下文条件下可达71.50%），暴露出系统性的过度自信而非推理能力不足。强制交互策略带来显著效果提升，证明现有策略未能有效激发潜在能力。纵向分析表明，在搜索性能提升七倍的同时，交互能力在15个月内持续停滞，这揭示了重要研究盲区。这种发展停滞与搜索任务固有的即时反馈特性，使得InteractComp成为评估和训练搜索智能体交互能力的宝贵资源。代码已发布于https://github.com/FoundationAgents/InteractComp。",
    "url": "https://huggingface.co/papers/2510.24668",
    "arxiv_url": "https://arxiv.org/abs/2510.24668"
  },
  {
    "title": "Tongyi DeepResearch Technical Report",
    "summary": "We present Tongyi DeepResearch, an agentic large language model, which is\nspecifically designed for long-horizon, deep information-seeking research\ntasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is\ndeveloped through an end-to-end training framework that combines agentic\nmid-training and agentic post-training, enabling scalable reasoning and\ninformation seeking across complex tasks. We design a highly scalable data\nsynthesis pipeline that is fully automatic, without relying on costly human\nannotation, and empowers all training stages. By constructing customized\nenvironments for each stage, our system enables stable and consistent\ninteractions throughout. Tongyi DeepResearch, featuring 30.5 billion total\nparameters, with only 3.3 billion activated per token, achieves\nstate-of-the-art performance across a range of agentic deep research\nbenchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH,\nWebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We\nopen-source the model, framework, and complete solutions to empower the\ncommunity.",
    "translation": "标题：通义深度研究技术报告\n\n摘要：本文介绍通义深度研究——一种专为长周期深度信息检索研究任务设计的智能体大语言模型。为激发自主深度研究能力，该模型通过端到端训练框架开发，融合智能体中期训练与后期训练，实现跨复杂任务的可扩展推理与信息检索。我们设计了高度可扩展的全自动数据合成流程，无需依赖高成本人工标注即可支撑全训练阶段。通过为各阶段构建定制化交互环境，系统确保了全流程稳定一致的交互体验。通义深度研究模型总参数量达305亿，每令牌仅激活33亿参数，在包括\"人类终极考试\"、BrowseComp、BrowseComp-ZH、WebWalkerQA、xbench-DeepSearch、FRAMES及xbench-DeepSearch-2510等一系列智能体深度研究基准测试中均达到最先进性能。我们开源模型、框架及完整解决方案，以赋能学术社区。",
    "url": "https://huggingface.co/papers/2510.24701",
    "arxiv_url": "https://arxiv.org/abs/2510.24701"
  },
  {
    "title": "AgentFold: Long-Horizon Web Agents with Proactive Context Management",
    "summary": "LLM-based web agents show immense promise for information seeking, yet their\neffectiveness on long-horizon tasks is hindered by a fundamental trade-off in\ncontext management. Prevailing ReAct-based agents suffer from context\nsaturation as they accumulate noisy, raw histories, while methods that fixedly\nsummarize the full history at each step risk the irreversible loss of critical\ndetails. Addressing these, we introduce AgentFold, a novel agent paradigm\ncentered on proactive context management, inspired by the human cognitive\nprocess of retrospective consolidation. AgentFold treats its context as a\ndynamic cognitive workspace to be actively sculpted, rather than a passive log\nto be filled. At each step, it learns to execute a `folding' operation, which\nmanages its historical trajectory at multiple scales: it can perform granular\ncondensations to preserve vital, fine-grained details, or deep consolidations\nto abstract away entire multi-step sub-tasks. The results on prominent\nbenchmarks are striking: with simple supervised fine-tuning (without continual\npre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp\nand 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or\nmatches open-source models of a dramatically larger scale, such as the\nDeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like\nOpenAI's o4-mini.",
    "translation": "标题：AgentFold：具有前瞻性上下文管理能力的长周期网络智能体\n\n摘要：基于大语言模型的网络智能体在信息检索领域展现出巨大潜力，但其在长周期任务中的效能受到上下文管理固有矛盾的制约。主流基于ReAct框架的智能体因积累大量噪声原始历史记录而面临上下文饱和问题，而固定式全历史摘要方法则可能导致关键细节的不可逆丢失。针对这些挑战，我们提出AgentFold——一种以前瞻性上下文管理为核心的新型智能体范式，其设计灵感源自人类回顾性巩固的认知机制。AgentFold将上下文视为可主动塑形的动态认知工作区，而非被动填充的日志记录。在每个决策步骤中，该系统通过习得的“折叠”操作对历史轨迹进行多尺度管理：既可执行细粒度压缩以保留至关重要的微观细节，也能实施深度整合以抽象化完整的多步骤子任务。在权威基准测试中的表现令人瞩目：仅通过简单的监督微调（无需持续预训练或强化学习），我们的AgentFold-30B-A3B智能体在BrowseComp上达到36.2%的准确率，在BrowseComp-ZH上达到47.3%。值得关注的是，该性能不仅超越或匹配了规模显著更大的开源模型（如DeepSeek-V3.1-671B-A37B），更超越了OpenAI o4-mini等领先的专有智能体系统。",
    "url": "https://huggingface.co/papers/2510.24699",
    "arxiv_url": "https://arxiv.org/abs/2510.24699"
  },
  {
    "title": "Game-TARS: Pretrained Foundation Models for Scalable Generalist\n  Multimodal Game Agents",
    "summary": "We present Game-TARS, a generalist game agent trained with a unified,\nscalable action space anchored to human-aligned native keyboard-mouse inputs.\nUnlike API- or GUI-based approaches, this paradigm enables large-scale\ncontinual pre-training across heterogeneous domains, including OS, web, and\nsimulation games. Game-TARS is pre-trained on over 500B tokens with diverse\ntrajectories and multimodal data. Key techniques include a decaying continual\nloss to reduce causal confusion and an efficient Sparse-Thinking strategy that\nbalances reasoning depth and inference cost. Experiments show that Game-TARS\nachieves about 2 times the success rate over the previous sota model on\nopen-world Minecraft tasks, is close to the generality of fresh humans in\nunseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet\nin FPS benchmarks. Scaling results on training-time and test-time confirm that\nthe unified action space sustains improvements when scaled to cross-game and\nmultimodal data. Our results demonstrate that simple, scalable action\nrepresentations combined with large-scale pre-training provide a promising path\ntoward generalist agents with broad computer-use abilities.",
    "translation": "标题：Game-TARS：面向可扩展通用型多模态游戏智能体的预训练基础模型\n\n摘要：本文提出Game-TARS——一种基于人类标准键鼠输入构建的统一可扩展动作空间的通用游戏智能体。与基于API或图形界面的方法不同，该范式支持跨操作系统、网页和模拟游戏等异构领域的大规模持续预训练。Game-TARS通过包含多样化轨迹与多模态数据的超过5000亿标记进行预训练，其核心技术包含：用于降低因果混淆的衰减持续损失函数，以及平衡推理深度与计算成本的高效稀疏思维策略。实验表明：在开放世界《我的世界》任务中，Game-TARS的成功率较先前最优模型提升约2倍；在未知网页3D游戏中接近人类新手的泛化能力；在FPS基准测试中超越GPT-5、Gemini-2.5-Pro和Claude-4-Sonnet。训练阶段与测试阶段的扩展实验结果证实，统一动作空间在跨游戏与多模态数据扩展时能持续提升性能。我们的研究证明，基于简洁可扩展的动作表示与大规模预训练相结合，为构建具有广泛计算机使用能力的通用智能体提供了可行路径。",
    "url": "https://huggingface.co/papers/2510.23691",
    "arxiv_url": "https://arxiv.org/abs/2510.23691"
  },
  {
    "title": "RoboOmni: Proactive Robot Manipulation in Omni-modal Context",
    "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid\nprogress in Vision-Language-Action (VLA) models for robotic manipulation.\nAlthough effective in many scenarios, current approaches largely rely on\nexplicit instructions, whereas in real-world interactions, humans rarely issue\ninstructions directly. Effective collaboration requires robots to infer user\nintentions proactively. In this work, we introduce cross-modal contextual\ninstructions, a new setting where intent is derived from spoken dialogue,\nenvironmental sounds, and visual cues rather than explicit commands. To address\nthis new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor\nframework based on end-to-end omni-modal LLMs that unifies intention\nrecognition, interaction confirmation, and action execution. RoboOmni fuses\nauditory and visual signals spatiotemporally for robust intention recognition,\nwhile supporting direct speech interaction. To address the absence of training\ndata for proactive intention recognition in robotic manipulation, we build\nOmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640\nbackgrounds, and six contextual instruction types. Experiments in simulation\nand real-world settings show that RoboOmni surpasses text- and ASR-based\nbaselines in success rate, inference speed, intention recognition, and\nproactive assistance.",
    "translation": "标题：RoboOmni：全模态情境下的主动式机器人操控\n\n摘要：多模态大语言模型的最新进展推动了机器人操控领域视觉-语言-动作模型的快速发展。尽管现有方法在许多场景中表现有效，但它们主要依赖显式指令，而在现实人机交互中人类很少直接发出指令。要实现高效协作，机器人需要具备主动推断用户意图的能力。本研究提出跨模态情境指令新范式，通过语音对话、环境声音和视觉线索而非显式命令来推导意图。针对这一新范式，我们提出RoboOmni——基于端到端全模态大语言模型的感知-思考-对话-执行框架，统一实现意图识别、交互确认和动作执行。该框架通过时空融合听觉与视觉信号实现鲁棒的意图识别，并支持直接语音交互。为解决机器人主动意图识别训练数据缺失的问题，我们构建了包含14万条操作序列、5000+说话人、2400种事件声音、640种背景和六类情境指令的OmniAction数据集。仿真与真实环境实验表明，RoboOmni在成功率、推理速度、意图识别准确率和主动辅助能力方面均优于基于文本和自动语音识别的基线方法。",
    "url": "https://huggingface.co/papers/2510.23763",
    "arxiv_url": "https://arxiv.org/abs/2510.23763"
  },
  {
    "title": "Uniform Discrete Diffusion with Metric Path for Video Generation",
    "summary": "Continuous-space video generation has advanced rapidly, while discrete\napproaches lag behind due to error accumulation and long-context inconsistency.\nIn this work, we revisit discrete generative modeling and present Uniform\ndiscRete diffuSion with metric pAth (URSA), a simple yet powerful framework\nthat bridges the gap with continuous approaches for the scalable video\ngeneration. At its core, URSA formulates the video generation task as an\niterative global refinement of discrete spatiotemporal tokens. It integrates\ntwo key designs: a Linearized Metric Path and a Resolution-dependent Timestep\nShifting mechanism. These designs enable URSA to scale efficiently to\nhigh-resolution image synthesis and long-duration video generation, while\nrequiring significantly fewer inference steps. Additionally, we introduce an\nasynchronous temporal fine-tuning strategy that unifies versatile tasks within\na single model, including interpolation and image-to-video generation.\nExtensive experiments on challenging video and image generation benchmarks\ndemonstrate that URSA consistently outperforms existing discrete methods and\nachieves performance comparable to state-of-the-art continuous diffusion\nmethods. Code and models are available at https://github.com/baaivision/URSA",
    "translation": "标题：基于度量路径的均匀离散扩散视频生成方法\n\n摘要：连续空间视频生成技术发展迅猛，而离散方法因误差累积和长上下文不一致性问题发展滞后。本研究重新审视离散生成建模，提出具有度量路径的均匀离散扩散框架（URSA），这一简洁而强大的框架通过可扩展视频生成弥合了与连续方法的差距。该框架核心将视频生成任务定义为离散时空标记的迭代全局优化过程，集成了两个关键设计：线性化度量路径和分辨率相关时间步偏移机制。这些设计使URSA能够高效扩展至高分辨率图像合成和长时序视频生成，同时显著减少推理步数。此外，我们提出异步时序微调策略，将插值和图像到视频生成等多种任务统一于单一模型中。在具有挑战性的视频与图像生成基准测试上的大量实验表明，URSA持续超越现有离散方法，并达到与最先进连续扩散方法相媲美的性能。代码与模型已发布于https://github.com/baaivision/URSA",
    "url": "https://huggingface.co/papers/2510.24717",
    "arxiv_url": "https://arxiv.org/abs/2510.24717"
  },
  {
    "title": "Group Relative Attention Guidance for Image Editing",
    "summary": "Recently, image editing based on Diffusion-in-Transformer models has\nundergone rapid development. However, existing editing methods often lack\neffective control over the degree of editing, limiting their ability to achieve\nmore customized results. To address this limitation, we investigate the\nMM-Attention mechanism within the DiT model and observe that the Query and Key\ntokens share a bias vector that is only layer-dependent. We interpret this bias\nas representing the model's inherent editing behavior, while the delta between\neach token and its corresponding bias encodes the content-specific editing\nsignals. Based on this insight, we propose Group Relative Attention Guidance, a\nsimple yet effective method that reweights the delta values of different tokens\nto modulate the focus of the model on the input image relative to the editing\ninstruction, enabling continuous and fine-grained control over editing\nintensity without any tuning. Extensive experiments conducted on existing image\nediting frameworks demonstrate that GRAG can be integrated with as few as four\nlines of code, consistently enhancing editing quality. Moreover, compared to\nthe commonly used Classifier-Free Guidance, GRAG achieves smoother and more\nprecise control over the degree of editing. Our code will be released at\nhttps://github.com/little-misfit/GRAG-Image-Editing.",
    "translation": "标题：图像编辑中的群组相对注意力引导机制\n\n摘要：基于Transformer架构的扩散模型在图像编辑领域近期取得了快速发展。然而，现有编辑方法往往缺乏对编辑程度的有效控制，限制了其实现更精细化编辑效果的能力。为解决这一局限性，我们深入研究了DiT模型中的多头注意力机制，发现查询向量与键向量共享仅与网络层相关的偏置向量。我们将该偏置向量解释为模型固有的编辑行为特征，而各表征向量与其对应偏置之间的差值则编码了具体内容的编辑信号。基于这一发现，我们提出了群组相对注意力引导方法——一种通过重新加权不同表征向量的差值来调节模型对输入图像与编辑指令的相对关注度的创新方案，无需任何参数调优即可实现连续、细粒度的编辑强度控制。在现有图像编辑框架上进行的大量实验表明，GRAG方法仅需四行代码即可实现集成，并能持续提升编辑质量。与常用的无分类器引导技术相比，GRAG能够实现更平滑、更精确的编辑程度控制。我们的代码将在https://github.com/little-misfit/GRAG-Image-Editing 发布。",
    "url": "https://huggingface.co/papers/2510.24657",
    "arxiv_url": "https://arxiv.org/abs/2510.24657"
  },
  {
    "title": "OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents",
    "summary": "With advances in decision-making and reasoning capabilities, multimodal\nagents show strong potential in computer application scenarios. Past\nevaluations have mainly assessed GUI interaction skills, while tool invocation\nabilities, such as those enabled by the Model Context Protocol (MCP), have been\nlargely overlooked. Comparing agents with integrated tool invocation to those\nevaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP,\nthe first comprehensive and fair benchmark for assessing computer-use agents'\ntool invocation, GUI operation, and decision-making abilities in a real-world\nenvironment. We design a novel automated code-generation pipeline to create\ntools and combine them with a curated selection from existing tools. Rigorous\nmanual validation yields 158 high-quality tools (covering 7 common\napplications), each verified for correct functionality, practical\napplicability, and versatility. Extensive evaluations of state-of-the-art\nmultimodal agents on OSWorld-MCP show that MCP tools generally improve task\nsuccess rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1%\nto 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of\nassessing tool invocation capabilities. However, even the strongest models have\nrelatively low tool invocation rates, Only 36.3%, indicating room for\nimprovement and highlighting the benchmark's challenge. By explicitly measuring\nMCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents\nand sets a new standard for evaluating performance in complex, tool-assisted\nenvironments. Our code, environment, and data are publicly available at\nhttps://osworld-mcp.github.io.",
    "translation": "标题：OSWorld-MCP：计算机应用智能体中MCP工具调用能力的基准测试研究\n\n摘要：随着决策与推理能力的进步，多模态智能体在计算机应用场景中展现出巨大潜力。现有评估主要关注图形用户界面交互能力，而对基于模型上下文协议（MCP）实现的工具调用能力评估尚存空白。将集成工具调用的智能体与仅评估GUI交互的智能体进行对比存在本质不公。本研究提出OSWorld-MCP——首个在真实环境下全面公正评估计算机应用智能体工具调用、GUI操作与决策能力的基准体系。我们设计了创新的自动化代码生成流程来创建工具，并将其与现有工具精选集相结合。通过严格人工验证最终获得158个高质量工具（覆盖7类常见应用），每个工具均通过功能性、实用性与多场景适用性三重检验。基于OSWorld-MCP对前沿多模态智能体的广泛测试表明：MCP工具能显著提升任务成功率（如OpenAI o3在15步时从8.3%提升至20.4%，Claude 4 Sonnet在50步时从40.1%提升至43.3%），印证了评估工具调用能力的重要性。然而即便最强模型的工具调用率也相对较低（仅36.3%），既揭示了改进空间，也凸显了本基准的前沿性。通过明确量化MCP工具使用能力，OSWorld-MCP深化了对多模态智能体的认知，为复杂工具辅助环境下的性能评估确立了新标准。相关代码、环境与数据已公开于https://osworld-mcp.github.io。",
    "url": "https://huggingface.co/papers/2510.24563",
    "arxiv_url": "https://arxiv.org/abs/2510.24563"
  },
  {
    "title": "Routing Matters in MoE: Scaling Diffusion Transformers with Explicit\n  Routing Guidance",
    "summary": "Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model\ncapacity while preserving computational efficiency. Despite its notable success\nin large language models (LLMs), existing attempts to apply MoE to Diffusion\nTransformers (DiTs) have yielded limited gains. We attribute this gap to\nfundamental differences between language and visual tokens. Language tokens are\nsemantically dense with pronounced inter-token variation, while visual tokens\nexhibit spatial redundancy and functional heterogeneity, hindering expert\nspecialization in vision MoE. To this end, we present ProMoE, an MoE framework\nfeaturing a two-step router with explicit routing guidance that promotes expert\nspecialization. Specifically, this guidance encourages the router to partition\nimage tokens into conditional and unconditional sets via conditional routing\naccording to their functional roles, and refine the assignments of conditional\nimage tokens through prototypical routing with learnable prototypes based on\nsemantic content. Moreover, the similarity-based expert allocation in latent\nspace enabled by prototypical routing offers a natural mechanism for\nincorporating explicit semantic guidance, and we validate that such guidance is\ncrucial for vision MoE. Building on this, we propose a routing contrastive loss\nthat explicitly enhances the prototypical routing process, promoting\nintra-expert coherence and inter-expert diversity. Extensive experiments on\nImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods\nunder both Rectified Flow and DDPM training objectives. Code and models will be\nmade publicly available.",
    "translation": "标题：MoE中的路由机制研究：基于显式路由指导的扩散变换器规模化扩展\n\n摘要：混合专家模型已成为在保持计算效率的同时扩展模型容量的重要范式。尽管该范式在大型语言模型中取得显著成功，但现有将MoE应用于扩散变换器的尝试收效甚微。我们认为这一差距源于语言与视觉标记的本质差异：语言标记具有语义密集性和显著的标记间差异性，而视觉标记则呈现空间冗余性和功能异质性，这阻碍了视觉MoE中的专家专业化。为此，我们提出ProMoE——一个具有显式路由指导的双阶段路由器的MoE框架，该框架通过功能角色划分促进专家专业化。具体而言，该指导机制通过条件路由将图像标记按功能角色划分为条件集和无条件集，并借助基于语义内容的可学习原型，通过原型路由优化条件图像标记的分配。此外，原型路由实现的潜在空间基于相似度的专家分配，为融入显式语义指导提供了天然机制，我们验证了此类指导对视觉MoE至关重要。基于此，我们提出路由对比损失函数，显式增强原型路由过程，促进专家内部一致性与专家间多样性。在ImageNet基准上的大量实验表明，ProMoE在整流流和DDPM训练目标下均优于现有最优方法。代码与模型将公开发布。",
    "url": "https://huggingface.co/papers/2510.24711",
    "arxiv_url": "https://arxiv.org/abs/2510.24711"
  },
  {
    "title": "WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling\n  Info-Rich Seeking",
    "summary": "Large Language Model (LLM)-based agents have emerged as a transformative\napproach for open-ended problem solving, with information seeking (IS) being a\ncore capability that enables autonomous reasoning and decision-making. While\nprior research has largely focused on improving retrieval depth, we observe\nthat current IS agents often suffer from low search efficiency, which in turn\nconstrains overall performance. A key factor underlying this inefficiency is\nthe sparsity of target entities in training tasks, which limits opportunities\nfor agents to learn and generalize efficient search behaviors. To address these\nchallenges, we propose WebLeaper, a framework for constructing high-coverage IS\ntasks and generating efficient solution trajectories. We formulate IS as a\ntree-structured reasoning problem, enabling a substantially larger set of\ntarget entities to be embedded within a constrained context. Leveraging curated\nWikipedia tables, we propose three variants for synthesizing IS tasks, Basic,\nUnion, and Reverse-Union, to systematically increase both IS efficiency and\nefficacy. Finally, we curate training trajectories by retaining only those that\nare simultaneously accurate and efficient, ensuring that the model is optimized\nfor both correctness and search performance. Extensive experiments on both\nbasic and comprehensive settings, conducted on five IS benchmarks, BrowserComp,\nGAIA, xbench-DeepSearch, WideSearch, and Seal-0, demonstrate that our method\nconsistently achieves improvements in both effectiveness and efficiency over\nstrong baselines.",
    "translation": "标题：WebLeaper：通过支持信息富集式搜索增强网络代理的效能与效率\n\n摘要：基于大语言模型的智能体已成为解决开放式问题的变革性方法，其中信息搜索作为核心能力支撑着自主推理与决策。尽管现有研究主要聚焦于提升检索深度，但我们发现当前信息搜索智能体普遍存在搜索效率低下的问题，这反过来制约了整体性能。造成该低效现象的关键因素在于训练任务中目标实体的稀疏性，限制了智能体学习并泛化高效搜索行为的机会。为应对这些挑战，我们提出WebLeaper框架，通过构建高覆盖度的信息搜索任务并生成高效解决方案轨迹来解决问题。我们将信息搜索建模为树状结构推理问题，使更多目标实体能够嵌入受限上下文中。借助精选的维基百科表格，我们提出基础型、联合型及反向联合型三种任务生成变体，系统提升信息搜索的效率和效果。最后，我们通过筛选同时具备准确性与高效性的训练轨迹，确保模型在正确性和搜索性能上均得到优化。在五大信息搜索基准测试（BrowserComp、GAIA、xbench-DeepSearch、WideSearch和Seal-0）上进行的广泛实验表明，我们的方法在基础场景和综合场景下均能持续超越强基线模型，在效能与效率方面实现双重提升。",
    "url": "https://huggingface.co/papers/2510.24697",
    "arxiv_url": "https://arxiv.org/abs/2510.24697"
  },
  {
    "title": "Repurposing Synthetic Data for Fine-grained Search Agent Supervision",
    "summary": "LLM-based search agents are increasingly trained on entity-centric synthetic\ndata to solve complex, knowledge-intensive tasks. However, prevailing training\nmethods like Group Relative Policy Optimization (GRPO) discard this rich entity\ninformation, relying instead on sparse, outcome-based rewards. This critical\nlimitation renders them unable to distinguish informative \"near-miss\"\nsamples-those with substantially correct reasoning but a flawed final\nanswer-from complete failures, thus discarding valuable learning signals. We\naddress this by leveraging the very entities discarded during training. Our\nempirical analysis reveals a strong positive correlation between the number of\nground-truth entities identified during an agent's reasoning process and final\nanswer accuracy. Building on this insight, we introduce Entity-aware Group\nRelative Policy Optimization (E-GRPO), a novel framework that formulates a\ndense entity-aware reward function. E-GRPO assigns partial rewards to incorrect\nsamples proportional to their entity match rate, enabling the model to\neffectively learn from these \"near-misses\". Experiments on diverse\nquestion-answering (QA) and deep research benchmarks show that E-GRPO\nconsistently and significantly outperforms the GRPO baseline. Furthermore, our\nanalysis reveals that E-GRPO not only achieves superior accuracy but also\ninduces more efficient reasoning policies that require fewer tool calls,\ndemonstrating a more effective and sample-efficient approach to aligning search\nagents.",
    "translation": "标题：合成数据在细粒度搜索代理监督中的再利用\n\n摘要：基于大语言模型的搜索代理正越来越多地使用以实体为中心的合成数据进行训练，以解决复杂的知识密集型任务。然而，当前主流的训练方法（如组相对策略优化）丢弃了这些丰富的实体信息，转而依赖稀疏的、基于结果的奖励机制。这一关键缺陷使得模型无法区分具有参考价值的\"近似正确\"样本（即推理过程基本正确但最终答案存在瑕疵的样本）与完全错误的样本，从而导致有价值学习信号的丢失。我们通过利用训练过程中被丢弃的实体信息来解决这一问题。实证分析表明，智能体在推理过程中识别出的真实实体数量与最终答案准确率之间存在显著正相关。基于这一发现，我们提出了实体感知的组相对策略优化框架——一种创新的、能够构建密集实体感知奖励函数的方法。该框架根据错误样本的实体匹配率为其分配相应比例的奖励，使模型能够从\"近似正确\"样本中有效学习。在多样化问答系统和深度研究基准测试上的实验表明，实体感知的组相对策略优化持续且显著优于基准方法。进一步分析显示，该框架不仅实现了更高的准确率，还诱导出更高效的推理策略——所需工具调用次数更少，这证明其在搜索代理对齐方面采用了更有效且样本效率更高的方法。",
    "url": "https://huggingface.co/papers/2510.24694",
    "arxiv_url": "https://arxiv.org/abs/2510.24694"
  },
  {
    "title": "STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D\n  Intelligence",
    "summary": "Despite rapid progress in Multi-modal Large Language Models and Large\nAudio-Language Models, existing audio benchmarks largely test semantics that\ncan be recovered from text captions, masking deficits in fine-grained\nperceptual reasoning. We formalize audio 4D intelligence that is defined as\nreasoning over sound dynamics in time and 3D space, and introduce STAR-Bench to\nmeasure it. STAR-Bench combines a Foundational Acoustic Perception setting (six\nattributes under absolute and relative regimes) with a Holistic Spatio-Temporal\nReasoning setting that includes segment reordering for continuous and discrete\nprocesses and spatial tasks spanning static localization, multi-source\nrelations, and dynamic trajectories. Our data curation pipeline uses two\nmethods to ensure high-quality samples. For foundational tasks, we use\nprocedurally synthesized and physics-simulated audio. For holistic data, we\nfollow a four-stage process that includes human annotation and final selection\nbased on human performance. Unlike prior benchmarks where caption-only\nanswering reduces accuracy slightly, STAR-Bench induces far larger drops\n(-31.5\\% temporal, -35.2\\% spatial), evidencing its focus on linguistically\nhard-to-describe cues. Evaluating 19 models reveals substantial gaps compared\nwith humans and a capability hierarchy: closed-source models are bottlenecked\nby fine-grained perception, while open-source models lag across perception,\nknowledge, and reasoning. Our STAR-Bench provides critical insights and a clear\npath forward for developing future models with a more robust understanding of\nthe physical world.",
    "translation": "标题：STAR-Bench：探索作为音频4D智能的深度时空推理能力\n\n摘要：尽管多模态大语言模型与大型音频-语言模型发展迅速，现有音频基准大多仅检验可从文本描述中还原的语义特征，这掩盖了模型在细粒度感知推理方面的缺陷。我们正式提出\"音频4D智能\"概念，即对声音在时间维度和三维空间中动态变化的推理能力，并引入STAR-Bench进行量化评估。该基准集成了基础听觉感知场景（包含绝对与相对两种判别机制下的六类属性）与整体时空推理场景，后者涵盖连续/离散过程的片段重组任务，以及静态定位、多源关联和动态轨迹三类空间任务。我们通过两种方法构建高质量数据：基础任务采用程序化合成与物理仿真音频；整体数据经过四阶段流程，包括人工标注与基于人类表现的最终筛选。相较于现有基准仅使准确率微降的情况，STAR-Bench引发更显著的性能落差（时序任务-31.5%，空间任务-35.2%），证明其聚焦于语言难以描述的感知线索。对19个模型的评估显示：相比人类表现存在明显差距，且呈现能力分层——闭源模型受限于细粒度感知，开源模型则在感知、知识与推理层面全面落后。STAR-Bench为开发具有更扎实物理世界认知能力的新一代模型提供了关键洞见与明确路径。",
    "url": "https://huggingface.co/papers/2510.24693",
    "arxiv_url": "https://arxiv.org/abs/2510.24693"
  },
  {
    "title": "AgentFrontier: Expanding the Capability Frontier of LLM Agents with\n  ZPD-Guided Data Synthesis",
    "summary": "Training large language model agents on tasks at the frontier of their\ncapabilities is key to unlocking advanced reasoning. We introduce a data\nsynthesis approach inspired by the educational theory of the Zone of Proximal\nDevelopment (ZPD), which defines this frontier as tasks an LLM cannot solve\nalone but can master with guidance. To operationalize this, we present the\nAgentFrontier Engine, an automated pipeline that synthesizes high-quality,\nmultidisciplinary data situated precisely within the LLM's ZPD. This engine\nsupports both continued pre-training with knowledge-intensive data and targeted\npost-training on complex reasoning tasks. From the same framework, we derive\nthe ZPD Exam, a dynamic and automated benchmark designed to evaluate agent\ncapabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on\nour synthesized data, which achieves state-of-the-art results on demanding\nbenchmarks like Humanity's Last Exam, even surpassing some leading proprietary\nagents. Our work demonstrates that a ZPD-guided approach to data synthesis\noffers a scalable and effective path toward building more capable LLM agents.",
    "translation": "标题：AgentFrontier：通过最近发展区引导的数据合成拓展大语言模型智能体的能力边界\n\n摘要：在大语言模型智能体能力边界任务上进行训练是解锁高级推理能力的关键。我们提出一种受最近发展区教育理论启发的数据合成方法，将该边界定义为大语言模型无法独立解决但能在引导下掌握的任务。为实现这一目标，我们开发了AgentFrontier引擎——一个自动化流程，能够精准合成位于大语言模型最近发展区内的高质量跨学科数据。该引擎既支持知识密集型数据的持续预训练，也支持复杂推理任务的针对性后训练。基于同一框架，我们构建了ZPD测评——一个动态自动化基准，专门用于评估智能体在边界任务上的表现。通过使用合成数据训练的AgentFrontier-30B-A3B模型，在\"人类终极考试\"等高要求基准测试中取得了最先进成果，甚至超越了部分领先的专有智能体。本研究证明，以最近发展区为指导的数据合成方法，为构建更强大语言模型智能体提供了可扩展的有效路径。",
    "url": "https://huggingface.co/papers/2510.24695",
    "arxiv_url": "https://arxiv.org/abs/2510.24695"
  },
  {
    "title": "Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal\n  Reasoning in MLLMs",
    "summary": "While Multimodal Large Language Models (MLLMs) excel at visual understanding,\nthey often struggle in complex scenarios that require visual planning and\nimagination. Inspired by how humans use sketching as a form of visual thinking\nto develop and communicate ideas, we introduce Latent Sketchpad, a framework\nthat equips MLLMs with an internal visual scratchpad. The internal visual\nrepresentations of MLLMs have traditionally been confined to perceptual\nunderstanding. We repurpose them to support generative visual thought without\ncompromising reasoning ability. Building on frontier MLLMs, our approach\nintegrates visual generation directly into their native autoregressive\nreasoning process. It allows the model to interleave textual reasoning with the\ngeneration of visual latents. These latents guide the internal thought process\nand can be translated into sketch images for interpretability. To realize this,\nwe introduce two components: a Context-Aware Vision Head autoregressively\nproduces visual representations, and a pretrained Sketch Decoder renders these\ninto human-interpretable images. We evaluate the framework on our new dataset\nMazePlanning. Experiments across various MLLMs show that Latent Sketchpad\ndelivers comparable or even superior reasoning performance to their backbone.\nIt further generalizes across distinct frontier MLLMs, including Gemma3 and\nQwen2.5-VL. By extending model's textual reasoning to visual thinking, our\nframework opens new opportunities for richer human-computer interaction and\nbroader applications. More details and resources are available on our project\npage: https://latent-sketchpad.github.io/.",
    "translation": "标题：潜在画板：通过视觉草图激发多模态大语言模型的多模态推理能力\n\n摘要：尽管多模态大语言模型在视觉理解方面表现出色，但在需要视觉规划与想象力的复杂场景中往往表现不佳。受人类通过草图作为视觉思维来发展和交流想法的启发，我们提出了潜在画板框架，为多模态大语言模型配备内部视觉草稿本。传统上，多模态大语言模型的内部视觉表征主要局限于感知理解。我们重新定位其功能，在不影响推理能力的前提下支持生成式视觉思维。基于前沿多模态大语言模型，我们的方法将视觉生成直接整合到其原生自回归推理过程中，使模型能够将文本推理与视觉潜在表征的生成交错进行。这些潜在表征既能引导内部思维过程，也可通过解码生成可解释的草图图像。为实现这一目标，我们引入两个核心组件：上下文感知视觉头通过自回归方式生成视觉表征，预训练草图解码器则将这些表征渲染为人类可理解的图像。我们在新构建的迷宫规划数据集上对该框架进行评估。实验结果表明，潜在画板在不同多模态大语言模型中均能取得与骨干模型相当或更优的推理性能，并成功泛化至Gemma3和Qwen2.5-VL等前沿模型。通过将模型的文本推理能力拓展至视觉思维领域，我们的框架为人机交互的丰富化和应用场景的拓展开辟了新途径。更多细节与资源请访问项目页面：https://latent-sketchpad.github.io/。",
    "url": "https://huggingface.co/papers/2510.24514",
    "arxiv_url": "https://arxiv.org/abs/2510.24514"
  },
  {
    "title": "Critique-RL: Training Language Models for Critiquing through Two-Stage\n  Reinforcement Learning",
    "summary": "Training critiquing language models to assess and provide feedback on model\noutputs is a promising way to improve LLMs for complex reasoning tasks.\nHowever, existing approaches typically rely on stronger supervisors for\nannotating critique data. To address this, we propose Critique-RL, an online RL\napproach for developing critiquing language models without stronger\nsupervision. Our approach operates on a two-player paradigm: the actor\ngenerates a response, the critic provides feedback, and the actor refines the\nresponse accordingly. We first reveal that relying solely on indirect reward\nsignals from the actor's outputs for RL optimization often leads to\nunsatisfactory critics: while their helpfulness (i.e., providing constructive\nfeedback) improves, the discriminability (i.e., determining whether a response\nis high-quality or not) remains poor, resulting in marginal performance gains.\nTo overcome this, Critique-RL adopts a two-stage optimization strategy. In\nstage I, it reinforces the discriminability of the critic with direct\nrule-based reward signals; in stage II, it introduces indirect rewards based on\nactor refinement to improve the critic's helpfulness, while maintaining its\ndiscriminability via appropriate regularization. Extensive experiments across\nvarious tasks and models show that Critique-RL delivers substantial performance\nimprovements. For example, it achieves a 9.02% gain on in-domain tasks and a\n5.70% gain on out-of-domain tasks for Qwen2.5-7B, highlighting its potential.",
    "translation": "标题：Critique-RL：基于两阶段强化学习的评述语言模型训练方法\n\n摘要：训练具备评述能力的语言模型以评估模型输出并提供反馈，是提升大语言模型在复杂推理任务表现的有效途径。然而现有方法通常依赖更强监督源进行评述数据标注。为此，我们提出Critique-RL——一种无需强监督即可开发评述语言模型的在线强化学习方法。该方法采用双智能体交互范式：执行者生成初始回答，评述者提供反馈，执行者据此优化回答。我们首先发现，仅依靠执行者输出产生的间接奖励信号进行强化学习优化，往往导致评述模型存在缺陷：其帮助性（即提供建设性反馈）虽有所提升，但判别能力（即判断回答质量优劣）仍然不足，最终导致性能提升有限。为突破此局限，Critique-RL采用两阶段优化策略：第一阶段通过基于规则的直接奖励信号强化评述者的判别能力；第二阶段引入基于执行者优化效果的间接奖励来提升评述者的帮助性，同时通过适当的正则化手段维持其判别能力。在多任务和多模型的广泛实验表明，Critique-RL能带来显著的性能提升。以Qwen2.5-7B模型为例，其在领域内任务和领域外任务上分别实现了9.02%和5.70%的性能增益，彰显了该方法的应用潜力。",
    "url": "https://huggingface.co/papers/2510.24320",
    "arxiv_url": "https://arxiv.org/abs/2510.24320"
  },
  {
    "title": "VisCoder2: Building Multi-Language Visualization Coding Agents",
    "summary": "Large language models (LLMs) have recently enabled coding agents capable of\ngenerating, executing, and revising visualization code. However, existing\nmodels often fail in practical workflows due to limited language coverage,\nunreliable execution, and lack of iterative correction mechanisms. Progress has\nbeen constrained by narrow datasets and benchmarks that emphasize single-round\ngeneration and single-language tasks. To address these challenges, we introduce\nthree complementary resources for advancing visualization coding agents.\nVisCode-Multi-679K is a large-scale, supervised dataset containing 679K\nvalidated and executable visualization samples with multi-turn correction\ndialogues across 12 programming languages. VisPlotBench is a benchmark for\nsystematic evaluation, featuring executable tasks, rendered outputs, and\nprotocols for both initial generation and multi-round self-debug. Finally, we\npresent VisCoder2, a family of multi-language visualization models trained on\nVisCode-Multi-679K. Experiments show that VisCoder2 significantly outperforms\nstrong open-source baselines and approaches the performance of proprietary\nmodels like GPT-4.1, with further gains from iterative self-debug, reaching\n82.4% overall execution pass rate at the 32B scale, particularly in symbolic or\ncompiler-dependent languages.",
    "translation": "标题：VisCoder2：构建多语言可视化编程智能体\n\n摘要：大语言模型近期催生了能够生成、执行及修正可视化代码的编程智能体。然而现有模型因语言覆盖有限、执行可靠性不足且缺乏迭代修正机制，在实际工作流程中往往表现不佳。该领域发展受限于侧重单轮生成和单一语言任务的狭窄数据集与评估基准。为应对这些挑战，我们提出三项互补资源以推进可视化编程智能体发展：VisCode-Multi-679K作为大规模监督数据集，包含67.9万经过验证的可执行可视化样本，涵盖12种编程语言的多轮修正对话；VisPlotBench作为系统化评估基准，具备可执行任务、渲染输出及支持初始生成与多轮自调试的评估方案；最终我们基于VisCode-Multi-679K训练出多语言可视化模型系列VisCoder2。实验表明，VisCoder2显著超越主流开源基线模型，并逼近GPT-4.1等专有模型性能，通过迭代自调试进一步将总体执行通过率提升至82.4%（320亿参数规模），在符号化或依赖编译器的编程语言中表现尤为突出。",
    "url": "https://huggingface.co/papers/2510.23642",
    "arxiv_url": "https://arxiv.org/abs/2510.23642"
  },
  {
    "title": "ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking",
    "summary": "Parallel thinking expands exploration breadth, complementing the deep\nexploration of information-seeking (IS) agents to further enhance\nproblem-solving capability. However, conventional parallel thinking faces two\nkey challenges in this setting: inefficiency from repeatedly rolling out from\nscratch, and difficulty in integrating long-horizon reasoning trajectories\nduring answer generation, as limited context capacity prevents full\nconsideration of the reasoning process. To address these issues, we propose\nParallelMuse, a two-stage paradigm designed for deep IS agents. The first\nstage, Functionality-Specified Partial Rollout, partitions generated sequences\ninto functional regions and performs uncertainty-guided path reuse and\nbranching to enhance exploration efficiency. The second stage, Compressed\nReasoning Aggregation, exploits reasoning redundancy to losslessly compress\ninformation relevant to answer derivation and synthesize a coherent final\nanswer. Experiments across multiple open-source agents and benchmarks\ndemonstrate up to 62% performance improvement with a 10--30% reduction in\nexploratory token consumption.",
    "translation": "标题：ParallelMuse：面向深度信息检索的智能体并行思考框架\n\n摘要：并行思考通过扩展探索广度，与信息检索智能体的深度探索形成互补，从而进一步提升问题解决能力。然而传统并行思考在此场景下面临两大挑战：因重复从头展开导致的低效性问题，以及在答案生成阶段难以整合长程推理轨迹——有限的上下文容量阻碍了对完整推理过程的充分考量。为解决这些问题，我们提出ParallelMuse这一面向深度信息检索智能体的双阶段范式。第一阶段\"功能指定式部分展开\"将生成序列划分为功能区域，通过不确定性引导的路径复用与分支策略提升探索效率；第二阶段\"压缩推理聚合\"利用推理冗余性，对答案推导相关的信息进行无损压缩并合成连贯的最终答案。在多个开源智能体与基准测试上的实验表明，该方法在减少10-30%探索性令牌消耗的同时，最高可实现62%的性能提升。",
    "url": "https://huggingface.co/papers/2510.24698",
    "arxiv_url": "https://arxiv.org/abs/2510.24698"
  },
  {
    "title": "ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining,\n  Finetuning, and Decoding the Curse of Multilinguality",
    "summary": "Scaling laws research has focused overwhelmingly on English -- yet the most\nprominent AI models explicitly serve billions of international users. In this\nwork, we undertake the largest multilingual scaling laws study to date,\ntotaling 774 multilingual training experiments, spanning 10M-8B model\nparameters, 400+ training languages and 48 evaluation languages. We introduce\nthe Adaptive Transfer Scaling Law (ATLAS) for both monolingual and multilingual\npretraining, which outperforms existing scaling laws' out-of-sample\ngeneralization often by more than 0.3 R^2. Our analyses of the experiments shed\nlight on multilingual learning dynamics, transfer properties between languages,\nand the curse of multilinguality. First, we derive a cross-lingual transfer\nmatrix, empirically measuring mutual benefit scores between 38 x 38=1444\nlanguage pairs. Second, we derive a language-agnostic scaling law that reveals\nhow to optimally scale model size and data when adding languages without\nsacrificing performance. Third, we identify the computational crossover points\nfor when to pretrain from scratch versus finetune from multilingual\ncheckpoints. We hope these findings provide the scientific foundation for\ndemocratizing scaling laws across languages, and enable practitioners to\nefficiently scale models -- beyond English-first AI.",
    "translation": "标题：ATLAS：面向多语言预训练、微调及破解多语诅咒的自适应迁移缩放法则\n\n摘要：缩放法则研究长期以来过度聚焦于英语语种，然而最前沿的AI模型明确服务于全球数十亿用户。本研究开展了迄今规模最大的多语言缩放法则实验，累计完成774组多语言训练，覆盖1000万至80亿参数规模、400余种训练语言及48种评估语言。我们提出了适用于单语与多语预训练的自适应迁移缩放法则（ATLAS），其样本外泛化能力较现有缩放法则平均提升超过0.3个R²指标。通过实验分析，我们揭示了多语言学习动态机制、语言间迁移特性以及多语诅咒现象：首先推导出跨语言迁移矩阵，实证测量38×38=1444组语言对的互惠分值；其次建立语言无关的缩放法则，揭示在扩展语言规模时如何优化模型参数与数据配比以保持性能；最后确定了从头预训练与基于多语检查点微调的计算效益临界点。这些发现有望为跨语言缩放法则的民主化奠定科学基础，助力实践者突破英语优先的AI发展范式，实现模型的高效扩展。\n\n摘要：\n[本研究突破传统缩放法则研究对英语语种的局限，通过774组多语言训练实验（涵盖1000万-80亿参数、400+训练语言、48种评估语言），提出自适应迁移缩放法则（ATLAS）。该法则在单语/多语预训练中显著提升泛化能力（R²平均提高0.3+），并揭示三大发现：1）构建38×38语言对迁移矩阵，量化1444组语言互惠关系；2）建立语言无关缩放定律，明确多语扩展时的最优参数-数据配比；3）界定从头预训练与多语微调的计算效益临界点。研究成果为跨语言缩放法则民主化奠定理论基础，推动AI突破英语中心主义的发展范式。]",
    "url": "https://huggingface.co/papers/2510.22037",
    "arxiv_url": "https://arxiv.org/abs/2510.22037"
  },
  {
    "title": "FunReason-MT Technical Report: Overcoming the Complexity Barrier in\n  Multi-Turn Function Calling",
    "summary": "Function calling (FC) empowers large language models (LLMs) and autonomous\nagents to interface with external tools, a critical capability for solving\ncomplex, real-world problems. As this ability becomes increasingly central to\nadvanced AI systems, the need for high-quality, multi-turn training data to\ndevelop and refine it cannot be overstated. Existing data synthesis methods,\nsuch as random environment sampling or multi-agent role-playing, are not\npowerful enough to generate high-quality data in real-world environments.\nPractical challenges come in three folds: targeted model training, isolation of\ntool architecture, and multi-turn logical dependency. To address these\nstructural deficiencies, we present FunReason-MT, a novel data synthesis\nframework for real-world multi-turn tool use. FunReason-MT resolves the\ncomplexity barrier in multi-turn FC data by employing 1) Environment-API Graph\nInteractions to gather varied high-quality trajectories, 2) Advanced Tool-Query\nSynthesis to simplify hard query construction, and 3) Guided Iterative Chain\nfor sophisticated CoT generation. Evaluations on Berkeley Function-Calling\nLeaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built\nupon FunReason-MT generated data achieves state-of-the-art performance among\ncomparable-sized models, outperforming most close-source models. Further\nperformance improvements on BFCLv4 confirm that FunReason-MT provides a\nreliable and robust source for agentic learning.",
    "translation": "标题：FunReason-MT技术报告：突破多轮函数调用的复杂性壁垒\n\n摘要：函数调用（FC）使大语言模型（LLM）与自主智能体能够对接外部工具，这是解决复杂现实问题的关键能力。随着该能力在先进人工智能系统中日益重要，对高质量多轮训练数据用以开发和完善该能力的需求不容忽视。现有数据合成方法（如随机环境采样或多智能体角色扮演）在现实环境中不足以生成高质量数据。实际挑战主要体现在三个方面：定向模型训练、工具架构隔离和多轮逻辑依赖。为克服这些结构性缺陷，我们提出FunReason-MT——面向现实场景多轮工具使用的新型数据合成框架。该框架通过以下方式突破多轮FC数据的复杂性壁垒：1）采用环境-API图交互收集多样化高质量轨迹；2）通过高级工具-查询合成简化复杂查询构建；3）利用引导式迭代链生成精细思维链。在伯克利函数调用排行榜（BFCLv3）上的评估证明了我们框架的强大性能：基于FunReason-MT生成数据训练的40亿参数模型，在同等规模模型中达到最优性能，超越多数闭源模型。在BFCLv4上的持续性能提升进一步证实，FunReason-MT为智能体学习提供了可靠且稳健的数据支撑。",
    "url": "https://huggingface.co/papers/2510.24645",
    "arxiv_url": "https://arxiv.org/abs/2510.24645"
  },
  {
    "title": "ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?",
    "summary": "Frontier AI agents show increasing promise as scientific research assistants,\nand may eventually be useful for extended, open-ended research workflows.\nHowever, in order to use agents for novel research, we must first assess the\nunderlying faithfulness and correctness of their work. To evaluate agents as\nresearch assistants, we introduce ReplicationBench, an evaluation framework\nthat tests whether agents can replicate entire research papers drawn from the\nastrophysics literature. Astrophysics, where research relies heavily on\narchival data and computational study while requiring little real-world\nexperimentation, is a particularly useful testbed for AI agents in scientific\nresearch. We split each paper into tasks which require agents to replicate the\npaper's core contributions, including the experimental setup, derivations, data\nanalysis, and codebase. Each task is co-developed with the original paper\nauthors and targets a key scientific result, enabling objective evaluation of\nboth faithfulness (adherence to original methods) and correctness (technical\naccuracy of results). ReplicationBench is extremely challenging for current\nfrontier language models: even the best-performing language models score under\n20%. We analyze ReplicationBench trajectories in collaboration with domain\nexperts and find a rich, diverse set of failure modes for agents in scientific\nresearch. ReplicationBench establishes the first benchmark of paper-scale,\nexpert-validated astrophysics research tasks, reveals insights about agent\nperformance generalizable to other domains of data-driven science, and provides\na scalable framework for measuring AI agents' reliability in scientific\nresearch.",
    "translation": "标题：ReplicationBench：人工智能代理能否复现天体物理学研究论文？\n\n摘要：前沿人工智能代理作为科研助手展现出日益广阔的前景，最终或可应用于扩展性开放研究流程。然而要将代理应用于创新性研究，我们首先需要评估其工作的本质忠实度与正确性。为此我们推出ReplicationBench评估框架，通过测试代理能否完整复现天体物理学领域的研究论文来评估其科研辅助能力。天体物理学研究主要依赖档案数据与计算分析而无需现实实验，这为人工智能代理提供了理想的科研测试场。我们将每篇论文解构为若干任务，要求代理复现论文的核心成果，包括实验设置、公式推导、数据分析和代码库重建。每个任务均与论文原作者共同设计，聚焦关键科学结论，从而实现对忠实度（遵循原始方法的程度）与正确性（成果技术准确性）的客观评估。当前最先进的语言模型在ReplicationBench上表现欠佳：最优模型的得分仍低于20%。通过与领域专家联合分析任务执行轨迹，我们发现了科研代理存在丰富多样的失效模式。ReplicationBench建立了首个经专家验证的论文级天体物理研究任务基准，揭示了可推广至其他数据驱动科学领域的代理性能洞见，并为衡量人工智能代理在科学研究中的可靠性提供了可扩展的评估框架。",
    "url": "https://huggingface.co/papers/2510.24591",
    "arxiv_url": "https://arxiv.org/abs/2510.24591"
  },
  {
    "title": "Generalization or Memorization: Dynamic Decoding for Mode Steering",
    "summary": "Large Language Models (LLMs) exhibit a troubling duality, capable of both\nremarkable generalization and brittle, verbatim memorization of their training\ndata. This unpredictability undermines their reliability in high-stakes\napplications. In this work, we propose a unified framework to understand,\nidentify, and control these distinct reasoning modes. First, we introduce a\ntheoretical model based on the Information Bottleneck (IB) principle,\nformalizing generalization as the learning of a compressed, task-relevant\nrepresentation and memorization as a failure to compress. Building on this\ntheory, we develop Dynamic Mode Steering (DMS), a novel inference-time\nalgorithm which comprises two components: (1) a lightweight, causally-grounded\nlinear probe that identifies the model's instantaneous reliance on\nmemorization, and (2) a dynamic activation steering mechanism that nudges the\nmodel's computation towards pre-identified generalization circuits. We frame\nDMS as a form of adaptive, self-contrastive decoding. Experiments on reasoning\nand faithfulness tasks demonstrate that DMS significantly improves logical\nconsistency and factual accuracy, thereby offering a principled approach to\nenhancing LLM reliability.",
    "translation": "标题：泛化抑或记忆：面向模式调控的动态解码机制\n\n摘要：大型语言模型呈现出令人担忧的双重特性：既能实现卓越的泛化能力，又可能对其训练数据产生僵化的逐字记忆。这种不可预测性严重影响了其在高风险应用中的可靠性。本研究提出统一框架以理解、识别并控制这两种不同的推理模式。首先，我们基于信息瓶颈原理构建理论模型，将泛化形式化为对压缩化任务相关表征的学习过程，而将记忆定义为压缩失败的表现。基于该理论，我们开发了动态模式调控技术——一种新型推理时算法，包含两个核心组件：（1）基于因果关系的轻量级线性探测器，用于识别模型对记忆机制的瞬时依赖程度；（2）动态激活导向机制，将模型计算过程引导至预先识别的泛化回路。我们将DMS框架定义为一种自适应自对比解码范式。在推理任务和事实一致性任务上的实验表明，该技术能显著提升逻辑连贯性与事实准确性，从而为增强大型语言模型可靠性提供了理论完备的解决方案。",
    "url": "https://huggingface.co/papers/2510.22099",
    "arxiv_url": "https://arxiv.org/abs/2510.22099"
  },
  {
    "title": "Rethinking Visual Intelligence: Insights from Video Pretraining",
    "summary": "Large language models (LLMs) have demonstrated that large-scale pretraining\nenables systems to adapt rapidly to new problems with little supervision in the\nlanguage domain. This success, however, has not translated as effectively to\nthe visual domain, where models, including LLMs, continue to struggle with\ncompositional understanding, sample efficiency, and general-purpose\nproblem-solving. We investigate Video Diffusion Models (VDMs) as a promising\ndirection for bridging this gap. Pretraining on spatiotemporal data endows\nthese models with strong inductive biases for structure and dynamics, which we\nhypothesize can support broad task adaptability. To test this, we design a\ncontrolled evaluation in which both a pretrained LLM and a pretrained VDM are\nequipped with lightweight adapters and presented with tasks in their natural\nmodalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,\nroute planning, and cellular automata, VDMs demonstrate higher data efficiency\nthan their language counterparts. Taken together, our results indicate that\nvideo pretraining offers inductive biases that support progress toward visual\nfoundation models.",
    "translation": "标题：视觉智能的再思考：基于视频预训练的启示\n\n摘要：大型语言模型（LLM）已证明在大规模预训练条件下，系统能够以极少监督快速适应语言领域的新问题。然而，这种成功尚未有效迁移至视觉领域——包括LLM在内的各类模型仍在组合理解、样本效率和通用问题解决能力方面面临挑战。本研究探索视频扩散模型（VDM）作为弥合这一差距的有效路径。通过对时空数据进行预训练，这类模型获得了对结构与动态的强归纳偏置，我们推测这种特性可支撑广泛的任务适应能力。为验证此假设，我们设计了对照实验：同时为预训练LLM和预训练VDM配备轻量化适配器，使其在各自自然模态下执行任务。在ARC-AGI、ConceptARC、视觉游戏、路径规划和元胞自动机等基准测试中，VDM展现出优于语言模型的数据效率。综合结果表明，视频预训练所提供的归纳偏置为构建视觉基础模型提供了重要推进路径。",
    "url": "https://huggingface.co/papers/2510.24448",
    "arxiv_url": "https://arxiv.org/abs/2510.24448"
  },
  {
    "title": "VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a\n  Unified Concept Set",
    "summary": "The alignment of vision-language representations endows current\nVision-Language Models (VLMs) with strong multi-modal reasoning capabilities.\nHowever, the interpretability of the alignment component remains uninvestigated\ndue to the difficulty in mapping the semantics of multi-modal representations\ninto a unified concept set. To address this problem, we propose VL-SAE, a\nsparse autoencoder that encodes vision-language representations into its hidden\nactivations. Each neuron in its hidden layer correlates to a concept\nrepresented by semantically similar images and texts, thereby interpreting\nthese representations with a unified concept set. To establish the\nneuron-concept correlation, we encourage semantically similar representations\nto exhibit consistent neuron activations during self-supervised training.\nFirst, to measure the semantic similarity of multi-modal representations, we\nperform their alignment in an explicit form based on cosine similarity. Second,\nwe construct the VL-SAE with a distance-based encoder and two modality-specific\ndecoders to ensure the activation consistency of semantically similar\nrepresentations. Experiments across multiple VLMs (e.g., CLIP, LLaVA)\ndemonstrate the superior capability of VL-SAE in interpreting and enhancing the\nvision-language alignment. For interpretation, the alignment between vision and\nlanguage representations can be understood by comparing their semantics with\nconcepts. For enhancement, the alignment can be strengthened by aligning\nvision-language representations at the concept level, contributing to\nperformance improvements in downstream tasks, including zero-shot image\nclassification and hallucination elimination. Codes are available at\nhttps://github.com/ssfgunner/VL-SAE.",
    "translation": "标题：VL-SAE：基于统一概念集的视觉语言对齐机制解析与增强\n\n摘要：视觉语言表征的对齐机制赋予当前视觉语言模型强大的多模态推理能力。然而由于难以将多模态表征语义映射到统一概念集，该对齐组件的可解释性研究尚属空白。为解决此问题，我们提出稀疏自编码器VL-SAE，其通过隐藏层激活值对视觉语言表征进行编码。隐藏层中每个神经元与由语义相似的图像和文本所表征的概念相关联，从而通过统一概念集实现对这些表征的语义解析。为建立神经元-概念关联，我们在自监督训练中促使语义相似的表征保持一致的神经元激活：首先基于余弦相似度以显式形式实现多模态表征对齐，从而度量其语义相似性；其次构建具有距离编码器和双模态专用解码器的VL-SAE架构，确保语义相似表征的激活一致性。在多个视觉语言模型（如CLIP、LLaVA）上的实验表明，VL-SAE在解析和增强视觉语言对齐方面具有卓越能力。在解析层面，通过对比视觉与语言表征与概念集的语义关联即可理解其对齐机制；在增强层面，通过概念级的视觉语言表征对齐可强化对齐效果，进而提升零样本图像分类和幻觉消除等下游任务性能。代码已发布于https://github.com/ssfgunner/VL-SAE。",
    "url": "https://huggingface.co/papers/2510.21323",
    "arxiv_url": "https://arxiv.org/abs/2510.21323"
  },
  {
    "title": "PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D\n  Part Understanding",
    "summary": "Understanding objects at the level of their constituent parts is fundamental\nto advancing computer vision, graphics, and robotics. While datasets like\nPartNet have driven progress in 3D part understanding, their reliance on\nuntextured geometries and expert-dependent annotation limits scalability and\nusability. We introduce PartNeXt, a next-generation dataset addressing these\ngaps with over 23,000 high-quality, textured 3D models annotated with\nfine-grained, hierarchical part labels across 50 categories. We benchmark\nPartNeXt on two tasks: (1) class-agnostic part segmentation, where\nstate-of-the-art methods (e.g., PartField, SAMPart3D) struggle with\nfine-grained and leaf-level parts, and (2) 3D part-centric question answering,\na new benchmark for 3D-LLMs that reveals significant gaps in open-vocabulary\npart grounding. Additionally, training Point-SAM on PartNeXt yields substantial\ngains over PartNet, underscoring the dataset's superior quality and diversity.\nBy combining scalable annotation, texture-aware labels, and multi-task\nevaluation, PartNeXt opens new avenues for research in structured 3D\nunderstanding.",
    "translation": "标题：PartNeXt：面向细粒度分层三维部件理解的下一代数据集\n\n摘要：在构件级别理解物体是推动计算机视觉、图形学和机器人技术发展的基础。尽管PartNet等数据集推动了三维部件理解的发展，但其对无纹理几何体和专家依赖型标注的依赖限制了可扩展性和可用性。我们推出PartNeXt这一新一代数据集，通过包含超过23,000个高质量带纹理三维模型（涵盖50个类别并标注细粒度分层部件标签）来解决这些缺陷。我们在两个任务上对PartNeXt进行基准测试：（1）类别无关部件分割——现有前沿方法（如PartField、SAMPart3D）在细粒度和叶级部件识别方面表现不佳；（2）三维部件中心问答——作为面向三维大语言模型的新基准，该任务揭示了开放词汇部件定位能力的显著不足。此外，使用PartNeXt训练Point-SAM相比PartNet带来显著性能提升，印证了该数据集更优的质量与多样性。通过结合可扩展标注、纹理感知标签和多任务评估，PartNeXt为结构化三维理解研究开辟了新途径。",
    "url": "https://huggingface.co/papers/2510.20155",
    "arxiv_url": "https://arxiv.org/abs/2510.20155"
  }
]