<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face 论文日报 - 2025-10-30</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background-color: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 28px;
        }
        
        h1 img {
            vertical-align: middle;
            margin-right: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 24px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        
        h3 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 20px;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        ul {
            margin-left: 20px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        strong {
            color: #2c3e50;
            font-weight: 600;
        }
        
        hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin: 30px 0;
        }
        
        /* 关键修复:限制图片宽度 */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        /* 确保图片容器也有宽度限制 */
        p img {
            max-width: 100%;
        }
        
        /* 论文详情区域样式 */
        .paper-section {
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 统计信息样式 */
        .stats {
            background-color: #e8f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
        }
        
        /* 响应式设计 */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1><img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png" width="30"/> Hugging Face 2025-10-30 论文日报</h1>
<h2>📊 今日论文统计</h2>
<ul>
<li>总论文数：16</li>
<li>热门领域：GPT, LLM, Transformer, RL</li>
</ul>
<h2>📝 论文详情</h2>
<h3>1. Video-Thinker：通过强化学习实现“视频思维”的突破</h3>
<p><strong>原文标题：</strong> Video-Thinker: Sparking "Thinking with Videos" via Reinforcement
  Learning</p>
<p><strong>摘要：</strong>
近期图像推理方法（尤其是“图像思维”范式）在多模态大语言模型中取得了显著成功，然而这种动态推理范式尚未扩展至视频推理任务。本文提出Video-Thinker模型，通过自主利用模型固有的“定位”与“描述”能力，在推理过程中持续生成思维线索，使多模态大语言模型具备视频思维能力。为激发此能力，我们构建了Video-Thinker-10K数据集，该精选数据集在思维链推理序列中呈现了自主工具使用特性。我们的训练策略首先通过监督微调学习推理格式，继而采用组相对策略优化强化推理能力。该方法使多模态大语言模型无需构建和调用外部工具，即可自主完成视频推理中的定位与描述任务。大量实验表明，Video-Thinker在领域内任务及具有挑战性的领域外视频推理基准（包括Video-Holmes、CG-Bench-Reasoning和VRBench）上均取得显著性能提升。我们的Video-Thinker-7B模型显著优于Video-R1等现有基线，在70亿参数规模的多模态大语言模型中确立了最先进的性能水平。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2510.23473">HuggingFace</a> | <a href="https://arxiv.org/abs/2510.23473">arXiv</a></p>
<hr />
<h3>2. JanusCoder：构建面向代码智能的基础视觉-编程交互框架</h3>
<p><strong>原文标题：</strong> JanusCoder: Towards a Foundational Visual-Programmatic Interface for
  Code Intelligence</p>
<p><strong>摘要：</strong>
神经代码智能的研究范畴正从基于文本的源代码快速扩展至程序生成的丰富视觉输出。这一视觉维度对于灵活内容生成和可视化作品的程序化精准编辑等高级应用至关重要。然而，高质量多模态代码数据的稀缺阻碍了研究进展，这一瓶颈源于数据合成与质量评估的双重挑战。为应对这些挑战，我们从数据构建与模型设计两个维度作出贡献：首先提出一套完整的合成工具包，通过利用数据模态间的协同效应，高效构建涵盖标准图表、复杂交互式网页界面及代码驱动动画的大规模高质量语料库。基于该工具包，我们构建了迄今规模最大的多模态代码数据集JanusCode-800K。以此为基础，我们训练了JanusCoder与JanusCoderV系列模型，建立起支持通过文本指令、视觉输入或其组合生成代码的视觉-编程交互接口。我们的统一模型突破了现有方法为孤立任务构建专用模型的局限。在文本主导与视觉主导的编码任务上的大量实验表明，JanusCoder系列性能卓越，其70亿至140亿参数规模的模型在多项指标上接近甚至超越商业模型。此外，深入分析为协调程序逻辑与视觉表达提供了关键见解。我们的代码与模型检查点已发布于https://github.com/InternLM/JanusCoder。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2510.23538">HuggingFace</a> | <a href="https://arxiv.org/abs/2510.23538">arXiv</a></p>
<hr />
<h3>3. RegionE：面向高效图像编辑的自适应区域感知生成框架</h3>
<p><strong>原文标题：</strong> RegionE: Adaptive Region-Aware Generation for Efficient Image Editing</p>
<p><strong>摘要：</strong>
近年来，基于指令的图像编辑技术受到广泛关注。在实际应用中，该技术通常仅需修改图像的特定区域，而其余大部分区域保持不变。尽管这两类区域在生成难度和计算冗余性上存在显著差异，但现有模型未考虑这种区别，而是对整个图像采用统一的生成流程。为此，我们提出RegionE——一种无需额外训练的自适应区域感知生成框架，可有效加速图像编辑任务。该框架包含三个核心组件：1）自适应区域划分。通过观测发现未编辑区域的生成轨迹呈线性特征，支持通过单步推理实现多步去噪预测。因此在早期去噪阶段，我们根据最终预估结果与参考图像的差异，将图像划分为编辑区域和未编辑区域；2）区域感知生成。完成区域划分后，对未编辑区域用单步预测替代多步去噪；对于生成轨迹呈弯曲特征的编辑区域，则采用局部迭代去噪。为提升局部迭代生成的效率与质量，我们提出区域指令键值缓存技术，在降低计算成本的同时融合全局信息；3）自适应速度衰减缓存。通过观测发现编辑区域相邻时间步的速度向量具有强相似性，我们进一步提出自适应速度衰减缓存机制以加速局部去噪过程。我们将RegionE应用于包括Step1X-Edit、FLUX.1 Kontext和Qwen-Image-Edit在内的前沿基础模型，分别实现了2.57倍、2.41倍和2.06倍的加速效果。经GPT-4o评估验证，该方法在保持语义一致性和视觉保真度方面均表现优异。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2510.25590">HuggingFace</a> | <a href="https://arxiv.org/abs/2510.25590">arXiv</a></p>
<hr />
<h3>4. 基于循环语言模型的潜在推理规模化研究</h3>
<p><strong>原文标题：</strong> Scaling Latent Reasoning via Looped Language Models</p>
<p><strong>摘要：</strong>
现代大语言模型主要通过显式文本生成（如思维链）来实现"思考"机制，这种设计将推理过程后置于训练阶段，未能充分利用预训练数据。我们提出并开源了以递归衔尾蛇命名的Ouro模型系列——一种基于预训练的循环语言模型架构，该模型通过以下方式将推理能力内建于预训练阶段：(i)在潜在空间进行迭代计算，(ii)采用熵正则化目标实现自适应深度分配，(iii)扩展至7.7万亿训练令牌。Ouro 1.4B和2.6B模型在广泛基准测试中表现出卓越性能，其效果可媲美当前最优的120亿参数大语言模型。通过受控实验表明，该优势并非源于知识容量的提升，而是来自更卓越的知识操纵能力。研究还证明，与显式思维链相比，循环语言模型生成的推理轨迹与最终输出具有更高一致性。我们的研究成果揭示了循环语言模型作为推理时代新型扩展方向的潜力。模型获取地址：http://ouro-llm.github.io。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2510.25741">HuggingFace</a> | <a href="https://arxiv.org/abs/2510.25741">arXiv</a></p>
<hr />
<h3>5. 工具十项全能：面向多样化、真实化及长周期任务执行的语言智能体基准测试</h3>
<p><strong>原文标题：</strong> The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic,
  and Long-Horizon Task Execution</p>
<p><strong>摘要：</strong>
现实世界的语言智能体需要处理跨应用程序的复杂多步骤工作流。例如，智能体可能通过协调日历和文件系统来管理电子邮件，或根据操作手册监控生产数据库以检测异常并生成报告。然而，现有语言智能体基准测试往往聚焦于狭窄领域或简化任务，缺乏评估智能体真实性能所需的多样性、真实性和长周期复杂性。为弥补这一空白，我们推出工具十项全能（Toolathlon）基准测试，该测试通过提供多样化应用程序与工具、真实环境设置及可靠的执行评估机制，对语言智能体进行综合评估。Toolathlon涵盖32个软件应用和604种工具，范围从谷歌日历和Notion等日常平台，到WooCommerce、Kubernetes和BigQuery等专业系统。大部分工具基于我们修订或自主实现的高质量模型上下文协议（MCP）服务器构建。与既往主要确保功能真实性但环境状态多样性有限的研究不同，我们提供了来自真实软件的初始环境状态，例如包含数十名学生的Canvas课程系统或真实财务报表。该基准测试共包含108项人工采集或精心设计的任务，平均需要跨20个交互轮次与多个应用程序进行交互才能完成。每项任务均通过专用评估脚本进行严格验证。对前沿模型的综合评估揭示了其显著缺陷：表现最佳的Claude-4.5-Sonnet模型成功率仅为38.6%，平均需要20.2次工具调用；而表现最佳的开源模型DeepSeek-V3.2-Exp成功率仅为20.1%。我们期待Toolathlon能推动面向现实世界长周期任务执行的更强大语言智能体的发展。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2510.25726">HuggingFace</a> | <a href="https://arxiv.org/abs/2510.25726">arXiv</a></p>
<hr />
<h3>6. 基于过程挖掘的推理感知分组相对策略优化</h3>
<p><strong>原文标题：</strong> Reasoning-Aware GRPO using Process Mining</p>
<p><strong>摘要：</strong>
基于强化学习的事后训练方法对于实现大型推理模型的多步推理能力至关重要，但现有奖励机制通常仅关注最终结果。我们提出PM4GRPO方法，这是一种推理感知的分组相对策略优化框架，通过在标准答案/格式奖励基础上引入推理过程的评估信号。该方法运用过程挖掘技术计算标量一致性奖励，用于量化策略模型与预训练教师模型在推理路径上的吻合程度。在五个基准测试上的实证结果表明，PM4GRPO显著优于现有的基于GRPO的事后训练方法。这些发现证实，利用过程挖掘技术实现推理感知的GRPO能有效提升策略模型的推理能力。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2510.25065">HuggingFace</a> | <a href="https://arxiv.org/abs/2510.25065">arXiv</a></p>
<hr />
<h3>7. VFXMaster：基于上下文学习的动态视觉特效生成框架</h3>
<p><strong>原文标题：</strong> VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context
  Learning</p>
<p><strong>摘要：</strong>
视觉特效对数字媒体的表现力具有关键作用，但其创作仍是生成式人工智能面临的重大挑战。现有方法普遍采用"单一特效对应单一LoRA"的范式，这种模式不仅资源消耗大，且本质上无法泛化至未见过的新特效，从而限制了系统的扩展性与创作自由度。为解决这一难题，我们提出VFXMaster——首个基于参考视频的统一视觉特效生成框架。该框架将特效生成重构为上下文学习任务，能够将参考视频中的多样化动态特效精准迁移至目标内容。特别值得关注的是，本方法对未见过的特效类别展现出卓越的泛化能力。具体而言，我们设计了上下文条件策略，通过参考样本对模型进行提示；创新性地引入上下文注意力掩码机制，精确解耦并注入核心特效属性，使单一统一模型在避免信息泄露的前提下实现特效仿现。此外，我们提出高效的单样本特效自适应机制，可基于用户提供的单段视频快速提升对复杂未知特效的泛化能力。大量实验证明，本方法能有效仿现多类别特效信息，并对域外特效表现出优异的泛化性能。为促进后续研究，我们将向社区公开代码、模型及完整数据集。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2510.25772">HuggingFace</a> | <a href="https://arxiv.org/abs/2510.25772">arXiv</a></p>
<hr />
<h3>8. ReForm：基于前瞻有界序列优化的反射式自动形式化方法</h3>
<p><strong>原文标题：</strong> ReForm: Reflective Autoformalization with Prospective Bounded Sequence
  Optimization</p>
<p><strong>摘要：</strong>
自动形式化旨在将自然语言数学命题转化为机器可验证的形式化陈述，对于运用形式化数学推理解决自然语言表述的数学问题具有关键意义。尽管大语言模型能够生成语法正确的形式化陈述，但往往难以保持原始问题的语义意图。这一局限源于现有方法将自动形式化简单视为翻译任务，缺乏人类专家自然运用的自我反思与迭代优化机制。为解决这些问题，我们提出ReForm——一种反射式自动形式化方法，通过将语义一致性评估深度整合至形式化过程，使模型能够迭代生成形式化陈述、评估语义保真度，并通过渐进优化实现自我纠错。为有效训练该反射模型，我们提出前瞻有界序列优化方法，通过在序列不同位置采用差异化奖励机制，确保模型既实现精确的形式化转换又完成可靠的语义验证，避免产生削弱反思效用的表面化评判。在四个自动形式化基准测试上的大量实验表明，ReForm相较最强基线模型平均提升17.2个百分点。为进一步确保评估可靠性，我们构建了ConsistencyCheck基准数据集，包含859项经专家标注的测试项。该数据集不仅验证了大语言模型作为评估者的有效性，还揭示出自动形式化任务本身固有的困难性：即使人类专家在最高38.5%的情况下也会产生语义错误。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2510.24592">HuggingFace</a> | <a href="https://arxiv.org/abs/2510.24592">arXiv</a></p>
<hr />
<h3>9. 将驾驶世界模型重新构想为感知任务的合成数据生成器</h3>
<p><strong>原文标题：</strong> Rethinking Driving World Model as Synthetic Data Generator for
  Perception Tasks</p>
<p><strong>摘要：</strong>
驾驶世界模型的最新进展实现了高质量RGB视频或多模态视频的可控生成。现有方法主要关注生成质量与可控性相关指标，却往往忽视对下游感知任务的评估——这对自动驾驶系统的性能至关重要。传统方法通常采用先在合成数据上预训练、再在真实数据上微调的训练策略，导致训练周期达到基准方法（仅使用真实数据）的两倍。当我们将基准方法的训练周期加倍时，合成数据的优势变得微乎其微。为充分论证合成数据的价值，我们提出Dream4Drive——一个专为增强下游感知任务设计的新型合成数据生成框架。该框架首先将输入视频分解为若干三维感知引导图，随后将三维资源渲染至这些引导图，最后通过微调驾驶世界模型生成经过编辑的多视角真实感视频，这些视频可用于训练下游感知模型。Dream4Drive实现了大规模生成多视角边缘案例的前所未有的灵活性，显著提升了自动驾驶中的边缘案例感知能力。为促进未来研究，我们还构建了名为DriveObj3D的大规模三维资源数据集，涵盖驾驶场景中的典型类别，支持多样化的三维感知视频编辑。综合实验表明，在不同训练周期下，Dream4Drive均能有效提升下游感知模型的性能。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2510.19195">HuggingFace</a> | <a href="https://arxiv.org/abs/2510.19195">arXiv</a></p>
<hr />
<h3>10. 并行循环Transformer：高效测试时计算扩展</h3>
<p><strong>原文标题：</strong> Parallel Loop Transformer for Efficient Test-Time Computation Scaling</p>
<p><strong>摘要：</strong>
大型语言模型虽然功能强大，但在实际推理应用中往往存在速度缓慢、计算成本高昂的问题。循环Transformer通过在多轮计算步骤（即"循环"）中复用相同权重来节省参数量，但该方法存在显著缺陷：循环必须顺序执行，导致推理延迟和内存需求随循环次数增加而递增，难以满足快速应用需求。为解决这一问题，我们提出并行循环Transformer（PLT）。这种新型架构既能保持深层循环模型的性能优势，又具备标准非循环模型的低延迟特性。PLT通过两大核心技术实现这一目标：首先，跨循环并行技术（CLP）通过单次前向传播同时处理不同标记的循环计算，打破了顺序依赖；其次，为控制内存增长，我们采用高效表征增强策略——将首轮循环产生的键值缓存共享至所有后续循环，并采用门控滑动窗口注意力机制（G-SWA）将全局共享信息与局部信息相融合，从而保持高精度。实验结果表明，PLT在实现传统循环模型高精度的同时，其延迟和内存开销与标准Transformer相比几乎持平。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2510.24824">HuggingFace</a> | <a href="https://arxiv.org/abs/2510.24824">arXiv</a></p>
<hr />
<h3>11. MASPRM：多智能体系统过程奖励模型</h3>
<p><strong>原文标题：</strong> MASPRM: Multi-Agent System Process Reward Model</p>
<p><strong>摘要：</strong>
多智能体系统（MAS）的实际部署需要强大的测试时性能，这推动了引导推理时搜索并选择性分配计算资源以提升质量的方法发展。我们提出多智能体系统过程奖励模型（MASPRM），该模型通过为智能体间交互片段中的每个动作和每个智能体分配价值，充当推理时控制器。MASPRM通过多智能体蒙特卡洛树搜索（MCTS） rollout进行训练，无需逐步骤人工标注，而是通过将回报传播至局部目标实现。在推理阶段，MASPRM指导逐步骤集束搜索和MCTS，将计算资源聚焦于潜力分支并实施早期剪枝。在GSM8K和MATH数据集上的实验表明，结合最终答案结果奖励模型（ORM）的MASPRM引导解码，相较于单次直通式MAS处理，其精确匹配率（EM）分别提升30.7和22.9个百分点。在GSM8K上训练的MASPRM模型无需重新训练即可零样本迁移至MATH数据集，在相同计算预算下实现8.4个EM点的提升。MASPRM作为即插即用的价值评估模型，能够估计单智能体进度并补充验证器式解码器，从而实现更可靠、具备计算感知能力的多智能体推理。代码地址：https://github.com/milad1378yz/MASPRM</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2510.24803">HuggingFace</a> | <a href="https://arxiv.org/abs/2510.24803">arXiv</a></p>
<hr />
<h3>12. 明闪万象：面向多模态感知与生成的稀疏统一架构</h3>
<p><strong>原文标题：</strong> Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal
  Perception and Generation</p>
<p><strong>摘要：</strong>
本文提出明闪万象——明万象的升级版本，该架构基于灵闪2.0的稀疏专家混合变体构建，总参数量达1000亿，其中每个令牌仅激活61亿参数。该架构实现了高效扩展（在显著提升模型容量的同时大幅改善计算效率），并强化了视觉、语音与文本领域的统一多模态智能，标志着向通用人工智能迈进的关键一步。相较于前代模型，升级版本在多模态理解与生成任务上均展现出显著提升：在语音识别领域实现重大突破，上下文语音识别达到业界最优水平，方言感知语音识别取得极具竞争力的结果；在图像生成方面，明闪万象实现了高保真文本渲染功能，并在图像编辑的场景连贯性与身份保持性上表现突出；此外，该模型创新性提出生成式分割能力，不仅具备独立的分割性能，更可增强图像生成的空间控制力并提升编辑一致性。值得关注的是，明闪万象在文图生成与生成式分割任务中均达到当前最优水平，并在全部12项上下文语音识别基准测试中刷新纪录，所有功能均集成于单一统一架构之中。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2510.24821">HuggingFace</a> | <a href="https://arxiv.org/abs/2510.24821">arXiv</a></p>
<hr />
<h3>13. SeeingEye：基于智能体信息流的纯文本大语言模型多模态推理能力解锁</h3>
<p><strong>原文标题：</strong> SeeingEye: Agentic Information Flow Unlocks Multimodal Reasoning In
  Text-only LLMs</p>
<p><strong>摘要：</strong>
纯文本大语言模型（如DeepSeek-R1）近期取得显著进展，展现出卓越的推理能力。然而当扩展至多模态任务时，这些模型仍表现脆弱或完全失效。现有方法主要依赖单一形式的图像描述，这种描述缺乏多样性且难以适配不同类型的视觉问答基准测试，导致无法建立有效传递细粒度视觉信息的规范化通道。我们提出SeeingEye模块化框架，通过基于智能体的小型视觉语言模型翻译器，解锁纯文本大语言模型的多模态推理能力。该翻译器作为感知智能体：可调用专用工具（如OCR和图像裁剪），将多模态输入迭代提炼为契合问题的结构化中间表示。这些结构化中间表示随后输入作为推理智能体的纯文本大语言模型。关键创新在于翻译器与推理器之间建立的多轮反馈交互机制，既能提取目标视觉细节，又能生成置信度更高的答案。在知识密集型视觉问答基准测试（包括MMMU和MIA-Bench）上的实验表明，SeeingEye不仅降低推理成本，更超越规模更大的端到端视觉语言模型。例如，结合30亿参数视觉翻译器与80亿参数语言推理器的实例，在基于知识的挑战性问题上性能优于单体320亿参数视觉语言模型。我们的研究结果证明，通过智能体信息流实现感知与推理的解耦，为多模态推理提供了可扩展的即插即用路径，使强效纯文本大语言模型得以充分发挥其推理潜能。代码已开源：https://github.com/ulab-uiuc/SeeingEye</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2510.25092">HuggingFace</a> | <a href="https://arxiv.org/abs/2510.25092">arXiv</a></p>
<hr />
<h3>14. TheraMind：面向纵向心理咨询的战略性自适应智能体</h3>
<p><strong>原文标题：</strong> TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological
  Counseling</p>
<p><strong>摘要：</strong>
基于大语言模型的心理咨询应用日益受到关注。然而现有方法普遍存在情感理解不足、策略适应性缺失以及跨会话治疗技术运用不充分等问题，尤其缺乏结合长期记忆的多轮对话能力，使其与真实临床实践存在显著差距。为突破这些关键瓶颈，我们提出TheraMind——一种面向纵向心理咨询的战略性自适应智能体。该系统的核心创新是新颖的双循环架构：通过会话内循环实现战术性对话管理，通过跨会话循环执行战略性治疗规划，从而解耦复杂咨询过程。会话内循环实时感知患者情绪状态以动态选择应答策略，同时利用跨会话记忆确保连续性；跨会话循环则通过评估每次咨询的治疗效果并调整后续干预方案，赋予系统长期自适应能力。我们在基于真实临床案例构建的高保真模拟环境中验证该方法。大量评估表明，TheraMind在连贯性、灵活性和治疗协调性等多会话指标上显著优于现有方法，验证了双循环设计在模拟战略性、自适应及纵向治疗行为方面的有效性。代码已公开于https://0mwwm0.github.io/TheraMind/。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2510.25758">HuggingFace</a> | <a href="https://arxiv.org/abs/2510.25758">arXiv</a></p>
<hr />
<h3>15. PairUni：面向统一多模态语言模型的成对训练方法</h3>
<p><strong>原文标题：</strong> PairUni: Pairwise Training for Unified Multimodal Language Models</p>
<p><strong>摘要：</strong>
统一视觉语言模型（UVLMs）需在单一架构中同时实现理解与生成任务，但这两类任务依赖异构数据与监督信号，导致在强化学习（RL）过程中难以实现任务平衡。本文提出PairUni统一框架，通过将数据重组为理解-生成（UG）配对样本并实施对齐优化。我们首先采用GPT-4技术增强单任务数据：为理解样本生成描述文本，为生成样本生成问答对，从而构建同源实例的对齐配对。此外，针对每个生成样本，我们检索语义相关的理解样本构建检索配对，实现异源但相关联数据点的连接。这种配对结构显式呈现跨任务语义关联，并支持一致性策略学习。为利用该结构，我们提出Pair-GPRO——基于群组相对策略优化的配对感知变体，通过为每对样本分配相似度评分来调节优势函数，从而增强对齐良好样本的学习效果并降低任务干扰。我们构建了包含16K组UG配对的高质量数据集PairUG用于RL微调，并在强基准模型Janus-Pro UVLMs上评估PairUni。实验表明该方法在多种UVLM模型上实现均衡性能提升，显著优于现有UVLM强化学习基线。代码地址：https://github.com/Haochen-Wang409/PairUni</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2510.25682">HuggingFace</a> | <a href="https://arxiv.org/abs/2510.25682">arXiv</a></p>
<hr />
<h3>16. BhashaBench V1：印度知识象限领域的综合性基准测试集</h3>
<p><strong>原文标题：</strong> BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic
  Domains</p>
<p><strong>摘要：</strong>
大语言模型的快速发展加剧了对领域与文化特异性评估的需求。现有基准测试集大多以英语为中心且缺乏领域针对性，限制了其在印度语境下的适用性。为弥补这一空白，我们推出BhashaBench V1——首个聚焦关键印度知识体系的领域特异性、多任务、双语基准测试集。该数据集包含74,166个精心构建的问答对（其中英语52,494个，印地语21,672个），数据源自主管部门及专业领域考试，涵盖农业、法律、金融与阿育吠陀四大核心领域，包含90余个子领域及500多个主题，支持细粒度评估。对29款大语言模型的测试结果表明：模型在不同领域和语言间存在显著性能差异，资源稀缺领域表现尤为薄弱。例如GPT-4o在法律领域总体准确率达76.49%，而在阿育吠陀领域仅为59.74%；所有领域内模型对英语内容的处理能力均优于印地语。子领域分析显示，网络法律、国际金融等领域表现相对较好，而潘查卡尔玛疗法、种子科学、人权等领域则明显薄弱。BhashaBench V1为评估大语言模型在印度多元知识领域的表现提供了全面数据集，可检验模型融合领域专业知识与双语理解的能力。所有代码、基准数据及相关资源均已公开，以支持开放研究。</p>
<p><strong>论文链接：</strong> <a href="https://huggingface.co/papers/2510.25409">HuggingFace</a> | <a href="https://arxiv.org/abs/2510.25409">arXiv</a></p>
<hr />
<h2>🔍 关键词云图</h2>
<p><img alt="关键词云图" src="../images/keywords_wordcloud.png" /></p>
<h2>📈 近期论文趋势</h2>
<p><img alt="论文趋势" src="../images/daily_papers.png" /></p>
<h2>🎙️ 语音播报</h2>
<ul>
<li><a href="../audio/2025-10-30_daily_papers.mp3">收听今日论文解读</a></li>
</ul>
<h2>📱 订阅渠道</h2>
<ul>
<li>GitHub: <a href="https://github.com/2404589803/hf-daily-paper-newsletter-chinese">hf-daily-paper-newsletter-chinese</a></li>
</ul>
    </div>
</body>
</html>